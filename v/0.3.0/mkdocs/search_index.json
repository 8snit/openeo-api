{
    "docs": [
        {
            "location": "/index.html",
            "text": "openEO - Concepts and API Reference\n\n\nWork in progress, please contribute by adding \nissues\n.\n\n\nopenEO develops an open API that connects clients like R, Python and JavaScript to big Earth observation cloud back-ends in a simple and unified way.\n\n\nThe following pages introduce the core concepts of the project. Make sure to introduce yourself to the major technical terms used in the openEO project by reading the \nglossary\n.\n\n\nThe openEO API defines a \nHTTP API\n that lets cloud back-ends with large Earth observation datasets communicate with front end analysis applications in an interoperable way. This documentation describes important API concepts and design decisions and gives a complete \nAPI reference documentation\n. As Swagger UI has some issues, e.g. rendering certain examples, you can also \nview the API reference documentation by importing it into the ReDoc demo\n, which probably renders the API specification better.\n\n\nAs an overview, the openEO API specifies how to\n\n\n\n\ndiscover which Earth observation data and processes are available at cloud back-ends,\n\n\nexecute (chained) processes on back-ends, \n\n\nrun \nuser-defined functions\n (UDFs) on back-ends where UDFs can be exposed to the data in different ways, \n\n\ndownload (intermediate) results as web services, and\n\n\nmanage user content including accounting.\n\n\n\n\nThe API is defined as an \nOpenAPI 3.0\n JSON file.\n\n\n \n\n\nopenEO\n, A Common, Open Source Interface between Earth Observation Data Infrastructures and Front-End Applications is a H2020 project funded under call EO-2-2017: EO Big Data Shift, under proposal number 776242. It will run from Oct 2017 to Sept 2020.\n\n\nThis project has received funding from the European Union\u2019s Horizon 2020 research and innovation programme under grant agreement No 776242. The contents of this website reflects only the authors\u2019 view; the European Commission is not responsible for any use that may be made of the information it provides.",
            "title": "Home"
        },
        {
            "location": "/index.html#openeo-concepts-and-api-reference",
            "text": "Work in progress, please contribute by adding  issues .  openEO develops an open API that connects clients like R, Python and JavaScript to big Earth observation cloud back-ends in a simple and unified way.  The following pages introduce the core concepts of the project. Make sure to introduce yourself to the major technical terms used in the openEO project by reading the  glossary .  The openEO API defines a  HTTP API  that lets cloud back-ends with large Earth observation datasets communicate with front end analysis applications in an interoperable way. This documentation describes important API concepts and design decisions and gives a complete  API reference documentation . As Swagger UI has some issues, e.g. rendering certain examples, you can also  view the API reference documentation by importing it into the ReDoc demo , which probably renders the API specification better.  As an overview, the openEO API specifies how to   discover which Earth observation data and processes are available at cloud back-ends,  execute (chained) processes on back-ends,   run  user-defined functions  (UDFs) on back-ends where UDFs can be exposed to the data in different ways,   download (intermediate) results as web services, and  manage user content including accounting.   The API is defined as an  OpenAPI 3.0  JSON file.     openEO , A Common, Open Source Interface between Earth Observation Data Infrastructures and Front-End Applications is a H2020 project funded under call EO-2-2017: EO Big Data Shift, under proposal number 776242. It will run from Oct 2017 to Sept 2020.  This project has received funding from the European Union\u2019s Horizon 2020 research and innovation programme under grant agreement No 776242. The contents of this website reflects only the authors\u2019 view; the European Commission is not responsible for any use that may be made of the information it provides.",
            "title": "openEO - Concepts and API Reference"
        },
        {
            "location": "/glossary/index.html",
            "text": "Glossary\n\n\nThis glossary introduces, and tries to define, the major technical terms used in the openEO project.\n\n\nThe acronym \nopenEO\n contracts two concepts:\n\n\n\n\nopen\n: used here in the context of open source software; open source software is available in source code form, and can be freely modified and redistributed; the openEO project will create open source software, reusable under a liberal open source license (Apache 2.0)\n\n\nEO\n: Earth observation; openEO targets the processing and analysis of Earth observation data\n\n\n\n\nFurther terms:\n\n\n\n\nAPI\n: application programming interface (\nwikipedia\n); a communication protocol between client and back-end\n\n\nclient\n: software environment (software) that end-users directly interact with, e.g. R (rstudio), Python (jupyter notebook), and JavaScript (web browser); R and Python are two major data science platforms; JavaScript is a major language for web development\n\n\n(cloud) back-end\n: server; computer infrastructure (one or more physical computers or virtual machines) used for storing EO data and processing it\n\n\nbig Earth observation cloud back-end\n server infrastructure where industry and researchers analyse large amounts of EO data\n\n\nsimple\n many end-users now use Python or R to analyse data and JavaScript to develop web applications; analysing large amounts of EO imagery should be equally simple, and seamlessly integrate with existing workflows\n\n\nunified\n current EO cloud back-ends all have \na different API\n, making EO data analysis hard to validate,difficult to reproduce, and back-ends difficult to compare in terms of capability and costs, or to combine in a joint analysis across back-ends. A unified API can resolve many of these problems.\n\n\n\n\nDatasets\n\n\nCEOS (\nCEOS OpenSearch Best Practice Document v1.2\n) defines \nGranules\n and \nCollections\n as follows:\n\n\n\n\n\"A \ngranule\n is the finest granularity of data that can be independently managed. A granule usually matches the individual file of EO satellite data.\"\n\n\n\"A \ncollection\n is an aggregation of granules sharing the same product specification. A collection typically corresponds to the series of products derived from data acquired by a sensor on board a satellite and having the same mode of operation.\"\n\n\n\n\nThe same document lists the synonyms used (by organisations) for:\n\n\n\n\ngranule\n: dataset (ISO 19115), dataset (ESA), granule (NASA), product (ESA, CNES), scene (JAXA)\n\n\ncollection\n: dataset series (ISO 19115), collection (CNES, NASA), dataset (JAXA), dataset series (ESA), product (JAXA)\n\n\n\n\nHere, we will use \ngranule\n and \ncollection\n.\n\n\nA \ngranule\n will typically refer to a limited area and a single overpass leading to a very short observation period (seconds), or a temporal aggregation of such data as e.g. for 16-day MODIS composites.\n\n\nThe open geospatial consortium published a document on \nOGC OpenSearch Geo and Time Extensions\n.\n\n\nProcesses and Jobs\n\n\nThe terms \nprocess\n, \nprocess graph\n and \njob\n have different meanings in the openEO API specification.\n\n\nA \nprocess\n is simply the description of an operation as provided by the back end, similar to a function definition in programming languages. \n\n\nIn this context openEO will:\n\n\n\n\nconsider, or allow to consider, band as a dimension\n\n\nconsider imagery (image collections) to consist of one \nor more\n collections, as argument to functions; allow filtering on a particular collection, or joining them into a single collection\n\n\nallow filtering on attributes, e.g. on cloud-free pixels, or pixels inside a \nMULTIPOLYGON\n describing the floodplains of the Danube. This filters on attributes rather than dimensions.\n\n\nProvide generic aggregate operations that aggregate over one or more dimensions. Clients may provide dimension-specific aggregation functions for particular cases (such as \nmin_time\n) \n\n\n\n\nA \nprocess graph\n includes specific process calls, i.e. references to one or more processes including specific values for input arguments similar to a function call in programming. However, process graphs can chain multiple processes. In particular, arguments of processes in general can be again (recursive) process graphs, input datasets, or simple scalar or array values.\n\n\nA \njob\n brings one process graph to the back-end and organizes its execution, which may or may not induce costs. Jobs furthermore allow to run process graphs with different constraints (see section on \nconstraints\n). Constraints define at which resolution and extent we look at the data during processing and hence allow to try out process graphs on small subsets, or work interactively within web map applications. For more information about jobs and their evaluation types, see the section on \njobs\n.\n\n\nUser-defined functions (UDFs)\n\n\nThe abbreviation \nUDF\n stands for \nuser-defined function\n. With this concept, users are able to upload custom code and have it executed e.g. for every pixel of a scene, allowing custom calculations on server-side data. See the section on \nUDFs\n for more information.\n\n\nAggregation vs. resampling\n\n\nAggregation\n computes new values from sets of values that are \nuniquely\n assigned to groups. It involves a grouping predicate (e.g. monthly, 100 m x 100 m grid cells; think of SQL's \ngroup_by\n), and an aggregation function (e.g., \nmean\n) that computes one or more new values from the original ones.\n\n\nExamples:\n\n\n\n\na time series aggregation may return a regression slope and intercept for every pixel time series, for a single band (group by: full time extent)\n\n\na time series may be aggregated to monthly values by computing the mean for all values in a month (group by: months)\n\n\nspatial\n aggregation involves computing e.g. \nmean\n pixel values on a 100 x 100 m grid, from 10 m x 10 m pixels, where each original pixel is assigned uniquely to a larger pixel (group by: 100 m x 100 m grid cells)\n\n\n\n\nNote that for the first example, the aggregation function not only requires time series values, but also their time stamps.\n\n\nResampling\n is a broader term where we have data at one resolution, and need values at another (also called \nscaling\n). In case we have values at a 100 m x 100 m grid and need values at a 10 m x 10 m grid, the original values will be reused many times, and may be be simply assigned to the nearest high resolution grid cells (\"nearest neighbor\"), or may be interpolated somehow (e.g. by bilinear interpolation). Resampling from finer to coarser grid by nearest neighbor may again be a special case of aggregation.\n\n\nWhen the target grid or time series has a lower resolution (larger grid cells) or lower frequency (longer time intervals) than the source grid, aggregation might be used for resampling. For example, if the resolutions are fairly similar, say the source collection has values for consecutive 10 day intervals and the target needs values for consecutive 16 day intervals, then some form of interpolation may be more appropriate than aggregation as defined here.",
            "title": "Glossary"
        },
        {
            "location": "/glossary/index.html#glossary",
            "text": "This glossary introduces, and tries to define, the major technical terms used in the openEO project.  The acronym  openEO  contracts two concepts:   open : used here in the context of open source software; open source software is available in source code form, and can be freely modified and redistributed; the openEO project will create open source software, reusable under a liberal open source license (Apache 2.0)  EO : Earth observation; openEO targets the processing and analysis of Earth observation data   Further terms:   API : application programming interface ( wikipedia ); a communication protocol between client and back-end  client : software environment (software) that end-users directly interact with, e.g. R (rstudio), Python (jupyter notebook), and JavaScript (web browser); R and Python are two major data science platforms; JavaScript is a major language for web development  (cloud) back-end : server; computer infrastructure (one or more physical computers or virtual machines) used for storing EO data and processing it  big Earth observation cloud back-end  server infrastructure where industry and researchers analyse large amounts of EO data  simple  many end-users now use Python or R to analyse data and JavaScript to develop web applications; analysing large amounts of EO imagery should be equally simple, and seamlessly integrate with existing workflows  unified  current EO cloud back-ends all have  a different API , making EO data analysis hard to validate,difficult to reproduce, and back-ends difficult to compare in terms of capability and costs, or to combine in a joint analysis across back-ends. A unified API can resolve many of these problems.",
            "title": "Glossary"
        },
        {
            "location": "/glossary/index.html#datasets",
            "text": "CEOS ( CEOS OpenSearch Best Practice Document v1.2 ) defines  Granules  and  Collections  as follows:   \"A  granule  is the finest granularity of data that can be independently managed. A granule usually matches the individual file of EO satellite data.\"  \"A  collection  is an aggregation of granules sharing the same product specification. A collection typically corresponds to the series of products derived from data acquired by a sensor on board a satellite and having the same mode of operation.\"   The same document lists the synonyms used (by organisations) for:   granule : dataset (ISO 19115), dataset (ESA), granule (NASA), product (ESA, CNES), scene (JAXA)  collection : dataset series (ISO 19115), collection (CNES, NASA), dataset (JAXA), dataset series (ESA), product (JAXA)   Here, we will use  granule  and  collection .  A  granule  will typically refer to a limited area and a single overpass leading to a very short observation period (seconds), or a temporal aggregation of such data as e.g. for 16-day MODIS composites.  The open geospatial consortium published a document on  OGC OpenSearch Geo and Time Extensions .",
            "title": "Datasets"
        },
        {
            "location": "/glossary/index.html#processes-and-jobs",
            "text": "The terms  process ,  process graph  and  job  have different meanings in the openEO API specification.  A  process  is simply the description of an operation as provided by the back end, similar to a function definition in programming languages.   In this context openEO will:   consider, or allow to consider, band as a dimension  consider imagery (image collections) to consist of one  or more  collections, as argument to functions; allow filtering on a particular collection, or joining them into a single collection  allow filtering on attributes, e.g. on cloud-free pixels, or pixels inside a  MULTIPOLYGON  describing the floodplains of the Danube. This filters on attributes rather than dimensions.  Provide generic aggregate operations that aggregate over one or more dimensions. Clients may provide dimension-specific aggregation functions for particular cases (such as  min_time )    A  process graph  includes specific process calls, i.e. references to one or more processes including specific values for input arguments similar to a function call in programming. However, process graphs can chain multiple processes. In particular, arguments of processes in general can be again (recursive) process graphs, input datasets, or simple scalar or array values.  A  job  brings one process graph to the back-end and organizes its execution, which may or may not induce costs. Jobs furthermore allow to run process graphs with different constraints (see section on  constraints ). Constraints define at which resolution and extent we look at the data during processing and hence allow to try out process graphs on small subsets, or work interactively within web map applications. For more information about jobs and their evaluation types, see the section on  jobs .  User-defined functions (UDFs)  The abbreviation  UDF  stands for  user-defined function . With this concept, users are able to upload custom code and have it executed e.g. for every pixel of a scene, allowing custom calculations on server-side data. See the section on  UDFs  for more information.",
            "title": "Processes and Jobs"
        },
        {
            "location": "/glossary/index.html#aggregation-vs-resampling",
            "text": "Aggregation  computes new values from sets of values that are  uniquely  assigned to groups. It involves a grouping predicate (e.g. monthly, 100 m x 100 m grid cells; think of SQL's  group_by ), and an aggregation function (e.g.,  mean ) that computes one or more new values from the original ones.  Examples:   a time series aggregation may return a regression slope and intercept for every pixel time series, for a single band (group by: full time extent)  a time series may be aggregated to monthly values by computing the mean for all values in a month (group by: months)  spatial  aggregation involves computing e.g.  mean  pixel values on a 100 x 100 m grid, from 10 m x 10 m pixels, where each original pixel is assigned uniquely to a larger pixel (group by: 100 m x 100 m grid cells)   Note that for the first example, the aggregation function not only requires time series values, but also their time stamps.  Resampling  is a broader term where we have data at one resolution, and need values at another (also called  scaling ). In case we have values at a 100 m x 100 m grid and need values at a 10 m x 10 m grid, the original values will be reused many times, and may be be simply assigned to the nearest high resolution grid cells (\"nearest neighbor\"), or may be interpolated somehow (e.g. by bilinear interpolation). Resampling from finer to coarser grid by nearest neighbor may again be a special case of aggregation.  When the target grid or time series has a lower resolution (larger grid cells) or lower frequency (longer time intervals) than the source grid, aggregation might be used for resampling. For example, if the resolutions are fairly similar, say the source collection has values for consecutive 10 day intervals and the target needs values for consecutive 16 day intervals, then some form of interpolation may be more appropriate than aggregation as defined here.",
            "title": "Aggregation vs. resampling"
        },
        {
            "location": "/arch/index.html",
            "text": "Architecture\n\n\nThe openEO API defines a language how clients communicate to back-ends in order to analyze large Earth observation datasets. The API will be implemented by drivers for specific back-ends. Some first architecture considerations are listed below.\n\n\n\n\nThe openEO API is a contract between clients and back-ends that describes the communication only\n\n\nEach back-end runs its own API instance including the specific back-end driver. There is no API instance that runs more than one driver.\n\n\nClients in R, Python, and JavaScript connect directly to the back-ends and communicate with the back-ends over \nHTTPS\n according to the openEO API specification.\n\n\nAPI instances can run on back-end servers or additional intermediate layers, which then communicate to back-ends in a back-end specific way.\n\n\nBack-ends may add functionality and extend the API wherever there is need.\n\n\nThere will be a central back-end registry service (openEO Hub), to allow users to search for back-ends with specific functionality and or data. \n\n\nThe openEO API will define \nprofiles\n in order group specific functionality.\n\n\n\n\n\n\nMicroservices\n\n\nTo simplify and structure the development, the API is divided into a few microservices.\n\n\n\n\n\n\n\n\nMicroservice\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nCapabilities\n\n\nThis microservice reports on the capabilities of the back-end, i.e. which API endpoints are implemented, which authentication methods are supported, and whether and how UDFs can be executed at the back-end.\n\n\n\n\n\n\nEO Data Discovery\n\n\nDescribes which datasets and image collections are available at the back-end.\n\n\n\n\n\n\nProcess Discovery\n\n\nProvides services to find out which processes a back-end provides, i.e., what users can do with the available data.\n\n\n\n\n\n\nUDF Runtime Discovery\n\n\nAllows discovering the programming languages and their runtime environments to execute user-defined functions.\n\n\n\n\n\n\nJob Management\n\n\nOrganizes and manages jobs that run processes on back-ends.\n\n\n\n\n\n\nFile Management\n\n\nOrganizes and manages user-uploaded files.\n\n\n\n\n\n\nProcess Graph Management\n\n\nOrganizes and manages user-defined process graphs.\n\n\n\n\n\n\nWeb Service Management\n\n\nWeb services to to access data and job results, e.g. as WCS or WMTS service.\n\n\n\n\n\n\nUser Content\n\n\nUser content and accounting.\n\n\n\n\n\n\nAuthentication\n\n\nAuthentication of users.",
            "title": "Architecture and Microservices"
        },
        {
            "location": "/arch/index.html#architecture",
            "text": "The openEO API defines a language how clients communicate to back-ends in order to analyze large Earth observation datasets. The API will be implemented by drivers for specific back-ends. Some first architecture considerations are listed below.   The openEO API is a contract between clients and back-ends that describes the communication only  Each back-end runs its own API instance including the specific back-end driver. There is no API instance that runs more than one driver.  Clients in R, Python, and JavaScript connect directly to the back-ends and communicate with the back-ends over  HTTPS  according to the openEO API specification.  API instances can run on back-end servers or additional intermediate layers, which then communicate to back-ends in a back-end specific way.  Back-ends may add functionality and extend the API wherever there is need.  There will be a central back-end registry service (openEO Hub), to allow users to search for back-ends with specific functionality and or data.   The openEO API will define  profiles  in order group specific functionality.",
            "title": "Architecture"
        },
        {
            "location": "/arch/index.html#microservices",
            "text": "To simplify and structure the development, the API is divided into a few microservices.     Microservice  Description      Capabilities  This microservice reports on the capabilities of the back-end, i.e. which API endpoints are implemented, which authentication methods are supported, and whether and how UDFs can be executed at the back-end.    EO Data Discovery  Describes which datasets and image collections are available at the back-end.    Process Discovery  Provides services to find out which processes a back-end provides, i.e., what users can do with the available data.    UDF Runtime Discovery  Allows discovering the programming languages and their runtime environments to execute user-defined functions.    Job Management  Organizes and manages jobs that run processes on back-ends.    File Management  Organizes and manages user-uploaded files.    Process Graph Management  Organizes and manages user-defined process graphs.    Web Service Management  Web services to to access data and job results, e.g. as WCS or WMTS service.    User Content  User content and accounting.    Authentication  Authentication of users.",
            "title": "Microservices"
        },
        {
            "location": "/guidelines/index.html",
            "text": "General API Guidelines\n\n\nTo provide the smoothest possible experience, it's important to have these APIs follow consistent design guidelines, thus making using them easy and intuitive.\n\n\nLanguage\n\n\nLanguage\n\n\nGenerally, English language MUST be used for all names, documentation etc.\n\n\nIn the specification the key words \u201cMUST\u201d, \u201cMUST NOT\u201d, \u201cREQUIRED\u201d, \u201cSHALL\u201d, \u201cSHALL NOT\u201d, \u201cSHOULD\u201d, \u201cSHOULD NOT\u201d, \u201cRECOMMENDED\u201d, \u201cMAY\u201d, and \u201cOPTIONAL\u201d in this document are to be interpreted as described in \nRFC 2119\n.\n\n\nCasing\n\n\nAll names SHOULD be written in snake case, i.e. words are separated with one\u00a0underscore\u00a0character (_) and no spaces, with all letters lowercased. Example: \nhello_world\n. This applies particularly to endpoints and JSON property names. HTTP header fields follow their respective casing conventions, e.g. \nContent-Type\n or \nOpenEO-Costs\n, despite being case-insensitive according to \nRFC 7230\n.\n\n\nTechnical requirements\n\n\nHTTP\n\n\nThe API developed by the openEO project uses \nHTTP REST Level 2\n for communication between client and back-end server.\n\n\nPublic APIs MUST be available via HTTPS only and all inbound calls MUST be HTTPS. \n\n\nVerbs\n\n\nEndpoints SHOULD use meaningful HTTP verbs (e.g. GET, POST, PUT, PATCH, DELETE).\n\n\nIf there is a need to transfer big chunks of data via GET requests, POST requests MAY be used as a replacement as they support to send data via request body.\n\n\nUnless otherwise stated, PATCH requests are only defined to work on the direct (first-level) children of the full JSON object. Therefore, changing a property on a deeper level of the full JSON object always requires to send the whole JSON object defined by the first-level property.\n\n\nResource naming\n\n\nNaming of endpoints SHOULD follow the REST principles. Therefore, endpoints SHOULD be centered around resources. Resource identifiers MUST be named with a noun in plural form except for single actions that can not be modelled with the regular HTTP verbs. Single actions MUST be single endpoint with a single HTTP verb (POST is RECOMMENDED) and no other endpoints beneath it.\n\n\nCross-Origin Resource Sharing (CORS)\n\n\nAll back-end providers SHOULD support CORS. More information can be found in the \ncorresponding section\n.\n\n\nStatus codes and error handling\n\n\nThe success of requests MUST be indicated using \nHTTP status codes\n according to \nRFC 7231\n. More information can be found in the section about \nstatus und error handling\n.\n\n\nRequests and response formats\n\n\nJSON\n\n\nWeb-based communication, especially when a mobile or other low-bandwidth client is involved, has moved quickly in the direction of JSON for a variety of reasons, including its tendency to be lighter weight and its ease of consumption with JavaScript-based clients. Therefore, services SHOULD use JSON as the default encoding. Other response formats can be requested using \nContent Negotiation\n.\n\n\nClients and servers MUST NOT rely on the order in which properties appears in JSON responses. When supported by the service, clients MAY request that array elements be returned in a specific order.\n\n\nCollections SHOULD NOT include nested JSON objects if those information can be requested from the individual resources.\n\n\nTemporal data\n\n\nDate, time, intervals and durations MUST be formatted according to ISO 8601 if there is an appropriate encoding available in the standard.",
            "title": "General Guidelines"
        },
        {
            "location": "/guidelines/index.html#general-api-guidelines",
            "text": "To provide the smoothest possible experience, it's important to have these APIs follow consistent design guidelines, thus making using them easy and intuitive.",
            "title": "General API Guidelines"
        },
        {
            "location": "/guidelines/index.html#language",
            "text": "Language  Generally, English language MUST be used for all names, documentation etc.  In the specification the key words \u201cMUST\u201d, \u201cMUST NOT\u201d, \u201cREQUIRED\u201d, \u201cSHALL\u201d, \u201cSHALL NOT\u201d, \u201cSHOULD\u201d, \u201cSHOULD NOT\u201d, \u201cRECOMMENDED\u201d, \u201cMAY\u201d, and \u201cOPTIONAL\u201d in this document are to be interpreted as described in  RFC 2119 .  Casing  All names SHOULD be written in snake case, i.e. words are separated with one\u00a0underscore\u00a0character (_) and no spaces, with all letters lowercased. Example:  hello_world . This applies particularly to endpoints and JSON property names. HTTP header fields follow their respective casing conventions, e.g.  Content-Type  or  OpenEO-Costs , despite being case-insensitive according to  RFC 7230 .",
            "title": "Language"
        },
        {
            "location": "/guidelines/index.html#technical-requirements",
            "text": "HTTP  The API developed by the openEO project uses  HTTP REST Level 2  for communication between client and back-end server.  Public APIs MUST be available via HTTPS only and all inbound calls MUST be HTTPS.   Verbs  Endpoints SHOULD use meaningful HTTP verbs (e.g. GET, POST, PUT, PATCH, DELETE).  If there is a need to transfer big chunks of data via GET requests, POST requests MAY be used as a replacement as they support to send data via request body.  Unless otherwise stated, PATCH requests are only defined to work on the direct (first-level) children of the full JSON object. Therefore, changing a property on a deeper level of the full JSON object always requires to send the whole JSON object defined by the first-level property.  Resource naming  Naming of endpoints SHOULD follow the REST principles. Therefore, endpoints SHOULD be centered around resources. Resource identifiers MUST be named with a noun in plural form except for single actions that can not be modelled with the regular HTTP verbs. Single actions MUST be single endpoint with a single HTTP verb (POST is RECOMMENDED) and no other endpoints beneath it.  Cross-Origin Resource Sharing (CORS)  All back-end providers SHOULD support CORS. More information can be found in the  corresponding section .  Status codes and error handling  The success of requests MUST be indicated using  HTTP status codes  according to  RFC 7231 . More information can be found in the section about  status und error handling .  Requests and response formats  JSON  Web-based communication, especially when a mobile or other low-bandwidth client is involved, has moved quickly in the direction of JSON for a variety of reasons, including its tendency to be lighter weight and its ease of consumption with JavaScript-based clients. Therefore, services SHOULD use JSON as the default encoding. Other response formats can be requested using  Content Negotiation .  Clients and servers MUST NOT rely on the order in which properties appears in JSON responses. When supported by the service, clients MAY request that array elements be returned in a specific order.  Collections SHOULD NOT include nested JSON objects if those information can be requested from the individual resources.  Temporal data  Date, time, intervals and durations MUST be formatted according to ISO 8601 if there is an appropriate encoding available in the standard.",
            "title": "Technical requirements"
        },
        {
            "location": "/errors/index.html",
            "text": "Status and error handling\n\n\nThe success of requests MUST be indicated using \nHTTP status codes\n according to \nRFC 7231\n.\n\n\nIf the API responds with a status code between 100 and 399 the back-end indicates that the request has been handled successfully.\n\n\nIn general an error is communicated with a status code between 400 and 599. Client errors are defined as a client passing invalid data to the service and the service\u00a0\ncorrectly\n rejecting that data. Examples include invalid credentials, incorrect parameters, unknown versions, or similar. These are generally \"4xx\" HTTP error codes and are the result of a client passing incorrect or invalid data. Client errors do\u00a0\nnot\n\u00a0contribute to overall API availability. \n\n\nServer errors are defined as the server failing to correctly return in response to a valid client request. These are generally \"5xx\" HTTP error codes. Server errors \ndo\n contribute to the overall API availability. Calls that fail due to rate limiting or quota failures MUST NOT count as server errors. \n\n\nJSON error object\n\n\nA JSON error object SHOULD be sent with all responses that have a status code between 400 and 599.\n\n\n{\n  \"id\": \"936DA01F-9ABD-4D9D-80C7-02AF85C822A8\",\n  \"code\": 123,\n  \"message\": \"A sample error message.\",\n  \"url\": \"http://www.openeo.org/docs/errors/123\"\n}\n\n\n\n\nSending \ncode\n and \nmessage\n is REQUIRED. \n\n\n\n\n\n\nA back-end MAY add a free-form \nid\n (unique identifier) to the error response to be able to log and track errors with further non-disclosable details.\n\n\n\n\n\n\nThe \ncode\n is either one of the standardized openEO error codes below or a proprietary error code with a number greater than 10000.\n\n\n\n\n\n\nThe \nmessage\n explains the reason the server is rejecting the request. For \"4xx\" error codes the message explains how the client needs to modify the request.\n\n\n\n\n\n\nBy default the message MUST be sent in English language. Content Negotiation is used to localize the error messages: If an \nAcceppt-Language\n header is sent by the client and a translation is available, the message should be translated accordingly and the \nContent-Language\n header must be present in the response. See \"\nHow to localize your API\n\" for more information.\n\n\n\n\nurl\n is an OPTIONAL attribute and contains a link to a resource that is explaining the error and potential solutions in-depth.\n\n\n\n\nStandardized status codes\n\n\nThe openEO API usually uses the following HTTP status codes for successful requests: \n\n\n\n\n200 OK\n:\n  Indicates a successful request \nwith\n a response body being sent.\n\n\n201 Created\n\n  Indicates a successful request that successfully created a new resource. Sends a \nLocation\n header to the newly created resource \nwithout\n a response body.\n\n\n202 Accepted\n\n  Indicates a successful request that successfully queued the creation of a new resource, but it has not been created yet. The response is sent \nwithout\n a response body.\n\n\n204 No Content\n:\n  Indicates a successful request \nwithout\n a response body being sent.\n\n\n\n\nThe openEO API often uses the following HTTP status codes for failed requests: \n\n\n\n\n\n\n400 Bad request\n:\n  The back-end responds with this error code whenever the error has its origin on client side and no other HTTP status code in the 400 range is suitable.\n\n\n\n\n\n\n401 Unauthorized\n:\n  The client \ndid not\n provide any authorization details (usually using the Authorization header), but authorization is required for this request to be processed.\n\n\n\n\n\n\n403 Forbidden\n:\n  The client \ndid\n provide authorization details (usually using the Authorization header), but the provided credentials or the authorization token is invalid or has expired.\n\n\n\n\n\n\n404 Not Found\n:\n  The resource specified by the path does not exist, i.e. one of the the resources belonging to the specified identifiers are not available at the back-end.\n  \nNote:\n Unsupported endpoints MUST use HTTP status code 501.\n\n\n\n\n\n\n500 Internal Server Error\n:\n  The error has its origin on server side and no other status code in the 500 range is suitable.\n\n\n\n\n\n\n501 Not implemented\n:\n  An endpoint is specified in the openEO API, but is not supported.\n\n\n\n\n\n\nIf a HTTP status code in the 400 range is returned, the client SHOULD NOT repeat the request without modifications. For HTTP status code in the 500 range, the client MAY repeat the same request later.\n\n\nAll HTTP status codes defined in RFC 7231 in the 400 and 500 ranges can be used as openEO error code in addition to the most used status codes mentioned here. Responding with openEO error codes 400 and 500 SHOULD be avoided in favor of any more specific standardized or proprietary openEO error code.\n\n\nGeneral error codes (xxx)\n\n\n\n\n\n\n\n\nopenEO Error Code\n\n\nDescription\n\n\nMessage\n\n\nHTTP Status Code\n\n\n\n\n\n\n\n\n\n\n404\n\n\nTo be used if the value of a path parameter is invalid, i.e. the requested resource is not available. \nNote:\n Unsupported endpoints MUST use code 501.\n\n\nNot Found.\n\n\n404\n\n\n\n\n\n\n501\n\n\nThe back-end responds with this error code whenever an endpoint is specified in the openEO API, but is not supported.\n\n\nNot implemented.\n\n\n501\n\n\n\n\n\n\n503\n\n\n\n\nService unavailable.\n\n\n503\n\n\n\n\n\n\n601\n\n\n\n\nParameter \nX\n is invalid.\n\n\n400\n\n\n\n\n\n\n611\n\n\nInvalid or unsupported CRS specified.\n\n\nInvalid CRS specified.\n\n\n400\n\n\n\n\n\n\n612\n\n\n\n\nCoordinate is out of bounds.\n\n\n400\n\n\n\n\n\n\n\n\nCapabilities (11xx)\n\n\nNone yet.\n\n\nData and process discovery (12xx)\n\n\nNone yet.\n\n\nUDFs (13xx)\n\n\n\n\n\n\n\n\nopenEO Error Code\n\n\nDescription\n\n\nMessage\n\n\nHTTP Status Code\n\n\n\n\n\n\n\n\n\n\n1301\n\n\n\n\nUDF programming language not supported.\n\n\n400 / 404\n\n\n\n\n\n\n1302\n\n\n\n\nUDF type not supported.\n\n\n400 / 404\n\n\n\n\n\n\n\n\nFile Handling (14xx)\n\n\n\n\n\n\n\n\nopenEO Error Code\n\n\nDescription\n\n\nMessage\n\n\nHTTP Status Code\n\n\n\n\n\n\n\n\n\n\n1401\n\n\nServer couldn't store file due to server-side reasons.\n\n\nUnable to store file.\n\n\n500\n\n\n\n\n\n\n1402\n\n\nThe storage quota has been exceeded by the user.\n\n\nInsufficient Storage.\n\n\n400\n\n\n\n\n\n\n1410\n\n\nFile format, file extension or mime type is not allowed.\n\n\nFile type not allowed.\n\n\n400\n\n\n\n\n\n\n1411\n\n\nFile exceeds allowed maximum file size.\n\n\nFile size it too large.\n\n\n400\n\n\n\n\n\n\n1412\n\n\nThe content of the file is invalid.\n\n\nFile content is invalid.\n\n\n400\n\n\n\n\n\n\n1413\n\n\nThe file is locked by a running job or another process.\n\n\nFile is locked.\n\n\n400\n\n\n\n\n\n\n\n\nProcess graphs (2xxx)\n\n\n\n\n\n\n\n\nopenEO Error Code\n\n\nDescription\n\n\nMessage\n\n\nHTTP Status Code\n\n\n\n\n\n\n\n\n\n\n2001\n\n\n\n\nNo process graph specified.\n\n\n400\n\n\n\n\n\n\n2002\n\n\n\n\nProcess graph structure is invalid.\n\n\n400\n\n\n\n\n\n\n2003\n\n\n\n\nThe array \nX\n contains values of multiple types.\n\n\n400\n\n\n\n\n\n\n2101\n\n\n\n\nProcess \nX\n is not supported.\n\n\n400\n\n\n\n\n\n\n2102\n\n\n\n\nProcess argument \nX\n is not supported.\n\n\n400\n\n\n\n\n\n\n2103\n\n\n\n\nInvalid value \nY\n for the process argument \nX\n specified.\n\n\n400\n\n\n\n\n\n\n2104\n\n\n\n\nRequired process argument \nX\n is missing.\n\n\n400\n\n\n\n\n\n\n\n\nJobs (3xxx)\n\n\n\n\n\n\n\n\nopenEO Error Code\n\n\nDescription\n\n\nMessage\n\n\nHTTP Status Code\n\n\n\n\n\n\n\n\n\n\n408\n\n\nThe (synchronous) request timed out.\n\n\nRequest timed out.\n\n\n408\n\n\n\n\n\n\n3001\n\n\n\n\nOutput format not supported.\n\n\n400\n\n\n\n\n\n\n3002\n\n\n\n\nOutput format argument \nX\n is not supported.\n\n\n400\n\n\n\n\n\n\n3003\n\n\n\n\nInvalid value \nY\n for the output format argument \nX\n specified.\n\n\n400\n\n\n\n\n\n\n3004\n\n\n\n\nData can't be transformed into the requested output format.\n\n\n400\n\n\n\n\n\n\n3005\n\n\nThe job is currently locked due to an enabled service or a running batch computation and can't be modified meanwhile.\n\n\nJob is locked.\n\n\n400\n\n\n\n\n\n\n3006\n\n\nThe job has not finished computing the results yet. Try again later.\n\n\nResults are not finished yet.\n\n\n400\n\n\n\n\n\n\n\n\nAuthorization, user content and billing (401-403, 4xxx)\n\n\n\n\n\n\n\n\nopenEO Error Code\n\n\nDescription\n\n\nMessage\n\n\nHTTP Status Code\n\n\n\n\n\n\n\n\n\n\n401\n\n\nThe back-end responds with this error code whenever the HTTP status code 401 is appropriate (see above) and no other openEO error code in the 4000 range is suitable.\n\n\nUnauthorized.\n\n\n401\n\n\n\n\n\n\n402\n\n\nThe budget required to fulfil the request are insufficient.\n\n\nPayment required.\n\n\n402\n\n\n\n\n\n\n403\n\n\nThe back-end responds with this error code whenever the HTTP status code 403 is appropriate (see above) and no other openEO error code in the 4000 range is suitable.\n\n\nForbidden.\n\n\n403\n\n\n\n\n\n\n4001\n\n\nThe specified password is not considered secure by the policy of the back-end provider or no password was given at all. The user needs to specify a different password to proceed.\n\n\nPassword does not meet the requirements.\n\n\n400\n\n\n\n\n\n\n4031\n\n\nInvalid authentication scheme (e.g. Bearer).\n\n\n\n\n403\n\n\n\n\n\n\n4032\n\n\nAuthorization token invalid or expired.\n\n\n\n\n403\n\n\n\n\n\n\n4033\n\n\n\n\nCredentials are not correct.\n\n\n403\n\n\n\n\n\n\n\n\nWeb services (5xxx)\n\n\n\n\n\n\n\n\nopenEO Error Code\n\n\nDescription\n\n\nMessage\n\n\nHTTP Status Code\n\n\n\n\n\n\n\n\n\n\n5001\n\n\n\n\nService type is not supported.\n\n\n400\n\n\n\n\n\n\n5101\n\n\nInvalid job id specified.\n\n\nJob does not exist.\n\n\n400\n\n\n\n\n\n\n5102\n\n\n\n\nService argument \nX\n is not supported.\n\n\n400\n\n\n\n\n\n\n5103\n\n\n\n\nInvalid value \nY\n for the service argument \nX\n specified.\n\n\n400\n\n\n\n\n\n\n5104\n\n\n\n\nRequired service argument \nX\n is missing.\n\n\n400",
            "title": "Error handling"
        },
        {
            "location": "/errors/index.html#status-and-error-handling",
            "text": "The success of requests MUST be indicated using  HTTP status codes  according to  RFC 7231 .  If the API responds with a status code between 100 and 399 the back-end indicates that the request has been handled successfully.  In general an error is communicated with a status code between 400 and 599. Client errors are defined as a client passing invalid data to the service and the service\u00a0 correctly  rejecting that data. Examples include invalid credentials, incorrect parameters, unknown versions, or similar. These are generally \"4xx\" HTTP error codes and are the result of a client passing incorrect or invalid data. Client errors do\u00a0 not \u00a0contribute to overall API availability.   Server errors are defined as the server failing to correctly return in response to a valid client request. These are generally \"5xx\" HTTP error codes. Server errors  do  contribute to the overall API availability. Calls that fail due to rate limiting or quota failures MUST NOT count as server errors.",
            "title": "Status and error handling"
        },
        {
            "location": "/errors/index.html#json-error-object",
            "text": "A JSON error object SHOULD be sent with all responses that have a status code between 400 and 599.  {\n  \"id\": \"936DA01F-9ABD-4D9D-80C7-02AF85C822A8\",\n  \"code\": 123,\n  \"message\": \"A sample error message.\",\n  \"url\": \"http://www.openeo.org/docs/errors/123\"\n}  Sending  code  and  message  is REQUIRED.     A back-end MAY add a free-form  id  (unique identifier) to the error response to be able to log and track errors with further non-disclosable details.    The  code  is either one of the standardized openEO error codes below or a proprietary error code with a number greater than 10000.    The  message  explains the reason the server is rejecting the request. For \"4xx\" error codes the message explains how the client needs to modify the request.    By default the message MUST be sent in English language. Content Negotiation is used to localize the error messages: If an  Acceppt-Language  header is sent by the client and a translation is available, the message should be translated accordingly and the  Content-Language  header must be present in the response. See \" How to localize your API \" for more information.   url  is an OPTIONAL attribute and contains a link to a resource that is explaining the error and potential solutions in-depth.",
            "title": "JSON error object"
        },
        {
            "location": "/errors/index.html#standardized-status-codes",
            "text": "The openEO API usually uses the following HTTP status codes for successful requests:    200 OK :\n  Indicates a successful request  with  a response body being sent.  201 Created \n  Indicates a successful request that successfully created a new resource. Sends a  Location  header to the newly created resource  without  a response body.  202 Accepted \n  Indicates a successful request that successfully queued the creation of a new resource, but it has not been created yet. The response is sent  without  a response body.  204 No Content :\n  Indicates a successful request  without  a response body being sent.   The openEO API often uses the following HTTP status codes for failed requests:     400 Bad request :\n  The back-end responds with this error code whenever the error has its origin on client side and no other HTTP status code in the 400 range is suitable.    401 Unauthorized :\n  The client  did not  provide any authorization details (usually using the Authorization header), but authorization is required for this request to be processed.    403 Forbidden :\n  The client  did  provide authorization details (usually using the Authorization header), but the provided credentials or the authorization token is invalid or has expired.    404 Not Found :\n  The resource specified by the path does not exist, i.e. one of the the resources belonging to the specified identifiers are not available at the back-end.\n   Note:  Unsupported endpoints MUST use HTTP status code 501.    500 Internal Server Error :\n  The error has its origin on server side and no other status code in the 500 range is suitable.    501 Not implemented :\n  An endpoint is specified in the openEO API, but is not supported.    If a HTTP status code in the 400 range is returned, the client SHOULD NOT repeat the request without modifications. For HTTP status code in the 500 range, the client MAY repeat the same request later.  All HTTP status codes defined in RFC 7231 in the 400 and 500 ranges can be used as openEO error code in addition to the most used status codes mentioned here. Responding with openEO error codes 400 and 500 SHOULD be avoided in favor of any more specific standardized or proprietary openEO error code.  General error codes (xxx)     openEO Error Code  Description  Message  HTTP Status Code      404  To be used if the value of a path parameter is invalid, i.e. the requested resource is not available.  Note:  Unsupported endpoints MUST use code 501.  Not Found.  404    501  The back-end responds with this error code whenever an endpoint is specified in the openEO API, but is not supported.  Not implemented.  501    503   Service unavailable.  503    601   Parameter  X  is invalid.  400    611  Invalid or unsupported CRS specified.  Invalid CRS specified.  400    612   Coordinate is out of bounds.  400     Capabilities (11xx)  None yet.  Data and process discovery (12xx)  None yet.  UDFs (13xx)     openEO Error Code  Description  Message  HTTP Status Code      1301   UDF programming language not supported.  400 / 404    1302   UDF type not supported.  400 / 404     File Handling (14xx)     openEO Error Code  Description  Message  HTTP Status Code      1401  Server couldn't store file due to server-side reasons.  Unable to store file.  500    1402  The storage quota has been exceeded by the user.  Insufficient Storage.  400    1410  File format, file extension or mime type is not allowed.  File type not allowed.  400    1411  File exceeds allowed maximum file size.  File size it too large.  400    1412  The content of the file is invalid.  File content is invalid.  400    1413  The file is locked by a running job or another process.  File is locked.  400     Process graphs (2xxx)     openEO Error Code  Description  Message  HTTP Status Code      2001   No process graph specified.  400    2002   Process graph structure is invalid.  400    2003   The array  X  contains values of multiple types.  400    2101   Process  X  is not supported.  400    2102   Process argument  X  is not supported.  400    2103   Invalid value  Y  for the process argument  X  specified.  400    2104   Required process argument  X  is missing.  400     Jobs (3xxx)     openEO Error Code  Description  Message  HTTP Status Code      408  The (synchronous) request timed out.  Request timed out.  408    3001   Output format not supported.  400    3002   Output format argument  X  is not supported.  400    3003   Invalid value  Y  for the output format argument  X  specified.  400    3004   Data can't be transformed into the requested output format.  400    3005  The job is currently locked due to an enabled service or a running batch computation and can't be modified meanwhile.  Job is locked.  400    3006  The job has not finished computing the results yet. Try again later.  Results are not finished yet.  400     Authorization, user content and billing (401-403, 4xxx)     openEO Error Code  Description  Message  HTTP Status Code      401  The back-end responds with this error code whenever the HTTP status code 401 is appropriate (see above) and no other openEO error code in the 4000 range is suitable.  Unauthorized.  401    402  The budget required to fulfil the request are insufficient.  Payment required.  402    403  The back-end responds with this error code whenever the HTTP status code 403 is appropriate (see above) and no other openEO error code in the 4000 range is suitable.  Forbidden.  403    4001  The specified password is not considered secure by the policy of the back-end provider or no password was given at all. The user needs to specify a different password to proceed.  Password does not meet the requirements.  400    4031  Invalid authentication scheme (e.g. Bearer).   403    4032  Authorization token invalid or expired.   403    4033   Credentials are not correct.  403     Web services (5xxx)     openEO Error Code  Description  Message  HTTP Status Code      5001   Service type is not supported.  400    5101  Invalid job id specified.  Job does not exist.  400    5102   Service argument  X  is not supported.  400    5103   Invalid value  Y  for the service argument  X  specified.  400    5104   Required service argument  X  is missing.  400",
            "title": "Standardized status codes"
        },
        {
            "location": "/cors/index.html",
            "text": "Cross-Origin Resource Sharing (CORS)\n\n\n\n\nCross-origin resource sharing (CORS) is a mechanism that allows restricted resources [...] on a web page to be requested from another domain outside the domain from which the first resource was served. [...]\nCORS defines a way in which a browser and server can interact to determine whether or not it is safe to allow the cross-origin request. It allows for more freedom and functionality than purely same-origin requests, but is more secure than simply allowing all cross-origin requests.\n\n\n\n\nSource: \nhttps://en.wikipedia.org/wiki/Cross-origin_resource_sharing\n\n\nopenEO-based back-ends are usually hosted on a different domain / host than the client that is requesting data from the back-end. Therefore most requests to the back-end are blocked by all modern browsers. This leads to the problem that the JavaScript library (and the Web Editor) can't access any back-end. Therefore, all back-end providers SHOULD support CORS. Without supporting CORS users can't access the back-end with browser-based clients, i.e. the \nJavaScript client\n. \nCORS is a recommendation of the W3C organization.\n The following chapters will explain how back-end providers can implement CORS support.\n\n\n1. Supporting the OPTIONS method\n\n\nAll endpoints must respond to the \nOPTIONS\n HTTP method. This is a response for the preflight requests made by the browsers. It needs to respond with a status code of \n204\n and send the HTTP headers shown in the table below. No body needs to be provided.\n\n\n\n\n\n\n\n\nName\n\n\nDescription\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nAccess-Control-Allow-Origin\n\n\nAllowed origin for the request, including protocol, host and port. It is RECOMMENDED to return the value of the request's origin header. If no \nOrigin\n is sent to the back-end CORS headers SCHOULD NOT be sent at all.\n\n\nhttp://client.isp.com:80\n\n\n\n\n\n\nAccess-Control-Allow-Credentials\n\n\nIf authorization is implemented by the back-end the value MUST be \ntrue\n.\n\n\ntrue\n\n\n\n\n\n\nAccess-Control-Allow-Headers\n\n\nComma-separated list of HTTP headers allowed to be send. MUST contain at least \nAuthorization\n if authorization is implemented by the back-end.\n\n\nAuthorization, Content-Type\n\n\n\n\n\n\nAccess-Control-Allow-Methods\n\n\nComma-separated list of HTTP methods allowed to be requested. Back-ends MUST list all implemented HTTP methods for the endpoint here.\n\n\nOPTIONS, GET, POST, PATCH, PUT, DELETE\n\n\n\n\n\n\nContent-Type\n\n\nSHOULD return the content type delivered by the request that the permission is requested for.\n\n\napplication/json\n\n\n\n\n\n\n\n\nExample request and response\n\n\nRequest:\n\n\nOPTIONS /api/v1/jobs HTTP/1.1\nHost: openeo.cloudprovider.com\nOrigin: http://client.org:8080\nAccess-Control-Request-Method: POST \nAccess-Control-Request-Headers: Authorization, Content-Type\n\n\n\n\nResponse:\n\n\nHTTP/1.1 204 No Content\nAccess-Control-Allow-Origin: http://client.org:8080\nAccess-Control-Allow-Credentials: true\nAccess-Control-Allow-Methods: OPTIONS, GET, POST, PATCH, PUT, DELETE\nAccess-Control-Allow-Headers: Authorization, Content-Type\nContent-Type: application/json\n\n\n\n\n2. Sending CORS headers for every endpoint\n\n\nThe following headers MUST be included with every response:\n\n\n\n\n\n\n\n\nName\n\n\nDescription\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nAccess-Control-Allow-Origin\n\n\nAllowed origin for the request, including protocol, host and port. It is RECOMMENDED to return the value of the request's origin header. If no \nOrigin\n is sent to the back-end CORS headers SHOULD NOT be sent at all.\n\n\nhttp://client.isp.com:80\n\n\n\n\n\n\nAccess-Control-Allow-Credentials\n\n\nIf authorization is implemented by the back-end the value MUST be \ntrue\n.\n\n\ntrue\n\n\n\n\n\n\n\n\nRemarks\n\n\nMost server can send the required headers and the responses to the OPTIONS requests globally. Otherwise you may want to use a proxy server to add the headers and OPTIONS responses.",
            "title": "CORS"
        },
        {
            "location": "/cors/index.html#cross-origin-resource-sharing-cors",
            "text": "Cross-origin resource sharing (CORS) is a mechanism that allows restricted resources [...] on a web page to be requested from another domain outside the domain from which the first resource was served. [...]\nCORS defines a way in which a browser and server can interact to determine whether or not it is safe to allow the cross-origin request. It allows for more freedom and functionality than purely same-origin requests, but is more secure than simply allowing all cross-origin requests.   Source:  https://en.wikipedia.org/wiki/Cross-origin_resource_sharing  openEO-based back-ends are usually hosted on a different domain / host than the client that is requesting data from the back-end. Therefore most requests to the back-end are blocked by all modern browsers. This leads to the problem that the JavaScript library (and the Web Editor) can't access any back-end. Therefore, all back-end providers SHOULD support CORS. Without supporting CORS users can't access the back-end with browser-based clients, i.e. the  JavaScript client .  CORS is a recommendation of the W3C organization.  The following chapters will explain how back-end providers can implement CORS support.",
            "title": "Cross-Origin Resource Sharing (CORS)"
        },
        {
            "location": "/cors/index.html#1-supporting-the-options-method",
            "text": "All endpoints must respond to the  OPTIONS  HTTP method. This is a response for the preflight requests made by the browsers. It needs to respond with a status code of  204  and send the HTTP headers shown in the table below. No body needs to be provided.     Name  Description  Example      Access-Control-Allow-Origin  Allowed origin for the request, including protocol, host and port. It is RECOMMENDED to return the value of the request's origin header. If no  Origin  is sent to the back-end CORS headers SCHOULD NOT be sent at all.  http://client.isp.com:80    Access-Control-Allow-Credentials  If authorization is implemented by the back-end the value MUST be  true .  true    Access-Control-Allow-Headers  Comma-separated list of HTTP headers allowed to be send. MUST contain at least  Authorization  if authorization is implemented by the back-end.  Authorization, Content-Type    Access-Control-Allow-Methods  Comma-separated list of HTTP methods allowed to be requested. Back-ends MUST list all implemented HTTP methods for the endpoint here.  OPTIONS, GET, POST, PATCH, PUT, DELETE    Content-Type  SHOULD return the content type delivered by the request that the permission is requested for.  application/json     Example request and response  Request:  OPTIONS /api/v1/jobs HTTP/1.1\nHost: openeo.cloudprovider.com\nOrigin: http://client.org:8080\nAccess-Control-Request-Method: POST \nAccess-Control-Request-Headers: Authorization, Content-Type  Response:  HTTP/1.1 204 No Content\nAccess-Control-Allow-Origin: http://client.org:8080\nAccess-Control-Allow-Credentials: true\nAccess-Control-Allow-Methods: OPTIONS, GET, POST, PATCH, PUT, DELETE\nAccess-Control-Allow-Headers: Authorization, Content-Type\nContent-Type: application/json",
            "title": "1. Supporting the OPTIONS method"
        },
        {
            "location": "/cors/index.html#2-sending-cors-headers-for-every-endpoint",
            "text": "The following headers MUST be included with every response:     Name  Description  Example      Access-Control-Allow-Origin  Allowed origin for the request, including protocol, host and port. It is RECOMMENDED to return the value of the request's origin header. If no  Origin  is sent to the back-end CORS headers SHOULD NOT be sent at all.  http://client.isp.com:80    Access-Control-Allow-Credentials  If authorization is implemented by the back-end the value MUST be  true .  true",
            "title": "2. Sending CORS headers for every endpoint"
        },
        {
            "location": "/cors/index.html#remarks",
            "text": "Most server can send the required headers and the responses to the OPTIONS requests globally. Otherwise you may want to use a proxy server to add the headers and OPTIONS responses.",
            "title": "Remarks"
        },
        {
            "location": "/usermanagement/index.html",
            "text": "User Management and Accounting\n\n\nIn general, the openEO API only defines a minimum subset of user management and accounting functionality. It allows to\n\n\n\n\nauthenticate and authorize\n a user, which may include \nuser registration with OpenID Connect\n,\n\n\nhandle storage space limits (disk quota),\n\n\nmanage billing, which includes to\n\n\nquery the credit a user has available,\n\n\nestimate costs for certain operations,\n\n\nget information about produced costs,\n\n\nlimit costs of certain operations.\n\n\n\n\nTherefore, the API leaves some aspects open that have to be handled by the back-ends separately, including \n\n\n\n\ncredential recovery, e.g. retrieving a forgotten password\n\n\nuser data management, e.g. changing the users payment details or email address\n\n\npayments, i.e. topping up credits for pre-paid services or paying for post-paid services\n\n\nother accounting related tasks, e.g. creating invoices,\n\n\nuser registration (only specified when OpenID Connect is implemented).",
            "title": "User Management and Accounting"
        },
        {
            "location": "/usermanagement/index.html#user-management-and-accounting",
            "text": "In general, the openEO API only defines a minimum subset of user management and accounting functionality. It allows to   authenticate and authorize  a user, which may include  user registration with OpenID Connect ,  handle storage space limits (disk quota),  manage billing, which includes to  query the credit a user has available,  estimate costs for certain operations,  get information about produced costs,  limit costs of certain operations.   Therefore, the API leaves some aspects open that have to be handled by the back-ends separately, including    credential recovery, e.g. retrieving a forgotten password  user data management, e.g. changing the users payment details or email address  payments, i.e. topping up credits for pre-paid services or paying for post-paid services  other accounting related tasks, e.g. creating invoices,  user registration (only specified when OpenID Connect is implemented).",
            "title": "User Management and Accounting"
        },
        {
            "location": "/jobs/index.html",
            "text": "Jobs\n\n\nAs described in the \nglossary\n, a \njob\n brings one process graph to the back-end and organizes its execution, which may or may not induce costs.\n\n\nPOST /jobs\n by default creates jobs to run computations \non demand\n, i.e. the requested data is calculated during the request. This is useful for web services where details like viewing extent or level of detail are not known in advance. Back-ends SHOULD make sure to cache processed data to avoid additional/high costs and waiting times for the user.\n\n\nResults can be pre-computed by creating one or multiple \nbatch jobs\n using  \nPOST /jobs/{job_id}/batches\n.  They are directly submitted to the back office's processing system. They will run only once, may include constraints, and will store results after execution. Batch jobs are typically time consuming such that user interaction is not possible.\n\n\nProcess graphs can also be \nexecuted  synchronously\n (\nPOST /jobs/previews\n). Results are delivered with the request itself and no job is created. Only lightweight computations, for example small previews, should be executed using this approach as timeouts are to be expected for \nlong-polling HTTP requests\n.\n\n\nExamples\n\n\nJobs are created by calling the endpoint \nPOST /jobs/{job_id}\n.\n\n\nUse case \n1\n and \n2\n are examples for normal jobs without prior batch processing.\n\n\nBatch jobs\n\n\nBatch jobs are created by calling the endpoint \nPOST /jobs/{job_id}/batches\n.\n\n\nAn explicit example for batch jobs is \nuse case 3\n.\n\n\nSynchronously executed jobs\n\n\nRetrieval of a GeoTIFF\n\n\nRequest\n\n\nHeader:\nPOST /jobs/previews HTTP/1.1\nContent-Type: application/json; charset=utf-8\n\nBody:\n{\n  \"process_graph\":{\n    \"process_id\":\"min_time\",\n    \"args\":{\n      \"imagery\":{\n        \"process_id\":\"NDVI\",\n        \"args\":{\n          \"imagery\":{\n            \"process_id\":\"filter_daterange\",\n            \"args\":{\n              \"imagery\":{\n                \"process_id\":\"filter_bbox\",\n                \"args\":{\n                  \"imagery\":{\n                    \"process_id\":\"get_data\",\n                    \"args\": {\n                      \"data_id\": \"S2_L2A_T32TPS_20M\"\n                    }\n                  },\n                  \"left\":652000,\n                  \"right\":672000,\n                  \"top\":5161000,\n                  \"bottom\":5181000,\n                  \"srs\":\"EPSG:32632\"\n                }\n              },\n              \"from\":\"2017-01-01\",\n              \"to\":\"2017-01-31\"\n            }\n          },\n          \"red\":\"B04\",\n          \"nir\":\"B8A\"\n        }\n      }\n    }\n  },\n  \"output\":{\n    \"format\":\"GTiff\",\n    \"args\": {\n      \"tiled\":true,\n      \"compress\":\"jpeg\",\n      \"photometric\":\"YCBCR\",\n      \"jpeg_quality\":80\n    }\n  }\n}\n\n\n\n\nResponse\n \n\n\nHeader:\nHTTP/1.1 200 OK\nContent-Type: image/tiff\nAccess-Control-Allow-Origin: <Origin>\n\nBody:\nomitted (the GeoTiff file contents)\n\n\n\n\nRetrieval of time series\n\n\nRequest\n\n\nHeader:\nPOST /jobs/previews HTTP/1.1\nContent-Type: application/json; charset=utf-8\n\nBody:\n{\n  \"process_graph\":{\n    \"process_id\":\"zonal_statistics\",\n    \"args\":{\n      \"imagery\":{\n        \"process_id\":\"filter_daterange\",\n        \"args\":{\n          \"imagery\":{\n            \"process_id\":\"filter_bbox\",\n            \"args\":{\n              \"imagery\":{\n                \"process_id\":\"filter_bands\",\n                \"args\":{\n                  \"imagery\":{\n                    \"process_id\": \"get_data\",\n                    \"args\": {\n                      \"data_id\":\"Sentinel2-L1C\"\n                    }\n                  },\n                  \"bands\":8\n                }\n              },\n              \"left\":16.1,\n              \"right\":16.6,\n              \"top\":48.6,\n              \"bottom\":47.2,\n              \"srs\":\"EPSG:4326\"\n            }\n          },\n          \"from\":\"2017-01-01\",\n          \"to\":\"2017-01-31\"\n        }\n      },\n      \"regions\":\"/users/me/files/\",\n      \"func\":\"avg\"\n    }\n  },\n  \"output\":{\n    \"format\":\"GPKG\"\n  }\n}\n\n\n\n\nResponse\n \n\n\nHeader:\nHTTP/1.1 200 OK\nContent-Type: application/octet-stream\nAccess-Control-Allow-Origin: <Origin>\n\nBody:\nomitted (the GeoPackage file contents)",
            "title": "Jobs"
        },
        {
            "location": "/jobs/index.html#jobs",
            "text": "As described in the  glossary , a  job  brings one process graph to the back-end and organizes its execution, which may or may not induce costs.  POST /jobs  by default creates jobs to run computations  on demand , i.e. the requested data is calculated during the request. This is useful for web services where details like viewing extent or level of detail are not known in advance. Back-ends SHOULD make sure to cache processed data to avoid additional/high costs and waiting times for the user.  Results can be pre-computed by creating one or multiple  batch jobs  using   POST /jobs/{job_id}/batches .  They are directly submitted to the back office's processing system. They will run only once, may include constraints, and will store results after execution. Batch jobs are typically time consuming such that user interaction is not possible.  Process graphs can also be  executed  synchronously  ( POST /jobs/previews ). Results are delivered with the request itself and no job is created. Only lightweight computations, for example small previews, should be executed using this approach as timeouts are to be expected for  long-polling HTTP requests .",
            "title": "Jobs"
        },
        {
            "location": "/jobs/index.html#examples",
            "text": "Jobs are created by calling the endpoint  POST /jobs/{job_id} .  Use case  1  and  2  are examples for normal jobs without prior batch processing.  Batch jobs  Batch jobs are created by calling the endpoint  POST /jobs/{job_id}/batches .  An explicit example for batch jobs is  use case 3 .  Synchronously executed jobs  Retrieval of a GeoTIFF  Request  Header:\nPOST /jobs/previews HTTP/1.1\nContent-Type: application/json; charset=utf-8\n\nBody:\n{\n  \"process_graph\":{\n    \"process_id\":\"min_time\",\n    \"args\":{\n      \"imagery\":{\n        \"process_id\":\"NDVI\",\n        \"args\":{\n          \"imagery\":{\n            \"process_id\":\"filter_daterange\",\n            \"args\":{\n              \"imagery\":{\n                \"process_id\":\"filter_bbox\",\n                \"args\":{\n                  \"imagery\":{\n                    \"process_id\":\"get_data\",\n                    \"args\": {\n                      \"data_id\": \"S2_L2A_T32TPS_20M\"\n                    }\n                  },\n                  \"left\":652000,\n                  \"right\":672000,\n                  \"top\":5161000,\n                  \"bottom\":5181000,\n                  \"srs\":\"EPSG:32632\"\n                }\n              },\n              \"from\":\"2017-01-01\",\n              \"to\":\"2017-01-31\"\n            }\n          },\n          \"red\":\"B04\",\n          \"nir\":\"B8A\"\n        }\n      }\n    }\n  },\n  \"output\":{\n    \"format\":\"GTiff\",\n    \"args\": {\n      \"tiled\":true,\n      \"compress\":\"jpeg\",\n      \"photometric\":\"YCBCR\",\n      \"jpeg_quality\":80\n    }\n  }\n}  Response    Header:\nHTTP/1.1 200 OK\nContent-Type: image/tiff\nAccess-Control-Allow-Origin: <Origin>\n\nBody:\nomitted (the GeoTiff file contents)  Retrieval of time series  Request  Header:\nPOST /jobs/previews HTTP/1.1\nContent-Type: application/json; charset=utf-8\n\nBody:\n{\n  \"process_graph\":{\n    \"process_id\":\"zonal_statistics\",\n    \"args\":{\n      \"imagery\":{\n        \"process_id\":\"filter_daterange\",\n        \"args\":{\n          \"imagery\":{\n            \"process_id\":\"filter_bbox\",\n            \"args\":{\n              \"imagery\":{\n                \"process_id\":\"filter_bands\",\n                \"args\":{\n                  \"imagery\":{\n                    \"process_id\": \"get_data\",\n                    \"args\": {\n                      \"data_id\":\"Sentinel2-L1C\"\n                    }\n                  },\n                  \"bands\":8\n                }\n              },\n              \"left\":16.1,\n              \"right\":16.6,\n              \"top\":48.6,\n              \"bottom\":47.2,\n              \"srs\":\"EPSG:4326\"\n            }\n          },\n          \"from\":\"2017-01-01\",\n          \"to\":\"2017-01-31\"\n        }\n      },\n      \"regions\":\"/users/me/files/\",\n      \"func\":\"avg\"\n    }\n  },\n  \"output\":{\n    \"format\":\"GPKG\"\n  }\n}  Response    Header:\nHTTP/1.1 200 OK\nContent-Type: application/octet-stream\nAccess-Control-Allow-Origin: <Origin>\n\nBody:\nomitted (the GeoPackage file contents)",
            "title": "Examples"
        },
        {
            "location": "/udfs/index.html",
            "text": "User-defined functions\n\n\nWork in progress!\n\n\nThe abbreviation \nUDF\n stands for \nuser-defined function\n. With this concept, users are able to upload custom code and have it executed e.g. for every pixel of a scene, allowing custom calculations on server-side data.\n\n\nUser-defined functions (UDFs) can be exposed to the data in different ways. This includes which parts of the data are passed to the function, how the function execution is parallelized, and how the expected output is structured.",
            "title": "UDFs"
        },
        {
            "location": "/udfs/index.html#user-defined-functions",
            "text": "Work in progress!  The abbreviation  UDF  stands for  user-defined function . With this concept, users are able to upload custom code and have it executed e.g. for every pixel of a scene, allowing custom calculations on server-side data.  User-defined functions (UDFs) can be exposed to the data in different ways. This includes which parts of the data are passed to the function, how the function execution is parallelized, and how the expected output is structured.",
            "title": "User-defined functions"
        },
        {
            "location": "/processgraphs/index.html",
            "text": "Process graphs\n\n\nA process graph includes specific process calls, i.e. references to one or more processes including specific values for input arguments similar to a function call in programming. However, process graphs can chain multiple processes. In particular, arguments of processes in general can be again (recursive) process graphs, input datasets, or simple scalar or array values.\n\n\nSchematic definition\n\n\nProcess\n\n\nA single process in a process graph is defined as follows:\n\n\n<Process> := {\n  \"process_id\": <string>,\n  \"description\": <string>,\n  \"args\": <ArgumentSet>\n}\n\n\n\n\nA process MUST always contain two key-value-pairs named \nprocess_id\n and \nargs\n and MAY contain a \ndescription\n, but MUST NOT hold other elements.\n\n\nprocess_id\n can currently contain three types of processes:\n\n\n\n\nBackend-defined processes, which are listed at \nGET /processes\n, e.g. \nfilter_bands\n.\n\n\nUser-defined process graphs, which are listed at \nGET /users/{user_id}/process_graphs\n. \n  They are prefixed with \n/user/\n, e.g. \n/user/my_process_graph\n.\n\n\nUser-defined functions (UDF), which is one of the predefined \nUDF types\n and can be explored at \nGET /udf_runtimes/{lang}/{udf_type}\n. UDFs are prefixed with \n/udf\n and additionally contain the runtime and the process name separated by \n/\n, e.g. \n/udf/Python/apply_pixel\n.\n\n\n\n\nArgument Set\n\n\nAn argument set for a process is defined as follows:\n\n\n<ArgumentSet> := {\n  <Key>: <Value>,\n  ...\n}\n\n\n\n\nThe key \n<Key>\n can be any valid JSON key, but it is RECOMMENDED to use \nsnake case\n and limit the characters to \na-z\n, \n0-9\n and the \n_\n.\n\n\nA value is defined as follows:\n\n\n<Value> := <string|number|array|boolean|null|Process>\n\n\n\n\nNote:\n string, number, array, boolean and null are the primitive data types supported by JSON. An array MUST always contain \none data type only\n and is allowed to contain the data types allowed for \n<Value>\n. In consequence, the objects allowed to be part of an array are processes only.\n\n\nNote:\n The expected names of arguments are defined by the process descriptions, which can be discovered with calls to \nGET /processes\n and \nGET /udf_runtimes/{lang}/{udf_type}\n. Therefore, the key name for a key-value-pair holding an image collection as value doesn't necessarily need to be named \nimagery\n. The name depends on the name of the corresponding process argument the image collection is assigned to. Example 2 demonstrates this by using \ncollection\n as a key once. \n\n\nExamples\n\n\nExample 1:\n A full process graph definition.\n\n\n{\n  \"process_id\":\"min_time\",\n  \"args\":{\n    \"imagery\":{\n      \"process_id\":\"/user/custom_ndvi\",\n      \"args\":{\n        \"imagery\":{\n          \"process_id\":\"filter_daterange\",\n          \"args\":{\n            \"imagery\":{\n              \"process_id\":\"filter_bbox\",\n              \"args\":{\n                \"imagery\":{\n                  \"process_id\":\"get_data\",\n                  \"args\": {\n                    \"data_id\": \"S2_L2A_T32TPS_20M\"\n                  }\n                },\n                \"left\":652000,\n                \"right\":672000,\n                \"top\":5161000,\n                \"bottom\":5181000,\n                \"srs\":\"EPSG:32632\"\n              }\n            },\n            \"from\":\"2017-01-01\",\n            \"to\":\"2017-01-31\"\n          }\n        },\n        \"red\":\"B04\",\n        \"nir\":\"B8A\"\n      }\n    }\n  }\n}\n\n\n\n\nExample 2:\n If a process needs multiple processes as input, it is allowed to use arrays of the respective types.\n\n\n{\n  \"imagery\":{\n    \"process_id\":\"union\",\n    \"args\":{\n      \"collection\":[\n        {\n          \"process_id\":\"filter_bands\",\n          \"args\":{\n            \"imagery\":{\n              \"process_id\":\"get_data\",\n              \"args\": {\n                \"data_id\": \"Sentinel2-L1C\"\n              }\n            },\n            \"bands\":8\n          }\n        },\n        {\n          \"process_id\":\"filter_bands\",\n          \"args\":{\n            \"imagery\":{\n              \"process_id\":\"get_data\",\n              \"args\": {\n                \"data_id\": \"Sentinel2-L1C\"\n              }\n            },\n            \"bands\":5\n          }\n        }\n      ]\n    }\n  }\n}\n\n\n\n\nCore processes\n\n\nThere are some processes that we define to be core processes that should be implemented by all back-ends:\n\n\n\n\nget_data\n\n\nfilter_bands\n\n\nfilter_daterange\n\n\nprocess_graph\n\n\nto be continued...\n\n\n\n\nNote:\n Currently there are only few defined processes. Those are currently only meant as an example how future documentation of processes may look like and to supplement the schematic definition above.\n\n\nLimitation:\n Process names (process ids) MUST never contain a forward slash \n/\n.\n\n\nget_data\n\n\nFilters and selects a single dataset provided by the back-end.\n\n\nArguments\n\n\nAny of the properties of a dataset, e.g.\n\n\n\n\ndata_id\n: Filter by data id\n\n\nextent\n: Filter by extent\n\n\ntime\n: Filter by time\n\n\nbands\n: Filter by band ids\n\n\n...\n\n\n\n\nThe back-end provider decides which of the potential datasets is the most relevant one to be selected.\n\n\nExamples\n\n\n{\n  \"process_id\": \"get_data\",\n  \"args\": {\n    \"data_id\":\"Sentinel2A-L1C\"\n  }\n}\n\n\n\n\n{\n  \"process_id\": \"get_data\",\n  \"args\": {\n    \"platform\": \"landsat-7\",\n    \"sensor\": \"modis\", \n    \"derived_from\": null\n  }\n}\n\n\n\n\nfilter_bands\n\n\nAllows to extract one or multiple bands of multi-band raster image collection. Bands can be chosen either by band id, band name or by wavelength.\n\n\nArguments\n\n\n\n\nimagery\n \n(required)\n: Image collection to filter\n\n\n\n\nAnd one of:\n\n\n\n\nbands\n: string or array of strings containing band ids.\n\n\nnames\n: string or array of strings containing band names.\n\n\nwavelengths\n: number or two-element array of numbers containing a wavelength or a minimum and maximum wavelength respectively.\n\n\n\n\nExamples\n\n\n{\n  \"process_id\": \"filter_bands\",\n  \"args\": {\n    \"imagery\":{\n      \"process_id\":\"get_data\",\n      \"args\": {\n        \"data_id\": \"Sentinel2A-L1C\"\n      }\n    },\n    \"bands\":1\n  }\n}\n\n\n\n\n{\n  \"process_id\": \"filter_bands\",\n  \"args\": {\n    \"imagery\":{\n      \"process_id\":\"get_data\",\n      \"args\": {\n        \"data_id\": \"Sentinel2A-L1C\"\n      }\n    },\n    \"wavelengths\":[1300,2000]\n  }\n}\n\n\n\n\nfilter_daterange\n\n\nAllows to filter an image collection by temporal extent.\n\n\nArguments\n\n\n\n\nimagery\n \n(required)\n: Image collection to filter\n\n\n\n\nAnd at least one of:\n\n\n\n\nfrom\n: Includes all data newer than the specified ISO 8601 date or date-time with simultaneous consideration of \nto\n.\n\n\nto\n: Includes all data older than the specified ISO 8601 date or date-time with simultaneous consideration of \nfrom\n.\n\n\n\n\nExamples\n\n\n{\n  \"process_id\":\"filter_daterange\",\n  \"args\":{\n    \"imagery\":{\n      \"process_id\":\"get_data\",\n      \"args\": {\n        \"data_id\": \"Sentinel2A-L1C\"\n      }\n    },\n    \"from\":\"2017-01-01\",\n    \"to\":\"2017-01-31\"\n  }\n}\n\n\n\n\nprocess_graph\n\n\nAnother process graph can be referenced with the process \nprocess_graph\n. This could even be an externally hosted process graph.\n\n\nArguments\n\n\n\n\nimagery\n \n(required)\n: Image collection to apply the process graph to\n\n\nuri\n \n(required)\n: An URI to a process graph.\n\n\n\n\nExamples\n\n\n{\n  \"process_id\":\"process_graph\",\n  \"args\":{\n    \"imagery\":{\n      \"process_id\":\"get_data\",\n      \"args\": {\n        \"data_id\": \"Sentinel2A-L1C\"\n      }\n    },\n    \"uri\":\"http://otherhost.org/api/v1/users/12345/process_graphs/abcdef\"\n  }\n}\n\n\n\n\nto be continued...",
            "title": "Process Graphs"
        },
        {
            "location": "/processgraphs/index.html#process-graphs",
            "text": "A process graph includes specific process calls, i.e. references to one or more processes including specific values for input arguments similar to a function call in programming. However, process graphs can chain multiple processes. In particular, arguments of processes in general can be again (recursive) process graphs, input datasets, or simple scalar or array values.",
            "title": "Process graphs"
        },
        {
            "location": "/processgraphs/index.html#schematic-definition",
            "text": "Process  A single process in a process graph is defined as follows:  <Process> := {\n  \"process_id\": <string>,\n  \"description\": <string>,\n  \"args\": <ArgumentSet>\n}  A process MUST always contain two key-value-pairs named  process_id  and  args  and MAY contain a  description , but MUST NOT hold other elements.  process_id  can currently contain three types of processes:   Backend-defined processes, which are listed at  GET /processes , e.g.  filter_bands .  User-defined process graphs, which are listed at  GET /users/{user_id}/process_graphs . \n  They are prefixed with  /user/ , e.g.  /user/my_process_graph .  User-defined functions (UDF), which is one of the predefined  UDF types  and can be explored at  GET /udf_runtimes/{lang}/{udf_type} . UDFs are prefixed with  /udf  and additionally contain the runtime and the process name separated by  / , e.g.  /udf/Python/apply_pixel .   Argument Set  An argument set for a process is defined as follows:  <ArgumentSet> := {\n  <Key>: <Value>,\n  ...\n}  The key  <Key>  can be any valid JSON key, but it is RECOMMENDED to use  snake case  and limit the characters to  a-z ,  0-9  and the  _ .  A value is defined as follows:  <Value> := <string|number|array|boolean|null|Process>  Note:  string, number, array, boolean and null are the primitive data types supported by JSON. An array MUST always contain  one data type only  and is allowed to contain the data types allowed for  <Value> . In consequence, the objects allowed to be part of an array are processes only.  Note:  The expected names of arguments are defined by the process descriptions, which can be discovered with calls to  GET /processes  and  GET /udf_runtimes/{lang}/{udf_type} . Therefore, the key name for a key-value-pair holding an image collection as value doesn't necessarily need to be named  imagery . The name depends on the name of the corresponding process argument the image collection is assigned to. Example 2 demonstrates this by using  collection  as a key once.   Examples  Example 1:  A full process graph definition.  {\n  \"process_id\":\"min_time\",\n  \"args\":{\n    \"imagery\":{\n      \"process_id\":\"/user/custom_ndvi\",\n      \"args\":{\n        \"imagery\":{\n          \"process_id\":\"filter_daterange\",\n          \"args\":{\n            \"imagery\":{\n              \"process_id\":\"filter_bbox\",\n              \"args\":{\n                \"imagery\":{\n                  \"process_id\":\"get_data\",\n                  \"args\": {\n                    \"data_id\": \"S2_L2A_T32TPS_20M\"\n                  }\n                },\n                \"left\":652000,\n                \"right\":672000,\n                \"top\":5161000,\n                \"bottom\":5181000,\n                \"srs\":\"EPSG:32632\"\n              }\n            },\n            \"from\":\"2017-01-01\",\n            \"to\":\"2017-01-31\"\n          }\n        },\n        \"red\":\"B04\",\n        \"nir\":\"B8A\"\n      }\n    }\n  }\n}  Example 2:  If a process needs multiple processes as input, it is allowed to use arrays of the respective types.  {\n  \"imagery\":{\n    \"process_id\":\"union\",\n    \"args\":{\n      \"collection\":[\n        {\n          \"process_id\":\"filter_bands\",\n          \"args\":{\n            \"imagery\":{\n              \"process_id\":\"get_data\",\n              \"args\": {\n                \"data_id\": \"Sentinel2-L1C\"\n              }\n            },\n            \"bands\":8\n          }\n        },\n        {\n          \"process_id\":\"filter_bands\",\n          \"args\":{\n            \"imagery\":{\n              \"process_id\":\"get_data\",\n              \"args\": {\n                \"data_id\": \"Sentinel2-L1C\"\n              }\n            },\n            \"bands\":5\n          }\n        }\n      ]\n    }\n  }\n}",
            "title": "Schematic definition"
        },
        {
            "location": "/processgraphs/index.html#core-processes",
            "text": "There are some processes that we define to be core processes that should be implemented by all back-ends:   get_data  filter_bands  filter_daterange  process_graph  to be continued...   Note:  Currently there are only few defined processes. Those are currently only meant as an example how future documentation of processes may look like and to supplement the schematic definition above.  Limitation:  Process names (process ids) MUST never contain a forward slash  / .  get_data  Filters and selects a single dataset provided by the back-end.  Arguments  Any of the properties of a dataset, e.g.   data_id : Filter by data id  extent : Filter by extent  time : Filter by time  bands : Filter by band ids  ...   The back-end provider decides which of the potential datasets is the most relevant one to be selected.  Examples  {\n  \"process_id\": \"get_data\",\n  \"args\": {\n    \"data_id\":\"Sentinel2A-L1C\"\n  }\n}  {\n  \"process_id\": \"get_data\",\n  \"args\": {\n    \"platform\": \"landsat-7\",\n    \"sensor\": \"modis\", \n    \"derived_from\": null\n  }\n}  filter_bands  Allows to extract one or multiple bands of multi-band raster image collection. Bands can be chosen either by band id, band name or by wavelength.  Arguments   imagery   (required) : Image collection to filter   And one of:   bands : string or array of strings containing band ids.  names : string or array of strings containing band names.  wavelengths : number or two-element array of numbers containing a wavelength or a minimum and maximum wavelength respectively.   Examples  {\n  \"process_id\": \"filter_bands\",\n  \"args\": {\n    \"imagery\":{\n      \"process_id\":\"get_data\",\n      \"args\": {\n        \"data_id\": \"Sentinel2A-L1C\"\n      }\n    },\n    \"bands\":1\n  }\n}  {\n  \"process_id\": \"filter_bands\",\n  \"args\": {\n    \"imagery\":{\n      \"process_id\":\"get_data\",\n      \"args\": {\n        \"data_id\": \"Sentinel2A-L1C\"\n      }\n    },\n    \"wavelengths\":[1300,2000]\n  }\n}  filter_daterange  Allows to filter an image collection by temporal extent.  Arguments   imagery   (required) : Image collection to filter   And at least one of:   from : Includes all data newer than the specified ISO 8601 date or date-time with simultaneous consideration of  to .  to : Includes all data older than the specified ISO 8601 date or date-time with simultaneous consideration of  from .   Examples  {\n  \"process_id\":\"filter_daterange\",\n  \"args\":{\n    \"imagery\":{\n      \"process_id\":\"get_data\",\n      \"args\": {\n        \"data_id\": \"Sentinel2A-L1C\"\n      }\n    },\n    \"from\":\"2017-01-01\",\n    \"to\":\"2017-01-31\"\n  }\n}  process_graph  Another process graph can be referenced with the process  process_graph . This could even be an externally hosted process graph.  Arguments   imagery   (required) : Image collection to apply the process graph to  uri   (required) : An URI to a process graph.   Examples  {\n  \"process_id\":\"process_graph\",\n  \"args\":{\n    \"imagery\":{\n      \"process_id\":\"get_data\",\n      \"args\": {\n        \"data_id\": \"Sentinel2A-L1C\"\n      }\n    },\n    \"uri\":\"http://otherhost.org/api/v1/users/12345/process_graphs/abcdef\"\n  }\n}  to be continued...",
            "title": "Core processes"
        },
        {
            "location": "/views/index.html",
            "text": "Constraints\n\n\nThe openEO API supports to restrict datasets by different dimensions and provides basic information to align datasets with different characteristics. This feature is called 'constraints', formerly known as 'data views'. They describe at which resolution and for which spatial and temporal extent the original Earth observation data are processed and hence can be used to run processes interactively on small parts of the original data without need to wait for long-running processes. \n\n\nHeterogeneous datasets are unified by the back-ends based on the specified constraints. For instance, the difference between a PROBA-V image and a Sentinel image, which have e a different projection and resolution, are automatically resampled and projected by the back-ends as soon as it is required to do so. Except specifying reasonable constraints, clients are not responsible to ensure that the data matches by first applying resampling or projections processes.\n\n\nTemporal references are always specified on the basis of the \nGregorian calendar\n. Therefore a temporal reference system can't be specified in the constraints object.\n\n\nExample\n\n\nThe following JSON object describes a coarse resolution (0.25\u00b0 x 0.25\u00b0) view of monthly aggregated data. \n\n\n{\n  \"spatial\":{\n    \"extent\":{\n      \"crs\":\"EPSG:4326\",\n      \"left\":-10.21,\n      \"top\":53.23,\n      \"right\":12.542,\n      \"bottom\":12.32\n    },\n    \"resolution\":0.25,\n    \"resampling\":\"nearest\",\n    \"crs\":\"EPSG:4326\"\n  },\n  \"temporal\":{\n    \"extent\":{\n      \"start\":\"2017-01-01\",\n      \"end\":\"2018-01-01\"\n    },\n    \"resolution\":\"P1M\",\n    \"resampling\":\"mean\"\n  }\n}",
            "title": "Constraints"
        },
        {
            "location": "/views/index.html#constraints",
            "text": "The openEO API supports to restrict datasets by different dimensions and provides basic information to align datasets with different characteristics. This feature is called 'constraints', formerly known as 'data views'. They describe at which resolution and for which spatial and temporal extent the original Earth observation data are processed and hence can be used to run processes interactively on small parts of the original data without need to wait for long-running processes.   Heterogeneous datasets are unified by the back-ends based on the specified constraints. For instance, the difference between a PROBA-V image and a Sentinel image, which have e a different projection and resolution, are automatically resampled and projected by the back-ends as soon as it is required to do so. Except specifying reasonable constraints, clients are not responsible to ensure that the data matches by first applying resampling or projections processes.  Temporal references are always specified on the basis of the  Gregorian calendar . Therefore a temporal reference system can't be specified in the constraints object.",
            "title": "Constraints"
        },
        {
            "location": "/views/index.html#example",
            "text": "The following JSON object describes a coarse resolution (0.25\u00b0 x 0.25\u00b0) view of monthly aggregated data.   {\n  \"spatial\":{\n    \"extent\":{\n      \"crs\":\"EPSG:4326\",\n      \"left\":-10.21,\n      \"top\":53.23,\n      \"right\":12.542,\n      \"bottom\":12.32\n    },\n    \"resolution\":0.25,\n    \"resampling\":\"nearest\",\n    \"crs\":\"EPSG:4326\"\n  },\n  \"temporal\":{\n    \"extent\":{\n      \"start\":\"2017-01-01\",\n      \"end\":\"2018-01-01\"\n    },\n    \"resolution\":\"P1M\",\n    \"resampling\":\"mean\"\n  }\n}",
            "title": "Example"
        },
        {
            "location": "/apireference/index.html",
            "text": "this is a placeholder file that will be replaced by generated docs of the OpenAPI spec automatically",
            "title": "API Reference"
        },
        {
            "location": "/poc/index.html",
            "text": "Proof of Concept (API v0.0.2)\n\n\nThis page gives a detailed description of the openEO proof of concept and gives a list and specification of what needs to be implemented. The proof of concept will consist of\n\n\n\n\nat least three clearly defined example processes (see below),\n\n\na prototypical API specification including communication API call sequences of the processes (see below),\n\n\nimplementations of the processes on three back-ends, and\n\n\nprototypical clients in R, Python and potentially JavaScript.\n\n\n\n\nBelow, we define example use cases and how they are translated to sequences of API calls:\n\n\n\n\nDeriving minimum NDVI measurements over pixel time series of Sentinel 2 imagery\n\n\nCreate a monthly aggregated Sentinel 1 product from a custom Python script\n\n\nCompute time series of zonal (regional) statistics of Sentinel 2 imagery over user-uploaded polygons\n\n\n\n\nNote:\n Authentication is not included in these examples. Enabling authentication needs the placeholder \n<Origin>\n to be set to the requesting host, including protocol, host name/IP and port, e.g. \nhttp://localhost:8080\n. This could be done by using the Origin header value from the request.\n\n\nUse Case 1\n\n\nDeriving minimum NDVI measurements over pixel time series of Sentinel 2 imagery.\n\n\n1. Check whether Sentinel 2A Level 1C data is available at the back-end\n\n\nRequest\n\n\nGET /data/Sentinel2A-L1C HTTP/1.1\n\n\n\n\nResponse\n\n\nHeader:\nHTTP/1.1 200 OK\nContent-Type: application/json; charset=utf-8\nAccess-Control-Allow-Origin: <Origin>\nAccess-Control-Allow-Credentials: true\n\nBody:\n{\n  \"product_id\":\"Sentinel-2A-L1C\",\n  \"description\":\"Sentinel 2 Level-1C: Top-of-atmosphere reflectances in cartographic geometry\",\n  \"source\":\"European Space Agency (ESA)\",\n  \"extent\":[\n    -34,\n    35,\n    39,\n    71\n  ],\n  \"time\":[\n    \"2016-01-01\",\n    \"2017-10-01\"\n  ],\n  \"bands\":[\n    {\n      \"band_id\":\"1\",\n      \"wavelength_nm\":443.9,\n      \"res_m\":60,\n      \"scale\":0.0001,\n      \"offset\":0,\n      \"type\":\"int16\",\n      \"unit\":\"1\"\n    },\n    {\n      \"band_id\":\"2\",\n      \"name\":\"blue\",\n      \"wavelength_nm\":496.6,\n      \"res_m\":10,\n      \"scale\":0.0001,\n      \"offset\":0,\n      \"type\":\"int16\",\n      \"unit\":\"1\"\n    },\n    {\n      \"band_id\":\"3\",\n      \"name\":\"green\",\n      \"wavelength_nm\":560,\n      \"res_m\":10,\n      \"scale\":0.0001,\n      \"offset\":0,\n      \"type\":\"int16\",\n      \"unit\":\"1\"\n    },\n    {\n      \"band_id\":\"4\",\n      \"name\":\"red\",\n      \"wavelength_nm\":664.5,\n      \"res_m\":10,\n      \"scale\":0.0001,\n      \"offset\":0,\n      \"type\":\"int16\",\n      \"unit\":\"1\"\n    },\n    {\n      \"band_id\":\"5\",\n      \"wavelength_nm\":703.9,\n      \"res_m\":20,\n      \"scale\":0.0001,\n      \"offset\":0,\n      \"type\":\"int16\",\n      \"unit\":\"1\"\n    },\n    {\n      \"band_id\":\"6\",\n      \"wavelength_nm\":740.2,\n      \"res_m\":20,\n      \"scale\":0.0001,\n      \"offset\":0,\n      \"type\":\"int16\",\n      \"unit\":\"1\"\n    },\n    {\n      \"band_id\":\"7\",\n      \"wavelength_nm\":782.5,\n      \"res_m\":20,\n      \"scale\":0.0001,\n      \"offset\":0,\n      \"type\":\"int16\",\n      \"unit\":\"1\"\n    },\n    {\n      \"band_id\":\"8\",\n      \"name\":\"nir\",\n      \"wavelength_nm\":835.1,\n      \"res_m\":10,\n      \"scale\":0.0001,\n      \"offset\":0,\n      \"type\":\"int16\",\n      \"unit\":\"1\"\n    },\n    {\n      \"band_id\":\"8a\",\n      \"wavelength_nm\":864.8,\n      \"res_m\":20,\n      \"scale\":0.0001,\n      \"offset\":0,\n      \"type\":\"int16\",\n      \"unit\":\"1\"\n    },\n    {\n      \"band_id\":\"9\",\n      \"wavelength_nm\":945,\n      \"res_m\":60,\n      \"scale\":0.0001,\n      \"offset\":0,\n      \"type\":\"int16\",\n      \"unit\":\"1\"\n    },\n    {\n      \"band_id\":\"10\",\n      \"wavelength_nm\":1373.5,\n      \"res_m\":60,\n      \"scale\":0.0001,\n      \"offset\":0,\n      \"type\":\"int16\",\n      \"unit\":\"1\"\n    },\n    {\n      \"band_id\":\"11\",\n      \"wavelength_nm\":1613.7,\n      \"res_m\":20,\n      \"scale\":0.0001,\n      \"offset\":0,\n      \"type\":\"int16\",\n      \"unit\":\"1\"\n    },\n    {\n      \"band_id\":\"12\",\n      \"wavelength_nm\":2202.4,\n      \"res_m\":20,\n      \"scale\":0.0001,\n      \"offset\":0,\n      \"type\":\"int16\",\n      \"unit\":\"1\"\n    }\n  ]\n}\n\n\n\n\n2. Check that needed processes are available\n\n\nRequest\n\n\nGET /processes/filter_bbox HTTP/1.1\n\n\n\n\nResponse\n\n\nHeader:\nHTTP/1.1 200 OK\nContent-Type: application/json; charset=utf-8\nAccess-Control-Allow-Origin: <Origin>\nAccess-Control-Allow-Credentials: true\n\nBody:\n{\n  \"process_id\":\"filter_bbox\",\n  \"description\":\"Drops observations from a collection that are located outside of a given bounding box.\",\n  \"args\":{\n    \"imagery\":{\n      \"description\":\"array of input collections with one element\"\n    },\n    \"left\":{\n      \"description\":\"left boundary (longitude / easting)\"\n    },\n    \"right\":{\n      \"description\":\"right boundary (longitude / easting)\"\n    },\n    \"top\":{\n      \"description\":\"top boundary (latitude / northing)\"\n    },\n    \"bottom\":{\n      \"description\":\"bottom boundary (latitude / northing)\"\n    },\n    \"srs\":{\n      \"description\":\"spatial reference system of boundaries as proj4 or EPSG:12345 like string\"\n    }\n  }\n}\n\n\n\n\nRequest\n\n\nGET /processes/filter_daterange HTTP/1.1\n\n\n\n\nResponse\n\n\nHeader:\nHTTP/1.1 200 OK\nContent-Type: application/json; charset=utf-8\nAccess-Control-Allow-Origin: <Origin>\nAccess-Control-Allow-Credentials: true\n\nBody:\n{\n  \"process_id\":\"filter_daterange\",\n  \"description\":\"Drops observations from a collection that have been captured before a start or after a given end date.\",\n  \"args\":{\n    \"imagery\":{\n      \"description\":\"array of input collections with one element\"\n    },\n    \"from\":{\n      \"description\":\"start date\"\n    },\n    \"to\":{\n      \"description\":\"end date\"\n    }\n  }\n}\n\n\n\n\nRequest\n\n\nGET /processes/NDVI HTTP/1.1\n\n\n\n\nResponse\n\n\nHeader:\nHTTP/1.1 200 OK\nContent-Type: application/json; charset=utf-8\nAccess-Control-Allow-Origin: <Origin>\nAccess-Control-Allow-Credentials: true\n\nBody:\n{\n  \"process_id\":\"NDVI\",\n  \"description\":\"Finds the minimum value of time series for all bands of the input dataset.\",\n  \"args\":{\n    \"imagery\":{\n      \"description\":\"array of input collections with one element\"\n    },\n    \"red\":{\n      \"description\":\"reference to the red band\"\n    },\n    \"nir\":{\n      \"description\":\"reference to the nir band\"\n    }\n  }\n}\n\n\n\n\nRequest\n\n\nGET /processes/min_time HTTP/1.1\n\n\n\n\nResponse\n\n\nHeader:\nHTTP/1.1 200 OK\nContent-Type: application/json; charset=utf-8\nAccess-Control-Allow-Origin: <Origin>\nAccess-Control-Allow-Credentials: true\n\nBody:\n{\n  \"process_id\":\"min_time\",\n  \"description\":\"Finds the minimum value of time series for all bands of the input dataset.\",\n  \"args\":{\n    \"imagery\":{\n      \"description\":\"array of input collections with one element\"\n    }\n  }\n}\n\n\n\n\n3. Create a job at the back-end\n\n\nRequest\n\n\nPOST /jobs HTTP/1.1\nContent-Type: application/json; charset=utf-8\n\nBody:\n{\n  \"process_graph\":{\n    \"process_id\":\"min_time\",\n    \"args\":{\n      \"imagery\":{\n        \"process_id\":\"NDVI\",\n        \"args\":{\n          \"imagery\":{\n            \"process_id\":\"filter_daterange\",\n            \"args\":{\n              \"imagery\":{\n                \"process_id\":\"filter_bbox\",\n                \"args\":{\n                  \"imagery\":{\n                    \"product_id\":\"S2_L2A_T32TPS_20M\"\n                  },\n                  \"left\":652000,\n                  \"right\":672000,\n                  \"top\":5161000,\n                  \"bottom\":5181000,\n                  \"srs\":\"EPSG:32632\"\n                }\n              },\n              \"from\":\"2017-01-01\",\n              \"to\":\"2017-01-31\"\n            }\n          },\n          \"red\":\"B04\",\n          \"nir\":\"B8A\"\n        }\n      }\n    }\n  }\n}\n\n\n\n\nResponse\n\n\nHeader:\nHTTP/1.1 200 OK\nContent-Type: application/json; charset=utf-8\nAccess-Control-Allow-Origin: <Origin>\nAccess-Control-Allow-Credentials: true\n\nBody:\n{\n  \"job_id\":\"2a8ffb20c2b235a3f3e3351f\",\n  \"status\":\"submitted\",\n  \"submitted\":\"2017-01-01T09:32:12Z\",\n  \"updated\":\"2017-01-01T09:36:18Z\",\n  \"user_id\":\"bd6f9faf93b4\",\n  \"consumed_credits\":0\n}\n\n\n\n\n4. Create a WCS service\n\n\nRequest\n\n\nPOST /services HTTP/1.1\nContent-Type: application/json; charset=utf-8\n\nBody:\n{\n  \"job_id\":\"2a8ffb20c2b235a3f3e3351f\",\n  \"type\":\"wcs\",\n  \"args\":{\n    \"VERSION\":\"2.0.1\"\n  }\n}\n\n\n\n\nResponse\n\n\nHeader:\nHTTP/1.1 200 OK\nContent-Type: application/json; charset=utf-8\nAccess-Control-Allow-Origin: <Origin>\nAccess-Control-Allow-Credentials: true\n\nBody:\n{\n  \"service_id\":\"4dab456f6501bbcd\",\n  \"service_url\":\"https://openeo.org/4dab456f6501bbcd/wcs\",\n  \"service_type\":\"wcs\",\n  \"service_args\":{\n    \"VERSION\":\"2.0.1\"\n  },\n  \"job_id\":\"2a8ffb20c2b235a3f3e3351f\"\n}\n\n\n\n\n5. Download the data on demand with WCS\n\n\nRequest\n\n\nGET https://openeo.org/4dab456f6501bbcd/wcs?SERVICE=WCS&VERSION=2.0.1&REQUEST=GetCapabilities HTTP/1.1\n\n\n\n\nResponse\n\n\nomitted\n\n\nRequest\n\n\nGET https://openeo.org/4dab456f6501bbcd/wcs?SERVICE=WCS&VERSION=2.0.1&REQUEST=GetCoverage&COVERAGEID=2a8ffb20c2b235a3f3e3351f&FORMAT=image/tiff&SUBSET=x,http://www.opengis.net/def/crs/EPSG/0/4326(16.1,16.5)&SUBSET=y,http://www.opengis.net/def/crs/EPSG/0/4326(47.9,48.6)&&SIZE=x(200)&SIZE=y(200) HTTP/1.1\n\n\n\n\nResponse\n \n\nomitted\n\n\n6. Stop the job (and the service)\n\n\nRequest\n\n\nPATCH /jobs/2a8ffb20c2b235a3f3e3351f/cancel HTTP/1.1\n\n\n\n\nResponse\n\n\nHeader:\nHTTP/1.1 200 OK\nAccess-Control-Allow-Origin: <Origin>\nAccess-Control-Allow-Credentials: true\n\nBody: none\n\n\n\n\nUse Case 2\n\n\nCreate a monthly aggregated Sentinel 1 product from a custom Python script.\n\n\n1. Ask the back-end for available Sentinel 1 data\n\n\nRequest\n\n\nGET /data/Sentinel1-L1-IW-GRD HTTP/1.1\n\n\n\n\nResponse\n\n\nHeader:\nHTTP/1.1 200 OK\nContent-Type: application/json; charset=utf-8\nAccess-Control-Allow-Origin: <Origin>\nAccess-Control-Allow-Credentials: true\n\nBody:\n{\n  \"product_id\":\"Sentinel1-L1-IW-GRD\",\n  \"description\":\"Sentinel 1 C-band Synthetic Aperture Radar (SAR) Ground Range Data\",\n  \"source\":\"European Space Agency (ESA)\",\n  \"extent\":[\n    -34,\n    35,\n    39,\n    71\n  ],\n  \"time\":[\n    \"2016-01-01\",\n    \"2017-10-01\"\n  ],\n  \"bands\":[\n    {\n      \"band_id\":\"VV\"\n    },\n    {\n      \"band_id\":\"VH\"\n    }\n  ]\n}\n\n\n\n\n2. Ask the back-end whether it supports Python UDFs of type aggregate_time and get details about expected parameters\n\n\nRequest\n\n\nGET /udf_runtimes HTTP/1.1\n\n\n\n\nResponse\n\n\nHeader:\nHTTP/1.1 200 OK\nContent-Type: application/json; charset=utf-8\nAccess-Control-Allow-Origin: <Origin>\nAccess-Control-Allow-Credentials: true\n\nBody:\n{\n  \"Python\":{\n    \"udf_types\":[\n      \"reduce_time\",\n      \"aggregate_time\",\n      \"apply_pixel\"\n    ],\n    \"versions\":{\n      \"3.6.3\":{\n        \"packages\":[\n          \"numpy\",\n          \"scipy\",\n          \"pandas\",\n          \"matplotlib\",\n          \"ipython\",\n          \"jupyter\",\n          \"GDAL\"\n        ]\n      }\n    }\n  }\n}\n\n\n\n\nRequest\n\n\nGET /udf_runtimes/Python/aggregate_time HTTP/1.1\n\n\n\n\nResponse\n\n\nHeader:\nHTTP/1.1 200 OK\nContent-Type: application/json; charset=utf-8\nAccess-Control-Allow-Origin: <Origin>\nAccess-Control-Allow-Credentials: true\n\n{\n  \"process_id\":\"/udf/Python/aggregate_time\",\n  \"description\":\"Runs a Python script for each time series of the input dataset.\",\n  \"args\":{\n    \"imagery\":{\n      \"description\":\"array of input collections with one element\"\n    },\n    \"script\":{\n      \"description\":\"Python script that will be executed over all time series, gets time series as (Pandas) DataFrame and expects a new DataFrame as output.\"\n    },\n    \"version\":{\n      \"description\":\"Python version to use, defaults to the latest available version.\",\n      \"required\":false,\n      \"default\":\"latest\"\n    }\n  }\n}\n\n\n\n\n3. Upload python script\n\n\nRequest\n\n\nPUT /users/me/files/s1_aggregate.py HTTP/1.1\n\n\n\n\nResponse\n\n\nHeader:\nHTTP/1.1 200 OK\nAccess-Control-Allow-Origin: <Origin>\nAccess-Control-Allow-Credentials: true\n\nBody: none\n\n\n\n\n4. Create a job\n\n\nRequest\n\n\nPOST /jobs HTTP/1.1\nContent-Type: application/json; charset=utf-8\n\nBody:\n{\n  \"process_graph\":{\n    \"process_id\":\"/udf/Python/aggregate_time\",\n    \"args\":{\n      \"script\":\"/users/me/files/s1_aggregate.py\",\n      \"imagery\":{\n        \"process_id\":\"filter_daterange\",\n        \"args\":{\n          \"imagery\":{\n            \"process_id\":\"filter_bbox\",\n            \"args\":{\n              \"imagery\":{\n                \"product_id\":\"Sentinel1-L1-IW-GRD\"\n              },\n              \"left\":16.1,\n              \"right\":16.6,\n              \"top\":48.6,\n              \"bottom\":47.2,\n              \"srs\":\"EPSG:4326\"\n            }\n          },\n          \"from\":\"2017-01-01\",\n          \"to\":\"2017-01-31\"\n        }\n      }\n    }\n  }\n}\n\n\n\n\nResponse\n\n\nHeader:\nHTTP/1.1 200 OK\nContent-Type: application/json; charset=utf-8\nAccess-Control-Allow-Origin: <Origin>\nAccess-Control-Allow-Credentials: true\n\nBody:\n{\n  \"job_id\":\"3723c32fb7b24698832ca71f2d3f18aa\",\n  \"status\":\"submitted\",\n  \"submitted\":\"2017-01-01T09:32:12Z\",\n  \"updated\":\"2017-01-01T09:36:18Z\",\n  \"user_id\":\"bd6f9faf93b4\",\n  \"consumed_credits\":0\n}\n\n\n\n\n5. Create a TMS service\n\n\nRequest\n\n\nPOST /services HTTP/1.1\nContent-Type: application/json; charset=utf-8\n\nBody:\n{\n  \"job_id\":\"3723c32fb7b24698832ca71f2d3f18aa\",\n  \"type\":\"tms\"\n}\n\n\n\n\nResponse\n\n\nHeader:\nHTTP/1.1 200 OK\nContent-Type: application/json; charset=utf-8\nAccess-Control-Allow-Origin: <Origin>\nAccess-Control-Allow-Credentials: true\n\nBody:\n{\n  \"service_id\":\"9dab4b6f6523\",\n  \"service_url\":\"http://cdn.cloudprovider.com/openeo/services/9dab4b6f6523/tms\",\n  \"service_type\":\"tms\",\n  \"job_id\":\"3723c32fb7b24698832ca71f2d3f18aa\"\n}\n\n\n\n\n6. Download results as TMS\n\n\nExample Request\n\n\nGET http://cdn.cloudprovider.com/openeo/services/9dab4b6f6523/tms/2017-01-01/12/2232/2668/?bands=1 HTTP/1.1\n\n\n\n\nResponse\n\n\nomitted\n\n\nUse Case 3\n\n\nCompute time series of zonal (regional) statistics of Sentinel 2 imagery over user-uploaded polygons\n\n\n1. Check whether Sentinel 2A Level 1C data is available at the back-end\n\n\nRequest\n\n\nGET /data/Sentinel2A-L1C HTTP/1.1\n\n\n\n\nResponse\n\n\nHeader:\nHTTP/1.1 200 OK\nContent-Type: application/json; charset=utf-8\nAccess-Control-Allow-Origin: <Origin>\nAccess-Control-Allow-Credentials: true\n\nBody:\n{\n  \"product_id\":\"Sentinel-2A-L1C\",\n  \"description\":\"Sentinel 2 Level-1C: Top-of-atmosphere reflectances in cartographic geometry\",\n  \"source\":\"European Space Agency (ESA)\",\n  \"extent\":[\n    -34,\n    35,\n    39,\n    71\n  ],\n  \"time\":[\n    \"2016-01-01\",\n    \"2017-10-01\"\n  ],\n  \"bands\":[\n    {\n      \"band_id\":\"1\",\n      \"wavelength_nm\":443.9,\n      \"res_m\":60,\n      \"scale\":0.0001,\n      \"offset\":0,\n      \"type\":\"int16\",\n      \"unit\":\"1\"\n    },\n    {\n      \"band_id\":\"2\",\n      \"name\":\"blue\",\n      \"wavelength_nm\":496.6,\n      \"res_m\":10,\n      \"scale\":0.0001,\n      \"offset\":0,\n      \"type\":\"int16\",\n      \"unit\":\"1\"\n    },\n    {\n      \"band_id\":\"3\",\n      \"name\":\"green\",\n      \"wavelength_nm\":560,\n      \"res_m\":10,\n      \"scale\":0.0001,\n      \"offset\":0,\n      \"type\":\"int16\",\n      \"unit\":\"1\"\n    },\n    {\n      \"band_id\":\"4\",\n      \"name\":\"red\",\n      \"wavelength_nm\":664.5,\n      \"res_m\":10,\n      \"scale\":0.0001,\n      \"offset\":0,\n      \"type\":\"int16\",\n      \"unit\":\"1\"\n    },\n    {\n      \"band_id\":\"5\",\n      \"wavelength_nm\":703.9,\n      \"res_m\":20,\n      \"scale\":0.0001,\n      \"offset\":0,\n      \"type\":\"int16\",\n      \"unit\":\"1\"\n    },\n    {\n      \"band_id\":\"6\",\n      \"wavelength_nm\":740.2,\n      \"res_m\":20,\n      \"scale\":0.0001,\n      \"offset\":0,\n      \"type\":\"int16\",\n      \"unit\":\"1\"\n    },\n    {\n      \"band_id\":\"7\",\n      \"wavelength_nm\":782.5,\n      \"res_m\":20,\n      \"scale\":0.0001,\n      \"offset\":0,\n      \"type\":\"int16\",\n      \"unit\":\"1\"\n    },\n    {\n      \"band_id\":\"8\",\n      \"name\":\"nir\",\n      \"wavelength_nm\":835.1,\n      \"res_m\":10,\n      \"scale\":0.0001,\n      \"offset\":0,\n      \"type\":\"int16\",\n      \"unit\":\"1\"\n    },\n    {\n      \"band_id\":\"8a\",\n      \"wavelength_nm\":864.8,\n      \"res_m\":20,\n      \"scale\":0.0001,\n      \"offset\":0,\n      \"type\":\"int16\",\n      \"unit\":\"1\"\n    },\n    {\n      \"band_id\":\"9\",\n      \"wavelength_nm\":945,\n      \"res_m\":60,\n      \"scale\":0.0001,\n      \"offset\":0,\n      \"type\":\"int16\",\n      \"unit\":\"1\"\n    },\n    {\n      \"band_id\":\"10\",\n      \"wavelength_nm\":1373.5,\n      \"res_m\":60,\n      \"scale\":0.0001,\n      \"offset\":0,\n      \"type\":\"int16\",\n      \"unit\":\"1\"\n    },\n    {\n      \"band_id\":\"11\",\n      \"wavelength_nm\":1613.7,\n      \"res_m\":20,\n      \"scale\":0.0001,\n      \"offset\":0,\n      \"type\":\"int16\",\n      \"unit\":\"1\"\n    },\n    {\n      \"band_id\":\"12\",\n      \"wavelength_nm\":2202.4,\n      \"res_m\":20,\n      \"scale\":0.0001,\n      \"offset\":0,\n      \"type\":\"int16\",\n      \"unit\":\"1\"\n    }\n  ]\n}\n\n\n\n\n\n\n2. Check whether the back-end supports computing \nzonal_statistics\n\n\nRequest\n\n\nGET /processes/zonal_statistics HTTP/1.1\n\n\n\n\nResponse\n\n\nHeader:\nHTTP/1.1 200 OK\nContent-Type: application/json; charset=utf-8\nAccess-Control-Allow-Origin: <Origin>\nAccess-Control-Allow-Credentials: true\n\nBody:\n{\n  \"process_id\":\"zonal_statistics\",\n  \"description\":\"Runs a Python script for each time series of the input dataset.\",\n  \"args\":{\n    \"imagery\":{\n      \"description\":\"array of input collections with one element\"\n    },\n    \"regions\":{\n      \"description\":\"Polygon file readable by OGR\"\n    },\n    \"func\":{\n      \"description\":\"Function to apply over the polygons, one of `avg`, `min`, `max`, `median`, `q25`, or `q75`.\",\n      \"required\":false,\n      \"default\":\"avg\"\n    }\n  }\n}\n\n\n\n\n3. Upload a GeoJSON Polygon\n\n\nRequest\n\n\nPUT /user/me/files/polygon1.json HTTP/1.1\n\n\n\n\nResponse\n\n\nHeader:\nHTTP/1.1 200 OK\nAccess-Control-Allow-Origin: <Origin>\nAccess-Control-Allow-Credentials: true\n\nBody: none\n\n\n\n\n4. Create a job\n\n\nRequest\n\n\nPOST /jobs HTTP/1.1\nContent-Type: application/json; charset=utf-8\n\nBody:\n{\n  \"process_graph\":{\n    \"process_id\":\"zonal_statistics\",\n    \"args\":{\n      \"imagery\":{\n        \"process_id\":\"filter_daterange\",\n        \"args\":{\n          \"imagery\":{\n            \"process_id\":\"filter_bbox\",\n            \"args\":{\n              \"imagery\":{\n                \"process_id\":\"filter_bands\",\n                \"args\":{\n                  \"imagery\":{\n                    \"product_id\":\"Sentinel2-L1C\"\n                  },\n                  \"bands\":8\n                }\n              },\n              \"left\":16.1,\n              \"right\":16.6,\n              \"top\":48.6,\n              \"bottom\":47.2,\n              \"srs\":\"EPSG:4326\"\n            }\n          },\n          \"from\":\"2017-01-01\",\n          \"to\":\"2017-01-31\"\n        }\n      },\n      \"regions\":\"/users/me/files/\",\n      \"func\":\"avg\"\n    }\n  },\n  \"output\":{\n    \"format\":\"GPKG\"\n  }\n}\n\n\n\n\nResponse\n\n\nHeader:\nHTTP/1.1 200 OK\nContent-Type: application/json; charset=utf-8\nAccess-Control-Allow-Origin: <Origin>\nAccess-Control-Allow-Credentials: true\n\nBody:\n{\n  \"job_id\":\"f6ea12c5e283438a921b525af826da08\",\n  \"status\":\"submitted\",\n  \"submitted\":\"2017-01-01T09:32:12Z\",\n  \"updated\":\"2017-01-01T09:36:18Z\",\n  \"user_id\":\"bd6f9faf93b4\",\n  \"consumed_credits\":0\n}\n\n\n\n\n5. Start batch computation at the back-end\n\n\nRequest\n\n\nPATCH /jobs/f6ea12c5e283438a921b525af826da08/queue HTTP/1.1\n\n\n\n\nResponse\n\n\nHeader:\nHTTP/1.1 200 OK\nAccess-Control-Allow-Origin: <Origin>\nAccess-Control-Allow-Credentials: true\n\nBody: none\n\n\n\n\n6. Check job status twice\n\n\nRequest\n\n\nGET /jobs/f6ea12c5e283438a921b525af826da08 HTTP/1.1\n\n\n\n\nResponse\n\n\nHeader:\nHTTP/1.1 200 OK\nContent-Type: application/json; charset=utf-8\nAccess-Control-Allow-Origin: <Origin>\nAccess-Control-Allow-Credentials: true\n\nBody:\n{\n  \"job_id\":\"f6ea12c5e283438a921b525af826da08\",\n  \"user_id\":\"bd6f9faf93b4\",\n  \"status\":\"running\",\n  \"process_graph\":{\n    \"process_id\":\"zonal_statistics\",\n    \"args\":{\n      \"imagery\":{\n        \"process_id\":\"filter_daterange\",\n        \"args\":{\n          \"imagery\":{\n            \"process_id\":\"filter_bbox\",\n            \"args\":{\n              \"imagery\":{\n                \"process_id\":\"filter_bands\",\n                \"args\":{\n                  \"imagery\":{\n                    \"product_id\":\"Sentinel2-L1C\"\n                  },\n                  \"bands\":8\n                }\n              },\n              \"left\":16.1,\n              \"right\":16.6,\n              \"top\":48.6,\n              \"bottom\":47.2,\n              \"srs\":\"EPSG:4326\"\n            }\n          },\n          \"from\":\"2017-01-01\",\n          \"to\":\"2017-01-31\"\n        }\n      },\n      \"regions\":\"/users/me/files/\",\n      \"func\":\"avg\"\n    }\n  },\n  \"output\":{\n    \"format\":\"GPKG\"\n  },\n  \"submitted\":\"2017-01-01 09:32:12\",\n  \"updated\":\"2017-01-01 09:34:11\",\n  \"consumed_credits\":231\n}\n\n\n\n\nRequest\n\n\nGET /jobs/f6ea12c5e283438a921b525af826da08 HTTP/1.1\n\n\n\n\nResponse\n\n\nHeader:\nHTTP/1.1 200 OK\nContent-Type: application/json; charset=utf-8\nAccess-Control-Allow-Origin: <Origin>\nAccess-Control-Allow-Credentials: true\n\nBody:\n{\n  \"job_id\":\"f6ea12c5e283438a921b525af826da08\",\n  \"user_id\":\"bd6f9faf93b4\",\n  \"status\":\"finished\",\n  \"process_graph\":{\n    \"process_id\":\"zonal_statistics\",\n    \"args\":{\n      \"imagery\":{\n        \"process_id\":\"filter_daterange\",\n        \"args\":{\n          \"imagery\":{\n            \"process_id\":\"filter_bbox\",\n            \"args\":{\n              \"imagery\":{\n                \"process_id\":\"filter_bands\",\n                \"args\":{\n                  \"imagery\":{\n                    \"product_id\":\"Sentinel2-L1C\"\n                  },\n                  \"bands\":8\n                }\n              },\n              \"left\":16.1,\n              \"right\":16.6,\n              \"top\":48.6,\n              \"bottom\":47.2,\n              \"srs\":\"EPSG:4326\"\n            }\n          },\n          \"from\":\"2017-01-01\",\n          \"to\":\"2017-01-31\"\n        }\n      },\n      \"regions\":\"/users/me/files/\",\n      \"func\":\"avg\"\n    }\n  },\n  \"output\":{\n    \"format\":\"GPKG\"\n  },\n  \"submitted\":\"2017-01-01 09:32:12\",\n  \"updated\":\"2017-01-01 09:36:57\",\n  \"consumed_credits\":450\n}\n\n\n\n\n7. Retrieve download links\n\n\nRequest\n\n\nGET /jobs/f6ea12c5e283438a921b525af826da08/download HTTP/1.1\n\n\n\n\nResponse\n\n\nHeader:\nHTTP/1.1 200 OK\nContent-Type: application/json; charset=utf-8\nAccess-Control-Allow-Origin: <Origin>\nAccess-Control-Allow-Credentials: true\n\nBody:\n[\n  \"https://cdn.openeo.org/4854b51643548ab8a858e2b8282711d8/1.gpkg\"\n]\n\n\n\n\n\n\n8. Download file(s)\n\n\nRequest\n\n\nGET https://cdn.openeo.org/4854b51643548ab8a858e2b8282711d8/1.gpkg HTTP/1.1\n\n\n\n\nResponse (GPKG file)\n\n\nomitted",
            "title": "Proof of Concept"
        },
        {
            "location": "/poc/index.html#proof-of-concept-api-v002",
            "text": "This page gives a detailed description of the openEO proof of concept and gives a list and specification of what needs to be implemented. The proof of concept will consist of   at least three clearly defined example processes (see below),  a prototypical API specification including communication API call sequences of the processes (see below),  implementations of the processes on three back-ends, and  prototypical clients in R, Python and potentially JavaScript.   Below, we define example use cases and how they are translated to sequences of API calls:   Deriving minimum NDVI measurements over pixel time series of Sentinel 2 imagery  Create a monthly aggregated Sentinel 1 product from a custom Python script  Compute time series of zonal (regional) statistics of Sentinel 2 imagery over user-uploaded polygons   Note:  Authentication is not included in these examples. Enabling authentication needs the placeholder  <Origin>  to be set to the requesting host, including protocol, host name/IP and port, e.g.  http://localhost:8080 . This could be done by using the Origin header value from the request.",
            "title": "Proof of Concept (API v0.0.2)"
        },
        {
            "location": "/poc/index.html#use-case-1",
            "text": "Deriving minimum NDVI measurements over pixel time series of Sentinel 2 imagery.  1. Check whether Sentinel 2A Level 1C data is available at the back-end  Request  GET /data/Sentinel2A-L1C HTTP/1.1  Response  Header:\nHTTP/1.1 200 OK\nContent-Type: application/json; charset=utf-8\nAccess-Control-Allow-Origin: <Origin>\nAccess-Control-Allow-Credentials: true\n\nBody:\n{\n  \"product_id\":\"Sentinel-2A-L1C\",\n  \"description\":\"Sentinel 2 Level-1C: Top-of-atmosphere reflectances in cartographic geometry\",\n  \"source\":\"European Space Agency (ESA)\",\n  \"extent\":[\n    -34,\n    35,\n    39,\n    71\n  ],\n  \"time\":[\n    \"2016-01-01\",\n    \"2017-10-01\"\n  ],\n  \"bands\":[\n    {\n      \"band_id\":\"1\",\n      \"wavelength_nm\":443.9,\n      \"res_m\":60,\n      \"scale\":0.0001,\n      \"offset\":0,\n      \"type\":\"int16\",\n      \"unit\":\"1\"\n    },\n    {\n      \"band_id\":\"2\",\n      \"name\":\"blue\",\n      \"wavelength_nm\":496.6,\n      \"res_m\":10,\n      \"scale\":0.0001,\n      \"offset\":0,\n      \"type\":\"int16\",\n      \"unit\":\"1\"\n    },\n    {\n      \"band_id\":\"3\",\n      \"name\":\"green\",\n      \"wavelength_nm\":560,\n      \"res_m\":10,\n      \"scale\":0.0001,\n      \"offset\":0,\n      \"type\":\"int16\",\n      \"unit\":\"1\"\n    },\n    {\n      \"band_id\":\"4\",\n      \"name\":\"red\",\n      \"wavelength_nm\":664.5,\n      \"res_m\":10,\n      \"scale\":0.0001,\n      \"offset\":0,\n      \"type\":\"int16\",\n      \"unit\":\"1\"\n    },\n    {\n      \"band_id\":\"5\",\n      \"wavelength_nm\":703.9,\n      \"res_m\":20,\n      \"scale\":0.0001,\n      \"offset\":0,\n      \"type\":\"int16\",\n      \"unit\":\"1\"\n    },\n    {\n      \"band_id\":\"6\",\n      \"wavelength_nm\":740.2,\n      \"res_m\":20,\n      \"scale\":0.0001,\n      \"offset\":0,\n      \"type\":\"int16\",\n      \"unit\":\"1\"\n    },\n    {\n      \"band_id\":\"7\",\n      \"wavelength_nm\":782.5,\n      \"res_m\":20,\n      \"scale\":0.0001,\n      \"offset\":0,\n      \"type\":\"int16\",\n      \"unit\":\"1\"\n    },\n    {\n      \"band_id\":\"8\",\n      \"name\":\"nir\",\n      \"wavelength_nm\":835.1,\n      \"res_m\":10,\n      \"scale\":0.0001,\n      \"offset\":0,\n      \"type\":\"int16\",\n      \"unit\":\"1\"\n    },\n    {\n      \"band_id\":\"8a\",\n      \"wavelength_nm\":864.8,\n      \"res_m\":20,\n      \"scale\":0.0001,\n      \"offset\":0,\n      \"type\":\"int16\",\n      \"unit\":\"1\"\n    },\n    {\n      \"band_id\":\"9\",\n      \"wavelength_nm\":945,\n      \"res_m\":60,\n      \"scale\":0.0001,\n      \"offset\":0,\n      \"type\":\"int16\",\n      \"unit\":\"1\"\n    },\n    {\n      \"band_id\":\"10\",\n      \"wavelength_nm\":1373.5,\n      \"res_m\":60,\n      \"scale\":0.0001,\n      \"offset\":0,\n      \"type\":\"int16\",\n      \"unit\":\"1\"\n    },\n    {\n      \"band_id\":\"11\",\n      \"wavelength_nm\":1613.7,\n      \"res_m\":20,\n      \"scale\":0.0001,\n      \"offset\":0,\n      \"type\":\"int16\",\n      \"unit\":\"1\"\n    },\n    {\n      \"band_id\":\"12\",\n      \"wavelength_nm\":2202.4,\n      \"res_m\":20,\n      \"scale\":0.0001,\n      \"offset\":0,\n      \"type\":\"int16\",\n      \"unit\":\"1\"\n    }\n  ]\n}  2. Check that needed processes are available  Request  GET /processes/filter_bbox HTTP/1.1  Response  Header:\nHTTP/1.1 200 OK\nContent-Type: application/json; charset=utf-8\nAccess-Control-Allow-Origin: <Origin>\nAccess-Control-Allow-Credentials: true\n\nBody:\n{\n  \"process_id\":\"filter_bbox\",\n  \"description\":\"Drops observations from a collection that are located outside of a given bounding box.\",\n  \"args\":{\n    \"imagery\":{\n      \"description\":\"array of input collections with one element\"\n    },\n    \"left\":{\n      \"description\":\"left boundary (longitude / easting)\"\n    },\n    \"right\":{\n      \"description\":\"right boundary (longitude / easting)\"\n    },\n    \"top\":{\n      \"description\":\"top boundary (latitude / northing)\"\n    },\n    \"bottom\":{\n      \"description\":\"bottom boundary (latitude / northing)\"\n    },\n    \"srs\":{\n      \"description\":\"spatial reference system of boundaries as proj4 or EPSG:12345 like string\"\n    }\n  }\n}  Request  GET /processes/filter_daterange HTTP/1.1  Response  Header:\nHTTP/1.1 200 OK\nContent-Type: application/json; charset=utf-8\nAccess-Control-Allow-Origin: <Origin>\nAccess-Control-Allow-Credentials: true\n\nBody:\n{\n  \"process_id\":\"filter_daterange\",\n  \"description\":\"Drops observations from a collection that have been captured before a start or after a given end date.\",\n  \"args\":{\n    \"imagery\":{\n      \"description\":\"array of input collections with one element\"\n    },\n    \"from\":{\n      \"description\":\"start date\"\n    },\n    \"to\":{\n      \"description\":\"end date\"\n    }\n  }\n}  Request  GET /processes/NDVI HTTP/1.1  Response  Header:\nHTTP/1.1 200 OK\nContent-Type: application/json; charset=utf-8\nAccess-Control-Allow-Origin: <Origin>\nAccess-Control-Allow-Credentials: true\n\nBody:\n{\n  \"process_id\":\"NDVI\",\n  \"description\":\"Finds the minimum value of time series for all bands of the input dataset.\",\n  \"args\":{\n    \"imagery\":{\n      \"description\":\"array of input collections with one element\"\n    },\n    \"red\":{\n      \"description\":\"reference to the red band\"\n    },\n    \"nir\":{\n      \"description\":\"reference to the nir band\"\n    }\n  }\n}  Request  GET /processes/min_time HTTP/1.1  Response  Header:\nHTTP/1.1 200 OK\nContent-Type: application/json; charset=utf-8\nAccess-Control-Allow-Origin: <Origin>\nAccess-Control-Allow-Credentials: true\n\nBody:\n{\n  \"process_id\":\"min_time\",\n  \"description\":\"Finds the minimum value of time series for all bands of the input dataset.\",\n  \"args\":{\n    \"imagery\":{\n      \"description\":\"array of input collections with one element\"\n    }\n  }\n}  3. Create a job at the back-end  Request  POST /jobs HTTP/1.1\nContent-Type: application/json; charset=utf-8\n\nBody:\n{\n  \"process_graph\":{\n    \"process_id\":\"min_time\",\n    \"args\":{\n      \"imagery\":{\n        \"process_id\":\"NDVI\",\n        \"args\":{\n          \"imagery\":{\n            \"process_id\":\"filter_daterange\",\n            \"args\":{\n              \"imagery\":{\n                \"process_id\":\"filter_bbox\",\n                \"args\":{\n                  \"imagery\":{\n                    \"product_id\":\"S2_L2A_T32TPS_20M\"\n                  },\n                  \"left\":652000,\n                  \"right\":672000,\n                  \"top\":5161000,\n                  \"bottom\":5181000,\n                  \"srs\":\"EPSG:32632\"\n                }\n              },\n              \"from\":\"2017-01-01\",\n              \"to\":\"2017-01-31\"\n            }\n          },\n          \"red\":\"B04\",\n          \"nir\":\"B8A\"\n        }\n      }\n    }\n  }\n}  Response  Header:\nHTTP/1.1 200 OK\nContent-Type: application/json; charset=utf-8\nAccess-Control-Allow-Origin: <Origin>\nAccess-Control-Allow-Credentials: true\n\nBody:\n{\n  \"job_id\":\"2a8ffb20c2b235a3f3e3351f\",\n  \"status\":\"submitted\",\n  \"submitted\":\"2017-01-01T09:32:12Z\",\n  \"updated\":\"2017-01-01T09:36:18Z\",\n  \"user_id\":\"bd6f9faf93b4\",\n  \"consumed_credits\":0\n}  4. Create a WCS service  Request  POST /services HTTP/1.1\nContent-Type: application/json; charset=utf-8\n\nBody:\n{\n  \"job_id\":\"2a8ffb20c2b235a3f3e3351f\",\n  \"type\":\"wcs\",\n  \"args\":{\n    \"VERSION\":\"2.0.1\"\n  }\n}  Response  Header:\nHTTP/1.1 200 OK\nContent-Type: application/json; charset=utf-8\nAccess-Control-Allow-Origin: <Origin>\nAccess-Control-Allow-Credentials: true\n\nBody:\n{\n  \"service_id\":\"4dab456f6501bbcd\",\n  \"service_url\":\"https://openeo.org/4dab456f6501bbcd/wcs\",\n  \"service_type\":\"wcs\",\n  \"service_args\":{\n    \"VERSION\":\"2.0.1\"\n  },\n  \"job_id\":\"2a8ffb20c2b235a3f3e3351f\"\n}  5. Download the data on demand with WCS  Request  GET https://openeo.org/4dab456f6501bbcd/wcs?SERVICE=WCS&VERSION=2.0.1&REQUEST=GetCapabilities HTTP/1.1  Response  omitted  Request  GET https://openeo.org/4dab456f6501bbcd/wcs?SERVICE=WCS&VERSION=2.0.1&REQUEST=GetCoverage&COVERAGEID=2a8ffb20c2b235a3f3e3351f&FORMAT=image/tiff&SUBSET=x,http://www.opengis.net/def/crs/EPSG/0/4326(16.1,16.5)&SUBSET=y,http://www.opengis.net/def/crs/EPSG/0/4326(47.9,48.6)&&SIZE=x(200)&SIZE=y(200) HTTP/1.1  Response   omitted  6. Stop the job (and the service)  Request  PATCH /jobs/2a8ffb20c2b235a3f3e3351f/cancel HTTP/1.1  Response  Header:\nHTTP/1.1 200 OK\nAccess-Control-Allow-Origin: <Origin>\nAccess-Control-Allow-Credentials: true\n\nBody: none",
            "title": "Use Case 1"
        },
        {
            "location": "/poc/index.html#use-case-2",
            "text": "Create a monthly aggregated Sentinel 1 product from a custom Python script.  1. Ask the back-end for available Sentinel 1 data  Request  GET /data/Sentinel1-L1-IW-GRD HTTP/1.1  Response  Header:\nHTTP/1.1 200 OK\nContent-Type: application/json; charset=utf-8\nAccess-Control-Allow-Origin: <Origin>\nAccess-Control-Allow-Credentials: true\n\nBody:\n{\n  \"product_id\":\"Sentinel1-L1-IW-GRD\",\n  \"description\":\"Sentinel 1 C-band Synthetic Aperture Radar (SAR) Ground Range Data\",\n  \"source\":\"European Space Agency (ESA)\",\n  \"extent\":[\n    -34,\n    35,\n    39,\n    71\n  ],\n  \"time\":[\n    \"2016-01-01\",\n    \"2017-10-01\"\n  ],\n  \"bands\":[\n    {\n      \"band_id\":\"VV\"\n    },\n    {\n      \"band_id\":\"VH\"\n    }\n  ]\n}  2. Ask the back-end whether it supports Python UDFs of type aggregate_time and get details about expected parameters  Request  GET /udf_runtimes HTTP/1.1  Response  Header:\nHTTP/1.1 200 OK\nContent-Type: application/json; charset=utf-8\nAccess-Control-Allow-Origin: <Origin>\nAccess-Control-Allow-Credentials: true\n\nBody:\n{\n  \"Python\":{\n    \"udf_types\":[\n      \"reduce_time\",\n      \"aggregate_time\",\n      \"apply_pixel\"\n    ],\n    \"versions\":{\n      \"3.6.3\":{\n        \"packages\":[\n          \"numpy\",\n          \"scipy\",\n          \"pandas\",\n          \"matplotlib\",\n          \"ipython\",\n          \"jupyter\",\n          \"GDAL\"\n        ]\n      }\n    }\n  }\n}  Request  GET /udf_runtimes/Python/aggregate_time HTTP/1.1  Response  Header:\nHTTP/1.1 200 OK\nContent-Type: application/json; charset=utf-8\nAccess-Control-Allow-Origin: <Origin>\nAccess-Control-Allow-Credentials: true\n\n{\n  \"process_id\":\"/udf/Python/aggregate_time\",\n  \"description\":\"Runs a Python script for each time series of the input dataset.\",\n  \"args\":{\n    \"imagery\":{\n      \"description\":\"array of input collections with one element\"\n    },\n    \"script\":{\n      \"description\":\"Python script that will be executed over all time series, gets time series as (Pandas) DataFrame and expects a new DataFrame as output.\"\n    },\n    \"version\":{\n      \"description\":\"Python version to use, defaults to the latest available version.\",\n      \"required\":false,\n      \"default\":\"latest\"\n    }\n  }\n}  3. Upload python script  Request  PUT /users/me/files/s1_aggregate.py HTTP/1.1  Response  Header:\nHTTP/1.1 200 OK\nAccess-Control-Allow-Origin: <Origin>\nAccess-Control-Allow-Credentials: true\n\nBody: none  4. Create a job  Request  POST /jobs HTTP/1.1\nContent-Type: application/json; charset=utf-8\n\nBody:\n{\n  \"process_graph\":{\n    \"process_id\":\"/udf/Python/aggregate_time\",\n    \"args\":{\n      \"script\":\"/users/me/files/s1_aggregate.py\",\n      \"imagery\":{\n        \"process_id\":\"filter_daterange\",\n        \"args\":{\n          \"imagery\":{\n            \"process_id\":\"filter_bbox\",\n            \"args\":{\n              \"imagery\":{\n                \"product_id\":\"Sentinel1-L1-IW-GRD\"\n              },\n              \"left\":16.1,\n              \"right\":16.6,\n              \"top\":48.6,\n              \"bottom\":47.2,\n              \"srs\":\"EPSG:4326\"\n            }\n          },\n          \"from\":\"2017-01-01\",\n          \"to\":\"2017-01-31\"\n        }\n      }\n    }\n  }\n}  Response  Header:\nHTTP/1.1 200 OK\nContent-Type: application/json; charset=utf-8\nAccess-Control-Allow-Origin: <Origin>\nAccess-Control-Allow-Credentials: true\n\nBody:\n{\n  \"job_id\":\"3723c32fb7b24698832ca71f2d3f18aa\",\n  \"status\":\"submitted\",\n  \"submitted\":\"2017-01-01T09:32:12Z\",\n  \"updated\":\"2017-01-01T09:36:18Z\",\n  \"user_id\":\"bd6f9faf93b4\",\n  \"consumed_credits\":0\n}  5. Create a TMS service  Request  POST /services HTTP/1.1\nContent-Type: application/json; charset=utf-8\n\nBody:\n{\n  \"job_id\":\"3723c32fb7b24698832ca71f2d3f18aa\",\n  \"type\":\"tms\"\n}  Response  Header:\nHTTP/1.1 200 OK\nContent-Type: application/json; charset=utf-8\nAccess-Control-Allow-Origin: <Origin>\nAccess-Control-Allow-Credentials: true\n\nBody:\n{\n  \"service_id\":\"9dab4b6f6523\",\n  \"service_url\":\"http://cdn.cloudprovider.com/openeo/services/9dab4b6f6523/tms\",\n  \"service_type\":\"tms\",\n  \"job_id\":\"3723c32fb7b24698832ca71f2d3f18aa\"\n}  6. Download results as TMS  Example Request  GET http://cdn.cloudprovider.com/openeo/services/9dab4b6f6523/tms/2017-01-01/12/2232/2668/?bands=1 HTTP/1.1  Response  omitted",
            "title": "Use Case 2"
        },
        {
            "location": "/poc/index.html#use-case-3",
            "text": "Compute time series of zonal (regional) statistics of Sentinel 2 imagery over user-uploaded polygons  1. Check whether Sentinel 2A Level 1C data is available at the back-end  Request  GET /data/Sentinel2A-L1C HTTP/1.1  Response  Header:\nHTTP/1.1 200 OK\nContent-Type: application/json; charset=utf-8\nAccess-Control-Allow-Origin: <Origin>\nAccess-Control-Allow-Credentials: true\n\nBody:\n{\n  \"product_id\":\"Sentinel-2A-L1C\",\n  \"description\":\"Sentinel 2 Level-1C: Top-of-atmosphere reflectances in cartographic geometry\",\n  \"source\":\"European Space Agency (ESA)\",\n  \"extent\":[\n    -34,\n    35,\n    39,\n    71\n  ],\n  \"time\":[\n    \"2016-01-01\",\n    \"2017-10-01\"\n  ],\n  \"bands\":[\n    {\n      \"band_id\":\"1\",\n      \"wavelength_nm\":443.9,\n      \"res_m\":60,\n      \"scale\":0.0001,\n      \"offset\":0,\n      \"type\":\"int16\",\n      \"unit\":\"1\"\n    },\n    {\n      \"band_id\":\"2\",\n      \"name\":\"blue\",\n      \"wavelength_nm\":496.6,\n      \"res_m\":10,\n      \"scale\":0.0001,\n      \"offset\":0,\n      \"type\":\"int16\",\n      \"unit\":\"1\"\n    },\n    {\n      \"band_id\":\"3\",\n      \"name\":\"green\",\n      \"wavelength_nm\":560,\n      \"res_m\":10,\n      \"scale\":0.0001,\n      \"offset\":0,\n      \"type\":\"int16\",\n      \"unit\":\"1\"\n    },\n    {\n      \"band_id\":\"4\",\n      \"name\":\"red\",\n      \"wavelength_nm\":664.5,\n      \"res_m\":10,\n      \"scale\":0.0001,\n      \"offset\":0,\n      \"type\":\"int16\",\n      \"unit\":\"1\"\n    },\n    {\n      \"band_id\":\"5\",\n      \"wavelength_nm\":703.9,\n      \"res_m\":20,\n      \"scale\":0.0001,\n      \"offset\":0,\n      \"type\":\"int16\",\n      \"unit\":\"1\"\n    },\n    {\n      \"band_id\":\"6\",\n      \"wavelength_nm\":740.2,\n      \"res_m\":20,\n      \"scale\":0.0001,\n      \"offset\":0,\n      \"type\":\"int16\",\n      \"unit\":\"1\"\n    },\n    {\n      \"band_id\":\"7\",\n      \"wavelength_nm\":782.5,\n      \"res_m\":20,\n      \"scale\":0.0001,\n      \"offset\":0,\n      \"type\":\"int16\",\n      \"unit\":\"1\"\n    },\n    {\n      \"band_id\":\"8\",\n      \"name\":\"nir\",\n      \"wavelength_nm\":835.1,\n      \"res_m\":10,\n      \"scale\":0.0001,\n      \"offset\":0,\n      \"type\":\"int16\",\n      \"unit\":\"1\"\n    },\n    {\n      \"band_id\":\"8a\",\n      \"wavelength_nm\":864.8,\n      \"res_m\":20,\n      \"scale\":0.0001,\n      \"offset\":0,\n      \"type\":\"int16\",\n      \"unit\":\"1\"\n    },\n    {\n      \"band_id\":\"9\",\n      \"wavelength_nm\":945,\n      \"res_m\":60,\n      \"scale\":0.0001,\n      \"offset\":0,\n      \"type\":\"int16\",\n      \"unit\":\"1\"\n    },\n    {\n      \"band_id\":\"10\",\n      \"wavelength_nm\":1373.5,\n      \"res_m\":60,\n      \"scale\":0.0001,\n      \"offset\":0,\n      \"type\":\"int16\",\n      \"unit\":\"1\"\n    },\n    {\n      \"band_id\":\"11\",\n      \"wavelength_nm\":1613.7,\n      \"res_m\":20,\n      \"scale\":0.0001,\n      \"offset\":0,\n      \"type\":\"int16\",\n      \"unit\":\"1\"\n    },\n    {\n      \"band_id\":\"12\",\n      \"wavelength_nm\":2202.4,\n      \"res_m\":20,\n      \"scale\":0.0001,\n      \"offset\":0,\n      \"type\":\"int16\",\n      \"unit\":\"1\"\n    }\n  ]\n}  2. Check whether the back-end supports computing  zonal_statistics  Request  GET /processes/zonal_statistics HTTP/1.1  Response  Header:\nHTTP/1.1 200 OK\nContent-Type: application/json; charset=utf-8\nAccess-Control-Allow-Origin: <Origin>\nAccess-Control-Allow-Credentials: true\n\nBody:\n{\n  \"process_id\":\"zonal_statistics\",\n  \"description\":\"Runs a Python script for each time series of the input dataset.\",\n  \"args\":{\n    \"imagery\":{\n      \"description\":\"array of input collections with one element\"\n    },\n    \"regions\":{\n      \"description\":\"Polygon file readable by OGR\"\n    },\n    \"func\":{\n      \"description\":\"Function to apply over the polygons, one of `avg`, `min`, `max`, `median`, `q25`, or `q75`.\",\n      \"required\":false,\n      \"default\":\"avg\"\n    }\n  }\n}  3. Upload a GeoJSON Polygon  Request  PUT /user/me/files/polygon1.json HTTP/1.1  Response  Header:\nHTTP/1.1 200 OK\nAccess-Control-Allow-Origin: <Origin>\nAccess-Control-Allow-Credentials: true\n\nBody: none  4. Create a job  Request  POST /jobs HTTP/1.1\nContent-Type: application/json; charset=utf-8\n\nBody:\n{\n  \"process_graph\":{\n    \"process_id\":\"zonal_statistics\",\n    \"args\":{\n      \"imagery\":{\n        \"process_id\":\"filter_daterange\",\n        \"args\":{\n          \"imagery\":{\n            \"process_id\":\"filter_bbox\",\n            \"args\":{\n              \"imagery\":{\n                \"process_id\":\"filter_bands\",\n                \"args\":{\n                  \"imagery\":{\n                    \"product_id\":\"Sentinel2-L1C\"\n                  },\n                  \"bands\":8\n                }\n              },\n              \"left\":16.1,\n              \"right\":16.6,\n              \"top\":48.6,\n              \"bottom\":47.2,\n              \"srs\":\"EPSG:4326\"\n            }\n          },\n          \"from\":\"2017-01-01\",\n          \"to\":\"2017-01-31\"\n        }\n      },\n      \"regions\":\"/users/me/files/\",\n      \"func\":\"avg\"\n    }\n  },\n  \"output\":{\n    \"format\":\"GPKG\"\n  }\n}  Response  Header:\nHTTP/1.1 200 OK\nContent-Type: application/json; charset=utf-8\nAccess-Control-Allow-Origin: <Origin>\nAccess-Control-Allow-Credentials: true\n\nBody:\n{\n  \"job_id\":\"f6ea12c5e283438a921b525af826da08\",\n  \"status\":\"submitted\",\n  \"submitted\":\"2017-01-01T09:32:12Z\",\n  \"updated\":\"2017-01-01T09:36:18Z\",\n  \"user_id\":\"bd6f9faf93b4\",\n  \"consumed_credits\":0\n}  5. Start batch computation at the back-end  Request  PATCH /jobs/f6ea12c5e283438a921b525af826da08/queue HTTP/1.1  Response  Header:\nHTTP/1.1 200 OK\nAccess-Control-Allow-Origin: <Origin>\nAccess-Control-Allow-Credentials: true\n\nBody: none  6. Check job status twice  Request  GET /jobs/f6ea12c5e283438a921b525af826da08 HTTP/1.1  Response  Header:\nHTTP/1.1 200 OK\nContent-Type: application/json; charset=utf-8\nAccess-Control-Allow-Origin: <Origin>\nAccess-Control-Allow-Credentials: true\n\nBody:\n{\n  \"job_id\":\"f6ea12c5e283438a921b525af826da08\",\n  \"user_id\":\"bd6f9faf93b4\",\n  \"status\":\"running\",\n  \"process_graph\":{\n    \"process_id\":\"zonal_statistics\",\n    \"args\":{\n      \"imagery\":{\n        \"process_id\":\"filter_daterange\",\n        \"args\":{\n          \"imagery\":{\n            \"process_id\":\"filter_bbox\",\n            \"args\":{\n              \"imagery\":{\n                \"process_id\":\"filter_bands\",\n                \"args\":{\n                  \"imagery\":{\n                    \"product_id\":\"Sentinel2-L1C\"\n                  },\n                  \"bands\":8\n                }\n              },\n              \"left\":16.1,\n              \"right\":16.6,\n              \"top\":48.6,\n              \"bottom\":47.2,\n              \"srs\":\"EPSG:4326\"\n            }\n          },\n          \"from\":\"2017-01-01\",\n          \"to\":\"2017-01-31\"\n        }\n      },\n      \"regions\":\"/users/me/files/\",\n      \"func\":\"avg\"\n    }\n  },\n  \"output\":{\n    \"format\":\"GPKG\"\n  },\n  \"submitted\":\"2017-01-01 09:32:12\",\n  \"updated\":\"2017-01-01 09:34:11\",\n  \"consumed_credits\":231\n}  Request  GET /jobs/f6ea12c5e283438a921b525af826da08 HTTP/1.1  Response  Header:\nHTTP/1.1 200 OK\nContent-Type: application/json; charset=utf-8\nAccess-Control-Allow-Origin: <Origin>\nAccess-Control-Allow-Credentials: true\n\nBody:\n{\n  \"job_id\":\"f6ea12c5e283438a921b525af826da08\",\n  \"user_id\":\"bd6f9faf93b4\",\n  \"status\":\"finished\",\n  \"process_graph\":{\n    \"process_id\":\"zonal_statistics\",\n    \"args\":{\n      \"imagery\":{\n        \"process_id\":\"filter_daterange\",\n        \"args\":{\n          \"imagery\":{\n            \"process_id\":\"filter_bbox\",\n            \"args\":{\n              \"imagery\":{\n                \"process_id\":\"filter_bands\",\n                \"args\":{\n                  \"imagery\":{\n                    \"product_id\":\"Sentinel2-L1C\"\n                  },\n                  \"bands\":8\n                }\n              },\n              \"left\":16.1,\n              \"right\":16.6,\n              \"top\":48.6,\n              \"bottom\":47.2,\n              \"srs\":\"EPSG:4326\"\n            }\n          },\n          \"from\":\"2017-01-01\",\n          \"to\":\"2017-01-31\"\n        }\n      },\n      \"regions\":\"/users/me/files/\",\n      \"func\":\"avg\"\n    }\n  },\n  \"output\":{\n    \"format\":\"GPKG\"\n  },\n  \"submitted\":\"2017-01-01 09:32:12\",\n  \"updated\":\"2017-01-01 09:36:57\",\n  \"consumed_credits\":450\n}  7. Retrieve download links  Request  GET /jobs/f6ea12c5e283438a921b525af826da08/download HTTP/1.1  Response  Header:\nHTTP/1.1 200 OK\nContent-Type: application/json; charset=utf-8\nAccess-Control-Allow-Origin: <Origin>\nAccess-Control-Allow-Credentials: true\n\nBody:\n[\n  \"https://cdn.openeo.org/4854b51643548ab8a858e2b8282711d8/1.gpkg\"\n]  8. Download file(s)  Request  GET https://cdn.openeo.org/4854b51643548ab8a858e2b8282711d8/1.gpkg HTTP/1.1  Response (GPKG file)  omitted",
            "title": "Use Case 3"
        }
    ]
}