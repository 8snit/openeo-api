{
    "docs": [
        {
            "location": "/index.html",
            "text": "openEO - Concepts and API Reference\n\n\nWork in progress, please contribute by adding \nissues\n.\n\n\nopenEO develops an open API that connects clients like R, Python and JavaScript to big Earth observation cloud back-ends in a simple and unified way.\n\n\nThe following pages introduce the core concepts of the project. Make sure to introduce yourself to the major technical terms used in the openEO project by reading the \nglossary\n.\n\n\nThe openEO API defines a \nHTTP API\n that lets cloud back-ends with large Earth observation datasets communicate with front end analysis applications in an interoperable way. This documentation describes important API concepts and design decisions and gives a complete \nAPI reference documentation\n.\n\n\nAs an overview, the openEO API specifies how to\n\n\n\n\ndiscover which Earth observation data and processes are available at cloud back-ends,\n\n\nexecute (chained) processes on back-ends, \n\n\nrun \nuser-defined functions\n (UDFs) on back-ends where UDFs can be exposed to the data in different ways, \n\n\ndownload (intermediate) results as web services, and\n\n\nmanage user content including accounting.\n\n\n\n\nThe API is defined as an \nOpenAPI 3.0\n JSON file.\n\n\n \n\n\nopenEO\n, A Common, Open Source Interface between Earth Observation Data Infrastructures and Front-End Applications is a H2020 project funded under call EO-2-2017: EO Big Data Shift, under proposal number 776242. It will run from Oct 2017 to Sept 2020.\n\n\nThis project has received funding from the European Union\u2019s Horizon 2020 research and innovation programme under grant agreement No 776242. The contents of this website reflects only the authors\u2019 view; the European Commission is not responsible for any use that may be made of the information it provides.",
            "title": "Introduction"
        },
        {
            "location": "/index.html#openeo-concepts-and-api-reference",
            "text": "Work in progress, please contribute by adding  issues .  openEO develops an open API that connects clients like R, Python and JavaScript to big Earth observation cloud back-ends in a simple and unified way.  The following pages introduce the core concepts of the project. Make sure to introduce yourself to the major technical terms used in the openEO project by reading the  glossary .  The openEO API defines a  HTTP API  that lets cloud back-ends with large Earth observation datasets communicate with front end analysis applications in an interoperable way. This documentation describes important API concepts and design decisions and gives a complete  API reference documentation .  As an overview, the openEO API specifies how to   discover which Earth observation data and processes are available at cloud back-ends,  execute (chained) processes on back-ends,   run  user-defined functions  (UDFs) on back-ends where UDFs can be exposed to the data in different ways,   download (intermediate) results as web services, and  manage user content including accounting.   The API is defined as an  OpenAPI 3.0  JSON file.     openEO , A Common, Open Source Interface between Earth Observation Data Infrastructures and Front-End Applications is a H2020 project funded under call EO-2-2017: EO Big Data Shift, under proposal number 776242. It will run from Oct 2017 to Sept 2020.  This project has received funding from the European Union\u2019s Horizon 2020 research and innovation programme under grant agreement No 776242. The contents of this website reflects only the authors\u2019 view; the European Commission is not responsible for any use that may be made of the information it provides.",
            "title": "openEO - Concepts and API Reference"
        },
        {
            "location": "/glossary/index.html",
            "text": "Glossary\n\n\nThis glossary introduces, and tries to define, the major technical terms used in the openEO project.\n\n\nThe acronym \nopenEO\n contracts two concepts:\n\n\n\n\nopen\n: used here in the context of open source software; open source software is available in source code form, and can be freely modified and redistributed; the openEO project will create open source software, reusable under a liberal open source license (Apache 2.0)\n\n\nEO\n: Earth observation; openEO targets the processing and analysis of Earth observation data\n\n\n\n\nFurther terms:\n\n\n\n\nAPI\n: application programming interface (\nwikipedia\n); a communication protocol between client and back-end\n\n\nclient\n: software environment (software) that end-users directly interact with, e.g. R (rstudio), Python (jupyter notebook), and JavaScript (web browser); R and Python are two major data science platforms; JavaScript is a major language for web development\n\n\n(cloud) back-end\n: server; computer infrastructure (one or more physical computers or virtual machines) used for storing EO data and processing it\n\n\nbig Earth observation cloud back-end\n server infrastructure where industry and researchers analyse large amounts of EO data\n\n\nsimple\n many end-users now use Python or R to analyse data and JavaScript to develop web applications; analysing large amounts of EO imagery should be equally simple, and seamlessly integrate with existing workflows\n\n\nunified\n current EO cloud back-ends all have \na different API\n, making EO data analysis hard to validate,difficult to reproduce, and back-ends difficult to compare in terms of capability and costs, or to combine in a joint analysis across back-ends. A unified API can resolve many of these problems.\n\n\n\n\nDatasets\n\n\nCEOS (\nCEOS OpenSearch Best Practice Document v1.2\n) defines \nGranules\n and \nCollections\n as follows:\n\n\n\n\n\"A \ngranule\n is the finest granularity of data that can be independently managed. A granule usually matches the individual file of EO satellite data.\"\n\n\n\"A \ncollection\n is an aggregation of granules sharing the same product specification. A collection typically corresponds to the series of products derived from data acquired by a sensor on board a satellite and having the same mode of operation.\"\n\n\n\n\nThe same document lists the synonyms used (by organisations) for:\n\n\n\n\ngranule\n: dataset (ISO 19115), dataset (ESA), granule (NASA), product (ESA, CNES), scene (JAXA)\n\n\ncollection\n: dataset series (ISO 19115), collection (CNES, NASA), dataset (JAXA), dataset series (ESA), product (JAXA)\n\n\n\n\nHere, we will use \ngranule\n and \ncollection\n.\n\n\nA \ngranule\n will typically refer to a limited area and a single overpass leading to a very short observation period (seconds), or a temporal aggregation of such data as e.g. for 16-day MODIS composites.\n\n\nThe open geospatial consortium published a document on \nOGC OpenSearch Geo and Time Extensions\n.\n\n\nProcesses and Jobs\n\n\nThe terms \nprocess and\n \nprocess graph\n have different meanings in the openEO API specification.\n\n\nA \nprocess\n is simply the description of an operation as provided by the back end, similar to a function definition in programming languages. \n\n\nIn this context openEO will:\n\n\n\n\nconsider, or allow to consider, band as a dimension\n\n\nconsider imagery (image collections) to consist of one \nor more\n collections, as argument to functions; allow filtering on a particular collection, or joining them into a single collection\n\n\nallow filtering on attributes, e.g. on cloud-free pixels, or pixels inside a \nMULTIPOLYGON\n describing the floodplains of the Danube. This filters on attributes rather than dimensions.\n\n\nProvide generic aggregate operations that aggregate over one or more dimensions. Clients may provide dimension-specific aggregation functions for particular cases (such as \nmin_time\n) \n\n\n\n\nA \nprocess graph\n includes specific process calls, i.e. references to one or more processes including specific values for input arguments similar to a function call in programming. However, process graphs can chain multiple processes. In particular, arguments of processes in general can be again (recursive) process graphs, input datasets, or simple scalar or array values.\n\n\nUser-defined functions (UDFs)\n\n\nThe abbreviation \nUDF\n stands for \nuser-defined function\n. With this concept, users are able to upload custom code and have it executed e.g. for every pixel of a scene, allowing custom calculations on server-side data. See the section on \nUDFs\n for more information.\n\n\nAggregation vs. resampling\n\n\nAggregation\n computes new values from sets of values that are \nuniquely\n assigned to groups. It involves a grouping predicate (e.g. monthly, 100 m x 100 m grid cells; think of SQL's \ngroup_by\n), and an aggregation function (e.g., \nmean\n) that computes one or more new values from the original ones.\n\n\nExamples:\n\n\n\n\na time series aggregation may return a regression slope and intercept for every pixel time series, for a single band (group by: full time extent)\n\n\na time series may be aggregated to monthly values by computing the mean for all values in a month (group by: months)\n\n\nspatial\n aggregation involves computing e.g. \nmean\n pixel values on a 100 x 100 m grid, from 10 m x 10 m pixels, where each original pixel is assigned uniquely to a larger pixel (group by: 100 m x 100 m grid cells)\n\n\n\n\nNote that for the first example, the aggregation function not only requires time series values, but also their time stamps.\n\n\nResampling\n is a broader term where we have data at one resolution, and need values at another (also called \nscaling\n). In case we have values at a 100 m x 100 m grid and need values at a 10 m x 10 m grid, the original values will be reused many times, and may be be simply assigned to the nearest high resolution grid cells (\"nearest neighbor\"), or may be interpolated somehow (e.g. by bilinear interpolation). Resampling from finer to coarser grid by nearest neighbor may again be a special case of aggregation.\n\n\nWhen the target grid or time series has a lower resolution (larger grid cells) or lower frequency (longer time intervals) than the source grid, aggregation might be used for resampling. For example, if the resolutions are fairly similar, say the source collection has values for consecutive 10 day intervals and the target needs values for consecutive 16 day intervals, then some form of interpolation may be more appropriate than aggregation as defined here.",
            "title": "Glossary"
        },
        {
            "location": "/glossary/index.html#glossary",
            "text": "This glossary introduces, and tries to define, the major technical terms used in the openEO project.  The acronym  openEO  contracts two concepts:   open : used here in the context of open source software; open source software is available in source code form, and can be freely modified and redistributed; the openEO project will create open source software, reusable under a liberal open source license (Apache 2.0)  EO : Earth observation; openEO targets the processing and analysis of Earth observation data   Further terms:   API : application programming interface ( wikipedia ); a communication protocol between client and back-end  client : software environment (software) that end-users directly interact with, e.g. R (rstudio), Python (jupyter notebook), and JavaScript (web browser); R and Python are two major data science platforms; JavaScript is a major language for web development  (cloud) back-end : server; computer infrastructure (one or more physical computers or virtual machines) used for storing EO data and processing it  big Earth observation cloud back-end  server infrastructure where industry and researchers analyse large amounts of EO data  simple  many end-users now use Python or R to analyse data and JavaScript to develop web applications; analysing large amounts of EO imagery should be equally simple, and seamlessly integrate with existing workflows  unified  current EO cloud back-ends all have  a different API , making EO data analysis hard to validate,difficult to reproduce, and back-ends difficult to compare in terms of capability and costs, or to combine in a joint analysis across back-ends. A unified API can resolve many of these problems.",
            "title": "Glossary"
        },
        {
            "location": "/glossary/index.html#datasets",
            "text": "CEOS ( CEOS OpenSearch Best Practice Document v1.2 ) defines  Granules  and  Collections  as follows:   \"A  granule  is the finest granularity of data that can be independently managed. A granule usually matches the individual file of EO satellite data.\"  \"A  collection  is an aggregation of granules sharing the same product specification. A collection typically corresponds to the series of products derived from data acquired by a sensor on board a satellite and having the same mode of operation.\"   The same document lists the synonyms used (by organisations) for:   granule : dataset (ISO 19115), dataset (ESA), granule (NASA), product (ESA, CNES), scene (JAXA)  collection : dataset series (ISO 19115), collection (CNES, NASA), dataset (JAXA), dataset series (ESA), product (JAXA)   Here, we will use  granule  and  collection .  A  granule  will typically refer to a limited area and a single overpass leading to a very short observation period (seconds), or a temporal aggregation of such data as e.g. for 16-day MODIS composites.  The open geospatial consortium published a document on  OGC OpenSearch Geo and Time Extensions .",
            "title": "Datasets"
        },
        {
            "location": "/glossary/index.html#processes-and-jobs",
            "text": "The terms  process and   process graph  have different meanings in the openEO API specification.  A  process  is simply the description of an operation as provided by the back end, similar to a function definition in programming languages.   In this context openEO will:   consider, or allow to consider, band as a dimension  consider imagery (image collections) to consist of one  or more  collections, as argument to functions; allow filtering on a particular collection, or joining them into a single collection  allow filtering on attributes, e.g. on cloud-free pixels, or pixels inside a  MULTIPOLYGON  describing the floodplains of the Danube. This filters on attributes rather than dimensions.  Provide generic aggregate operations that aggregate over one or more dimensions. Clients may provide dimension-specific aggregation functions for particular cases (such as  min_time )    A  process graph  includes specific process calls, i.e. references to one or more processes including specific values for input arguments similar to a function call in programming. However, process graphs can chain multiple processes. In particular, arguments of processes in general can be again (recursive) process graphs, input datasets, or simple scalar or array values.",
            "title": "Processes and Jobs"
        },
        {
            "location": "/glossary/index.html#user-defined-functions-udfs",
            "text": "The abbreviation  UDF  stands for  user-defined function . With this concept, users are able to upload custom code and have it executed e.g. for every pixel of a scene, allowing custom calculations on server-side data. See the section on  UDFs  for more information.",
            "title": "User-defined functions (UDFs)"
        },
        {
            "location": "/glossary/index.html#aggregation-vs-resampling",
            "text": "Aggregation  computes new values from sets of values that are  uniquely  assigned to groups. It involves a grouping predicate (e.g. monthly, 100 m x 100 m grid cells; think of SQL's  group_by ), and an aggregation function (e.g.,  mean ) that computes one or more new values from the original ones.  Examples:   a time series aggregation may return a regression slope and intercept for every pixel time series, for a single band (group by: full time extent)  a time series may be aggregated to monthly values by computing the mean for all values in a month (group by: months)  spatial  aggregation involves computing e.g.  mean  pixel values on a 100 x 100 m grid, from 10 m x 10 m pixels, where each original pixel is assigned uniquely to a larger pixel (group by: 100 m x 100 m grid cells)   Note that for the first example, the aggregation function not only requires time series values, but also their time stamps.  Resampling  is a broader term where we have data at one resolution, and need values at another (also called  scaling ). In case we have values at a 100 m x 100 m grid and need values at a 10 m x 10 m grid, the original values will be reused many times, and may be be simply assigned to the nearest high resolution grid cells (\"nearest neighbor\"), or may be interpolated somehow (e.g. by bilinear interpolation). Resampling from finer to coarser grid by nearest neighbor may again be a special case of aggregation.  When the target grid or time series has a lower resolution (larger grid cells) or lower frequency (longer time intervals) than the source grid, aggregation might be used for resampling. For example, if the resolutions are fairly similar, say the source collection has values for consecutive 10 day intervals and the target needs values for consecutive 16 day intervals, then some form of interpolation may be more appropriate than aggregation as defined here.",
            "title": "Aggregation vs. resampling"
        },
        {
            "location": "/codeofconduct/index.html",
            "text": "Contributor Code of Conduct\n\n\nAs contributors and maintainers of this project, we pledge to respect all people who \ncontribute through reporting issues, posting feature requests, updating documentation,\nsubmitting pull requests or patches, and other activities.\n\n\nWe are committed to making participation in this project a harassment-free experience for\neveryone, regardless of level of experience, gender, gender identity and expression,\nsexual orientation, disability, personal appearance, body size, race, ethnicity, age, or religion.\n\n\nExamples of unacceptable behavior by participants include the use of sexual language or\nimagery, derogatory comments or personal attacks, trolling, public or private harassment,\ninsults, or other unprofessional conduct.\n\n\nProject maintainers have the right and responsibility to remove, edit, or reject comments,\ncommits, code, wiki edits, issues, and other contributions that are not aligned to this \nCode of Conduct. Project maintainers who do not follow the Code of Conduct may be removed \nfrom the project team.\n\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be reported by \nopening an issue or contacting one or more of the project maintainers.\n\n\nThis Code of Conduct is adapted from the Contributor Covenant \n(http://contributor-covenant.org), version 1.0.0, available at\nhttp://contributor-covenant.org/version/1/0/0/",
            "title": "Contributor Code of Conduct"
        },
        {
            "location": "/codeofconduct/index.html#contributor-code-of-conduct",
            "text": "As contributors and maintainers of this project, we pledge to respect all people who \ncontribute through reporting issues, posting feature requests, updating documentation,\nsubmitting pull requests or patches, and other activities.  We are committed to making participation in this project a harassment-free experience for\neveryone, regardless of level of experience, gender, gender identity and expression,\nsexual orientation, disability, personal appearance, body size, race, ethnicity, age, or religion.  Examples of unacceptable behavior by participants include the use of sexual language or\nimagery, derogatory comments or personal attacks, trolling, public or private harassment,\ninsults, or other unprofessional conduct.  Project maintainers have the right and responsibility to remove, edit, or reject comments,\ncommits, code, wiki edits, issues, and other contributions that are not aligned to this \nCode of Conduct. Project maintainers who do not follow the Code of Conduct may be removed \nfrom the project team.  Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by \nopening an issue or contacting one or more of the project maintainers.  This Code of Conduct is adapted from the Contributor Covenant \n(http://contributor-covenant.org), version 1.0.0, available at\nhttp://contributor-covenant.org/version/1/0/0/",
            "title": "Contributor Code of Conduct"
        },
        {
            "location": "/softwareguidelines/index.html",
            "text": "Software development guidelines\n\n\nThis document describes guidelines for software developers, written\nfor the \nopenEO\n project. Since the openEO\ninfrastructure will encompasses several programming languages and\nsoftware environments, this document does not prescribe particular\ntools or platforms but rather focuses on general principles and\nmethods behind them.\n\n\n\n\nLicense: all software developed in the openeo project and published on the \nopeneo github\n organisation shall be licensed under the \nApache 2.0 license\n. If software repositories deviate from this, or contain code or other artifacts that deviates from this, this shall be described in the first paragraph of the \nREADME.md\n file.\n\n\nLocation: openEO software is developed under the \nopenEO Github organisation\n\n\nProof-of-concept versus sustainable: each repository shall indicate its status: either \nproof-of-concept\n, or \nsustainable\n. Proof-of-concept code is meant to work but comes without quality assurance. Software repositories with proof-of-concept developments shall clearly say so in the title and/or first paragraph of the README.md file.\n\n\nSustainable code should undergo standard \nquality checks\n, and point out its \ndocumentation\n\n\nSustainable code shall undergo \ncode review\n;\nno direct commits to master; any commit shall come in the form of\na PR, commit after review.\n\n\nSustainable code shall be written in a \nTest-driven manner\n, and repositories shall at the top of their \nREADME.md\n give indication of the degree to which code is covered by tests.\n\n\nContinuous integration\n shall be used to indicate code currently passes its test on CI platforms\n\n\nA \nCode of conduct\n describes the rules and constraints to developers and contributors.\n\n\nVersion numbers of sustainable software releases shall follow \nSemantic Versioning 2.0.0\n.  \n\n\n\n\nSoftware quality guidelines\n\n\n\n\nsoftware shall be written in such a way that another person can understand its intention\n\n\ncomment lines shall be used sparsely, but effectively\n\n\nreuse of unstable or esoteric libraries shall be avoided\n\n\n\n\nSoftware documentation guidelines\n\n\nSoftware documentation shall include:\n\n installation instructions\n\n usage instructions\n\n explain in detail the intention of the software\n\n pointers to reference documents explaining overarching concepts \n\n\nEach repository's \nREADME.md\n shall point to the documentation.\n\n\nReference documentation shall be written using well-defined reference documentation language, such as \nRFC2119\n or \narc42\n, and refer to the definitions used.\n\n\nSoftware review\n\n\n\n\nsustainable software development shall take place by always having two persons involved in a change to the master branch: individuals push to branches, pull request indicate readiness to be taken up in the master branch, a second developer reviews the pull request before merging it into the master branch.\n\n\nsoftware review discussions shall be intelligible for external developers, and serve as implicit documentation of development decisions taken\n\n\n\n\nTest-driven development\n\n\nSoftware shall be developed in a test-driven fashion, meaning that while the code is written, tests are developed that verify, to a reasonable extent, the correctness of the code. Tools such as \ncodecov.io\n to automatically indicate the amount of code covered by tests, and code that is not covered by tests shall be used in combination with a continuous integration framework.\n\n\nContinuous integration\n\n\nRepositories containing running software shall use\nan appropriate continuous integration platform, such as\n\ntravis\n or similar, to show whether the\ncurrent build passes all checks. This helps understand contributors\nthat the software passes tests on an independent platform, and\nmay give insights in the way the software is compiled, deployed\nand tested.",
            "title": "Software Development Guidelines"
        },
        {
            "location": "/softwareguidelines/index.html#software-development-guidelines",
            "text": "This document describes guidelines for software developers, written\nfor the  openEO  project. Since the openEO\ninfrastructure will encompasses several programming languages and\nsoftware environments, this document does not prescribe particular\ntools or platforms but rather focuses on general principles and\nmethods behind them.   License: all software developed in the openeo project and published on the  openeo github  organisation shall be licensed under the  Apache 2.0 license . If software repositories deviate from this, or contain code or other artifacts that deviates from this, this shall be described in the first paragraph of the  README.md  file.  Location: openEO software is developed under the  openEO Github organisation  Proof-of-concept versus sustainable: each repository shall indicate its status: either  proof-of-concept , or  sustainable . Proof-of-concept code is meant to work but comes without quality assurance. Software repositories with proof-of-concept developments shall clearly say so in the title and/or first paragraph of the README.md file.  Sustainable code should undergo standard  quality checks , and point out its  documentation  Sustainable code shall undergo  code review ;\nno direct commits to master; any commit shall come in the form of\na PR, commit after review.  Sustainable code shall be written in a  Test-driven manner , and repositories shall at the top of their  README.md  give indication of the degree to which code is covered by tests.  Continuous integration  shall be used to indicate code currently passes its test on CI platforms  A  Code of conduct  describes the rules and constraints to developers and contributors.  Version numbers of sustainable software releases shall follow  Semantic Versioning 2.0.0 .",
            "title": "Software development guidelines"
        },
        {
            "location": "/softwareguidelines/index.html#software-quality-guidelines",
            "text": "software shall be written in such a way that another person can understand its intention  comment lines shall be used sparsely, but effectively  reuse of unstable or esoteric libraries shall be avoided",
            "title": "Software quality guidelines"
        },
        {
            "location": "/softwareguidelines/index.html#software-documentation-guidelines",
            "text": "Software documentation shall include:  installation instructions  usage instructions  explain in detail the intention of the software  pointers to reference documents explaining overarching concepts   Each repository's  README.md  shall point to the documentation.  Reference documentation shall be written using well-defined reference documentation language, such as  RFC2119  or  arc42 , and refer to the definitions used.",
            "title": "Software documentation guidelines"
        },
        {
            "location": "/softwareguidelines/index.html#software-review",
            "text": "sustainable software development shall take place by always having two persons involved in a change to the master branch: individuals push to branches, pull request indicate readiness to be taken up in the master branch, a second developer reviews the pull request before merging it into the master branch.  software review discussions shall be intelligible for external developers, and serve as implicit documentation of development decisions taken",
            "title": "Software review"
        },
        {
            "location": "/softwareguidelines/index.html#test-driven-development",
            "text": "Software shall be developed in a test-driven fashion, meaning that while the code is written, tests are developed that verify, to a reasonable extent, the correctness of the code. Tools such as  codecov.io  to automatically indicate the amount of code covered by tests, and code that is not covered by tests shall be used in combination with a continuous integration framework.",
            "title": "Test-driven development"
        },
        {
            "location": "/softwareguidelines/index.html#continuous-integration",
            "text": "Repositories containing running software shall use\nan appropriate continuous integration platform, such as travis  or similar, to show whether the\ncurrent build passes all checks. This helps understand contributors\nthat the software passes tests on an independent platform, and\nmay give insights in the way the software is compiled, deployed\nand tested.",
            "title": "Continuous integration"
        },
        {
            "location": "/gettingstarted-users/index.html",
            "text": "Getting started for users\n\n\nCurrently, there are three official client libraries and a web-based interface for openEO.\n\n\nIf you are \nunfamiliar\n with programming, you could start using the \nweb-based editor for openEO\n. It supports visual modelling of your algorithms and a simplified JavaScript based access to the openEO workflows and providers. \n\n\nIf you are \nfamiliar\n with programming, you could choose a client library for three programming languages:\n\n\n\n\nJavaScript\n (client-side and server-side)\n\n\nPython\n\n\nR\n\n\n\n\nFollow the links above to find usage instructions for each of the client libraries.\n\n\nDidn't find your programming language? You can also access the \nopenEO API\n implementations directly or start implementing your own client library. Feel free to contact us for further assistance.",
            "title": "Users"
        },
        {
            "location": "/gettingstarted-users/index.html#getting-started-for-users",
            "text": "Currently, there are three official client libraries and a web-based interface for openEO.  If you are  unfamiliar  with programming, you could start using the  web-based editor for openEO . It supports visual modelling of your algorithms and a simplified JavaScript based access to the openEO workflows and providers.   If you are  familiar  with programming, you could choose a client library for three programming languages:   JavaScript  (client-side and server-side)  Python  R   Follow the links above to find usage instructions for each of the client libraries.  Didn't find your programming language? You can also access the  openEO API  implementations directly or start implementing your own client library. Feel free to contact us for further assistance.",
            "title": "Getting started for users"
        },
        {
            "location": "/gettingstarted-backends/index.html",
            "text": "Getting started for back-end providers\n\n\nTBD",
            "title": "Back-end providers"
        },
        {
            "location": "/gettingstarted-backends/index.html#getting-started-for-back-end-providers",
            "text": "TBD",
            "title": "Getting started for back-end providers"
        },
        {
            "location": "/poc/index.html",
            "text": "Proof of Concept (API v0.0.2)\n\n\nThis page gives a detailed description of the openEO proof of concept and gives a list and specification of what needs to be implemented. The proof of concept will consist of\n\n\n\n\nat least three clearly defined example processes (see below),\n\n\na prototypical API specification including communication API call sequences of the processes (see below),\n\n\nimplementations of the processes on three back-ends, and\n\n\nprototypical clients in R, Python and potentially JavaScript.\n\n\n\n\nBelow, we define example use cases and how they are translated to sequences of API calls:\n\n\n\n\nDeriving minimum NDVI measurements over pixel time series of Sentinel 2 imagery\n\n\nCreate a monthly aggregated Sentinel 1 product from a custom Python script\n\n\nCompute time series of zonal (regional) statistics of Sentinel 2 imagery over user-uploaded polygons\n\n\n\n\nNote:\n Authentication is not included in these examples. Enabling authentication needs the placeholder \n<Origin>\n to be set to the requesting host, including protocol, host name/IP and port, e.g. \nhttp://localhost:8080\n. This could be done by using the Origin header value from the request.\n\n\nUse Case 1\n\n\nDeriving minimum NDVI measurements over pixel time series of Sentinel 2 imagery.\n\n\n1. Check whether Sentinel 2A Level 1C data is available at the back-end\n\n\nRequest\n\n\nGET /data/Sentinel2A-L1C HTTP/1.1\n\n\n\n\nResponse\n\n\nHeader:\nHTTP/1.1 200 OK\nContent-Type: application/json; charset=utf-8\nAccess-Control-Allow-Origin: <Origin>\nAccess-Control-Allow-Credentials: true\n\nBody:\n{\n  \"product_id\":\"Sentinel-2A-L1C\",\n  \"description\":\"Sentinel 2 Level-1C: Top-of-atmosphere reflectances in cartographic geometry\",\n  \"source\":\"European Space Agency (ESA)\",\n  \"extent\":[\n    -34,\n    35,\n    39,\n    71\n  ],\n  \"time\":[\n    \"2016-01-01\",\n    \"2017-10-01\"\n  ],\n  \"bands\":[\n    {\n      \"band_id\":\"1\",\n      \"wavelength_nm\":443.9,\n      \"res_m\":60,\n      \"scale\":0.0001,\n      \"offset\":0,\n      \"type\":\"int16\",\n      \"unit\":\"1\"\n    },\n    {\n      \"band_id\":\"2\",\n      \"name\":\"blue\",\n      \"wavelength_nm\":496.6,\n      \"res_m\":10,\n      \"scale\":0.0001,\n      \"offset\":0,\n      \"type\":\"int16\",\n      \"unit\":\"1\"\n    },\n    {\n      \"band_id\":\"3\",\n      \"name\":\"green\",\n      \"wavelength_nm\":560,\n      \"res_m\":10,\n      \"scale\":0.0001,\n      \"offset\":0,\n      \"type\":\"int16\",\n      \"unit\":\"1\"\n    },\n    {\n      \"band_id\":\"4\",\n      \"name\":\"red\",\n      \"wavelength_nm\":664.5,\n      \"res_m\":10,\n      \"scale\":0.0001,\n      \"offset\":0,\n      \"type\":\"int16\",\n      \"unit\":\"1\"\n    },\n    {\n      \"band_id\":\"5\",\n      \"wavelength_nm\":703.9,\n      \"res_m\":20,\n      \"scale\":0.0001,\n      \"offset\":0,\n      \"type\":\"int16\",\n      \"unit\":\"1\"\n    },\n    {\n      \"band_id\":\"6\",\n      \"wavelength_nm\":740.2,\n      \"res_m\":20,\n      \"scale\":0.0001,\n      \"offset\":0,\n      \"type\":\"int16\",\n      \"unit\":\"1\"\n    },\n    {\n      \"band_id\":\"7\",\n      \"wavelength_nm\":782.5,\n      \"res_m\":20,\n      \"scale\":0.0001,\n      \"offset\":0,\n      \"type\":\"int16\",\n      \"unit\":\"1\"\n    },\n    {\n      \"band_id\":\"8\",\n      \"name\":\"nir\",\n      \"wavelength_nm\":835.1,\n      \"res_m\":10,\n      \"scale\":0.0001,\n      \"offset\":0,\n      \"type\":\"int16\",\n      \"unit\":\"1\"\n    },\n    {\n      \"band_id\":\"8a\",\n      \"wavelength_nm\":864.8,\n      \"res_m\":20,\n      \"scale\":0.0001,\n      \"offset\":0,\n      \"type\":\"int16\",\n      \"unit\":\"1\"\n    },\n    {\n      \"band_id\":\"9\",\n      \"wavelength_nm\":945,\n      \"res_m\":60,\n      \"scale\":0.0001,\n      \"offset\":0,\n      \"type\":\"int16\",\n      \"unit\":\"1\"\n    },\n    {\n      \"band_id\":\"10\",\n      \"wavelength_nm\":1373.5,\n      \"res_m\":60,\n      \"scale\":0.0001,\n      \"offset\":0,\n      \"type\":\"int16\",\n      \"unit\":\"1\"\n    },\n    {\n      \"band_id\":\"11\",\n      \"wavelength_nm\":1613.7,\n      \"res_m\":20,\n      \"scale\":0.0001,\n      \"offset\":0,\n      \"type\":\"int16\",\n      \"unit\":\"1\"\n    },\n    {\n      \"band_id\":\"12\",\n      \"wavelength_nm\":2202.4,\n      \"res_m\":20,\n      \"scale\":0.0001,\n      \"offset\":0,\n      \"type\":\"int16\",\n      \"unit\":\"1\"\n    }\n  ]\n}\n\n\n\n\n2. Check that needed processes are available\n\n\nRequest\n\n\nGET /processes/filter_bbox HTTP/1.1\n\n\n\n\nResponse\n\n\nHeader:\nHTTP/1.1 200 OK\nContent-Type: application/json; charset=utf-8\nAccess-Control-Allow-Origin: <Origin>\nAccess-Control-Allow-Credentials: true\n\nBody:\n{\n  \"process_id\":\"filter_bbox\",\n  \"description\":\"Drops observations from a collection that are located outside of a given bounding box.\",\n  \"args\":{\n    \"imagery\":{\n      \"description\":\"array of input collections with one element\"\n    },\n    \"left\":{\n      \"description\":\"left boundary (longitude / easting)\"\n    },\n    \"right\":{\n      \"description\":\"right boundary (longitude / easting)\"\n    },\n    \"top\":{\n      \"description\":\"top boundary (latitude / northing)\"\n    },\n    \"bottom\":{\n      \"description\":\"bottom boundary (latitude / northing)\"\n    },\n    \"srs\":{\n      \"description\":\"spatial reference system of boundaries as proj4 or EPSG:12345 like string\"\n    }\n  }\n}\n\n\n\n\nRequest\n\n\nGET /processes/filter_daterange HTTP/1.1\n\n\n\n\nResponse\n\n\nHeader:\nHTTP/1.1 200 OK\nContent-Type: application/json; charset=utf-8\nAccess-Control-Allow-Origin: <Origin>\nAccess-Control-Allow-Credentials: true\n\nBody:\n{\n  \"process_id\":\"filter_daterange\",\n  \"description\":\"Drops observations from a collection that have been captured before a start or after a given end date.\",\n  \"args\":{\n    \"imagery\":{\n      \"description\":\"array of input collections with one element\"\n    },\n    \"from\":{\n      \"description\":\"start date\"\n    },\n    \"to\":{\n      \"description\":\"end date\"\n    }\n  }\n}\n\n\n\n\nRequest\n\n\nGET /processes/NDVI HTTP/1.1\n\n\n\n\nResponse\n\n\nHeader:\nHTTP/1.1 200 OK\nContent-Type: application/json; charset=utf-8\nAccess-Control-Allow-Origin: <Origin>\nAccess-Control-Allow-Credentials: true\n\nBody:\n{\n  \"process_id\":\"NDVI\",\n  \"description\":\"Finds the minimum value of time series for all bands of the input dataset.\",\n  \"args\":{\n    \"imagery\":{\n      \"description\":\"array of input collections with one element\"\n    },\n    \"red\":{\n      \"description\":\"reference to the red band\"\n    },\n    \"nir\":{\n      \"description\":\"reference to the nir band\"\n    }\n  }\n}\n\n\n\n\nRequest\n\n\nGET /processes/min_time HTTP/1.1\n\n\n\n\nResponse\n\n\nHeader:\nHTTP/1.1 200 OK\nContent-Type: application/json; charset=utf-8\nAccess-Control-Allow-Origin: <Origin>\nAccess-Control-Allow-Credentials: true\n\nBody:\n{\n  \"process_id\":\"min_time\",\n  \"description\":\"Finds the minimum value of time series for all bands of the input dataset.\",\n  \"args\":{\n    \"imagery\":{\n      \"description\":\"array of input collections with one element\"\n    }\n  }\n}\n\n\n\n\n3. Create a job at the back-end\n\n\nRequest\n\n\nPOST /jobs HTTP/1.1\nContent-Type: application/json; charset=utf-8\n\nBody:\n{\n  \"process_graph\":{\n    \"process_id\":\"min_time\",\n    \"args\":{\n      \"imagery\":{\n        \"process_id\":\"NDVI\",\n        \"args\":{\n          \"imagery\":{\n            \"process_id\":\"filter_daterange\",\n            \"args\":{\n              \"imagery\":{\n                \"process_id\":\"filter_bbox\",\n                \"args\":{\n                  \"imagery\":{\n                    \"product_id\":\"S2_L2A_T32TPS_20M\"\n                  },\n                  \"left\":652000,\n                  \"right\":672000,\n                  \"top\":5161000,\n                  \"bottom\":5181000,\n                  \"srs\":\"EPSG:32632\"\n                }\n              },\n              \"from\":\"2017-01-01\",\n              \"to\":\"2017-01-31\"\n            }\n          },\n          \"red\":\"B04\",\n          \"nir\":\"B8A\"\n        }\n      }\n    }\n  }\n}\n\n\n\n\nResponse\n\n\nHeader:\nHTTP/1.1 200 OK\nContent-Type: application/json; charset=utf-8\nAccess-Control-Allow-Origin: <Origin>\nAccess-Control-Allow-Credentials: true\n\nBody:\n{\n  \"job_id\":\"2a8ffb20c2b235a3f3e3351f\",\n  \"status\":\"submitted\",\n  \"submitted\":\"2017-01-01T09:32:12Z\",\n  \"updated\":\"2017-01-01T09:36:18Z\",\n  \"user_id\":\"bd6f9faf93b4\",\n  \"consumed_credits\":0\n}\n\n\n\n\n4. Create a WCS service\n\n\nRequest\n\n\nPOST /services HTTP/1.1\nContent-Type: application/json; charset=utf-8\n\nBody:\n{\n  \"job_id\":\"2a8ffb20c2b235a3f3e3351f\",\n  \"type\":\"wcs\",\n  \"args\":{\n    \"VERSION\":\"2.0.1\"\n  }\n}\n\n\n\n\nResponse\n\n\nHeader:\nHTTP/1.1 200 OK\nContent-Type: application/json; charset=utf-8\nAccess-Control-Allow-Origin: <Origin>\nAccess-Control-Allow-Credentials: true\n\nBody:\n{\n  \"service_id\":\"4dab456f6501bbcd\",\n  \"service_url\":\"https://openeo.org/4dab456f6501bbcd/wcs\",\n  \"service_type\":\"wcs\",\n  \"service_args\":{\n    \"VERSION\":\"2.0.1\"\n  },\n  \"job_id\":\"2a8ffb20c2b235a3f3e3351f\"\n}\n\n\n\n\n5. Download the data on demand with WCS\n\n\nRequest\n\n\nGET https://openeo.org/4dab456f6501bbcd/wcs?SERVICE=WCS&VERSION=2.0.1&REQUEST=GetCapabilities HTTP/1.1\n\n\n\n\nResponse\n\n\nomitted\n\n\nRequest\n\n\nGET https://openeo.org/4dab456f6501bbcd/wcs?SERVICE=WCS&VERSION=2.0.1&REQUEST=GetCoverage&COVERAGEID=2a8ffb20c2b235a3f3e3351f&FORMAT=image/tiff&SUBSET=x,http://www.opengis.net/def/crs/EPSG/0/4326(16.1,16.5)&SUBSET=y,http://www.opengis.net/def/crs/EPSG/0/4326(47.9,48.6)&&SIZE=x(200)&SIZE=y(200) HTTP/1.1\n\n\n\n\nResponse\n \n\nomitted\n\n\n6. Stop the job (and the service)\n\n\nRequest\n\n\nPATCH /jobs/2a8ffb20c2b235a3f3e3351f/cancel HTTP/1.1\n\n\n\n\nResponse\n\n\nHeader:\nHTTP/1.1 200 OK\nAccess-Control-Allow-Origin: <Origin>\nAccess-Control-Allow-Credentials: true\n\nBody: none\n\n\n\n\nUse Case 2\n\n\nCreate a monthly aggregated Sentinel 1 product from a custom Python script.\n\n\n1. Ask the back-end for available Sentinel 1 data\n\n\nRequest\n\n\nGET /data/Sentinel1-L1-IW-GRD HTTP/1.1\n\n\n\n\nResponse\n\n\nHeader:\nHTTP/1.1 200 OK\nContent-Type: application/json; charset=utf-8\nAccess-Control-Allow-Origin: <Origin>\nAccess-Control-Allow-Credentials: true\n\nBody:\n{\n  \"product_id\":\"Sentinel1-L1-IW-GRD\",\n  \"description\":\"Sentinel 1 C-band Synthetic Aperture Radar (SAR) Ground Range Data\",\n  \"source\":\"European Space Agency (ESA)\",\n  \"extent\":[\n    -34,\n    35,\n    39,\n    71\n  ],\n  \"time\":[\n    \"2016-01-01\",\n    \"2017-10-01\"\n  ],\n  \"bands\":[\n    {\n      \"band_id\":\"VV\"\n    },\n    {\n      \"band_id\":\"VH\"\n    }\n  ]\n}\n\n\n\n\n2. Ask the back-end whether it supports Python UDFs of type aggregate_time and get details about expected parameters\n\n\nRequest\n\n\nGET /udf_runtimes HTTP/1.1\n\n\n\n\nResponse\n\n\nHeader:\nHTTP/1.1 200 OK\nContent-Type: application/json; charset=utf-8\nAccess-Control-Allow-Origin: <Origin>\nAccess-Control-Allow-Credentials: true\n\nBody:\n{\n  \"Python\":{\n    \"udf_types\":[\n      \"reduce_time\",\n      \"aggregate_time\",\n      \"apply_pixel\"\n    ],\n    \"versions\":{\n      \"3.6.3\":{\n        \"packages\":[\n          \"numpy\",\n          \"scipy\",\n          \"pandas\",\n          \"matplotlib\",\n          \"ipython\",\n          \"jupyter\",\n          \"GDAL\"\n        ]\n      }\n    }\n  }\n}\n\n\n\n\nRequest\n\n\nGET /udf_runtimes/Python/aggregate_time HTTP/1.1\n\n\n\n\nResponse\n\n\nHeader:\nHTTP/1.1 200 OK\nContent-Type: application/json; charset=utf-8\nAccess-Control-Allow-Origin: <Origin>\nAccess-Control-Allow-Credentials: true\n\n{\n  \"process_id\":\"/udf/Python/aggregate_time\",\n  \"description\":\"Runs a Python script for each time series of the input dataset.\",\n  \"args\":{\n    \"imagery\":{\n      \"description\":\"array of input collections with one element\"\n    },\n    \"script\":{\n      \"description\":\"Python script that will be executed over all time series, gets time series as (Pandas) DataFrame and expects a new DataFrame as output.\"\n    },\n    \"version\":{\n      \"description\":\"Python version to use, defaults to the latest available version.\",\n      \"required\":false,\n      \"default\":\"latest\"\n    }\n  }\n}\n\n\n\n\n3. Upload python script\n\n\nRequest\n\n\nPUT /users/me/files/s1_aggregate.py HTTP/1.1\n\n\n\n\nResponse\n\n\nHeader:\nHTTP/1.1 200 OK\nAccess-Control-Allow-Origin: <Origin>\nAccess-Control-Allow-Credentials: true\n\nBody: none\n\n\n\n\n4. Create a job\n\n\nRequest\n\n\nPOST /jobs HTTP/1.1\nContent-Type: application/json; charset=utf-8\n\nBody:\n{\n  \"process_graph\":{\n    \"process_id\":\"/udf/Python/aggregate_time\",\n    \"args\":{\n      \"script\":\"/users/me/files/s1_aggregate.py\",\n      \"imagery\":{\n        \"process_id\":\"filter_daterange\",\n        \"args\":{\n          \"imagery\":{\n            \"process_id\":\"filter_bbox\",\n            \"args\":{\n              \"imagery\":{\n                \"product_id\":\"Sentinel1-L1-IW-GRD\"\n              },\n              \"left\":16.1,\n              \"right\":16.6,\n              \"top\":48.6,\n              \"bottom\":47.2,\n              \"srs\":\"EPSG:4326\"\n            }\n          },\n          \"from\":\"2017-01-01\",\n          \"to\":\"2017-01-31\"\n        }\n      }\n    }\n  }\n}\n\n\n\n\nResponse\n\n\nHeader:\nHTTP/1.1 200 OK\nContent-Type: application/json; charset=utf-8\nAccess-Control-Allow-Origin: <Origin>\nAccess-Control-Allow-Credentials: true\n\nBody:\n{\n  \"job_id\":\"3723c32fb7b24698832ca71f2d3f18aa\",\n  \"status\":\"submitted\",\n  \"submitted\":\"2017-01-01T09:32:12Z\",\n  \"updated\":\"2017-01-01T09:36:18Z\",\n  \"user_id\":\"bd6f9faf93b4\",\n  \"consumed_credits\":0\n}\n\n\n\n\n5. Create a TMS service\n\n\nRequest\n\n\nPOST /services HTTP/1.1\nContent-Type: application/json; charset=utf-8\n\nBody:\n{\n  \"job_id\":\"3723c32fb7b24698832ca71f2d3f18aa\",\n  \"type\":\"tms\"\n}\n\n\n\n\nResponse\n\n\nHeader:\nHTTP/1.1 200 OK\nContent-Type: application/json; charset=utf-8\nAccess-Control-Allow-Origin: <Origin>\nAccess-Control-Allow-Credentials: true\n\nBody:\n{\n  \"service_id\":\"9dab4b6f6523\",\n  \"service_url\":\"http://cdn.cloudprovider.com/openeo/services/9dab4b6f6523/tms\",\n  \"service_type\":\"tms\",\n  \"job_id\":\"3723c32fb7b24698832ca71f2d3f18aa\"\n}\n\n\n\n\n6. Download results as TMS\n\n\nExample Request\n\n\nGET http://cdn.cloudprovider.com/openeo/services/9dab4b6f6523/tms/2017-01-01/12/2232/2668/?bands=1 HTTP/1.1\n\n\n\n\nResponse\n\n\nomitted\n\n\nUse Case 3\n\n\nCompute time series of zonal (regional) statistics of Sentinel 2 imagery over user-uploaded polygons\n\n\n1. Check whether Sentinel 2A Level 1C data is available at the back-end\n\n\nRequest\n\n\nGET /data/Sentinel2A-L1C HTTP/1.1\n\n\n\n\nResponse\n\n\nHeader:\nHTTP/1.1 200 OK\nContent-Type: application/json; charset=utf-8\nAccess-Control-Allow-Origin: <Origin>\nAccess-Control-Allow-Credentials: true\n\nBody:\n{\n  \"product_id\":\"Sentinel-2A-L1C\",\n  \"description\":\"Sentinel 2 Level-1C: Top-of-atmosphere reflectances in cartographic geometry\",\n  \"source\":\"European Space Agency (ESA)\",\n  \"extent\":[\n    -34,\n    35,\n    39,\n    71\n  ],\n  \"time\":[\n    \"2016-01-01\",\n    \"2017-10-01\"\n  ],\n  \"bands\":[\n    {\n      \"band_id\":\"1\",\n      \"wavelength_nm\":443.9,\n      \"res_m\":60,\n      \"scale\":0.0001,\n      \"offset\":0,\n      \"type\":\"int16\",\n      \"unit\":\"1\"\n    },\n    {\n      \"band_id\":\"2\",\n      \"name\":\"blue\",\n      \"wavelength_nm\":496.6,\n      \"res_m\":10,\n      \"scale\":0.0001,\n      \"offset\":0,\n      \"type\":\"int16\",\n      \"unit\":\"1\"\n    },\n    {\n      \"band_id\":\"3\",\n      \"name\":\"green\",\n      \"wavelength_nm\":560,\n      \"res_m\":10,\n      \"scale\":0.0001,\n      \"offset\":0,\n      \"type\":\"int16\",\n      \"unit\":\"1\"\n    },\n    {\n      \"band_id\":\"4\",\n      \"name\":\"red\",\n      \"wavelength_nm\":664.5,\n      \"res_m\":10,\n      \"scale\":0.0001,\n      \"offset\":0,\n      \"type\":\"int16\",\n      \"unit\":\"1\"\n    },\n    {\n      \"band_id\":\"5\",\n      \"wavelength_nm\":703.9,\n      \"res_m\":20,\n      \"scale\":0.0001,\n      \"offset\":0,\n      \"type\":\"int16\",\n      \"unit\":\"1\"\n    },\n    {\n      \"band_id\":\"6\",\n      \"wavelength_nm\":740.2,\n      \"res_m\":20,\n      \"scale\":0.0001,\n      \"offset\":0,\n      \"type\":\"int16\",\n      \"unit\":\"1\"\n    },\n    {\n      \"band_id\":\"7\",\n      \"wavelength_nm\":782.5,\n      \"res_m\":20,\n      \"scale\":0.0001,\n      \"offset\":0,\n      \"type\":\"int16\",\n      \"unit\":\"1\"\n    },\n    {\n      \"band_id\":\"8\",\n      \"name\":\"nir\",\n      \"wavelength_nm\":835.1,\n      \"res_m\":10,\n      \"scale\":0.0001,\n      \"offset\":0,\n      \"type\":\"int16\",\n      \"unit\":\"1\"\n    },\n    {\n      \"band_id\":\"8a\",\n      \"wavelength_nm\":864.8,\n      \"res_m\":20,\n      \"scale\":0.0001,\n      \"offset\":0,\n      \"type\":\"int16\",\n      \"unit\":\"1\"\n    },\n    {\n      \"band_id\":\"9\",\n      \"wavelength_nm\":945,\n      \"res_m\":60,\n      \"scale\":0.0001,\n      \"offset\":0,\n      \"type\":\"int16\",\n      \"unit\":\"1\"\n    },\n    {\n      \"band_id\":\"10\",\n      \"wavelength_nm\":1373.5,\n      \"res_m\":60,\n      \"scale\":0.0001,\n      \"offset\":0,\n      \"type\":\"int16\",\n      \"unit\":\"1\"\n    },\n    {\n      \"band_id\":\"11\",\n      \"wavelength_nm\":1613.7,\n      \"res_m\":20,\n      \"scale\":0.0001,\n      \"offset\":0,\n      \"type\":\"int16\",\n      \"unit\":\"1\"\n    },\n    {\n      \"band_id\":\"12\",\n      \"wavelength_nm\":2202.4,\n      \"res_m\":20,\n      \"scale\":0.0001,\n      \"offset\":0,\n      \"type\":\"int16\",\n      \"unit\":\"1\"\n    }\n  ]\n}\n\n\n\n\n\n\n2. Check whether the back-end supports computing \nzonal_statistics\n\n\nRequest\n\n\nGET /processes/zonal_statistics HTTP/1.1\n\n\n\n\nResponse\n\n\nHeader:\nHTTP/1.1 200 OK\nContent-Type: application/json; charset=utf-8\nAccess-Control-Allow-Origin: <Origin>\nAccess-Control-Allow-Credentials: true\n\nBody:\n{\n  \"process_id\":\"zonal_statistics\",\n  \"description\":\"Runs a Python script for each time series of the input dataset.\",\n  \"args\":{\n    \"imagery\":{\n      \"description\":\"array of input collections with one element\"\n    },\n    \"regions\":{\n      \"description\":\"Polygon file readable by OGR\"\n    },\n    \"func\":{\n      \"description\":\"Function to apply over the polygons, one of `avg`, `min`, `max`, `median`, `q25`, or `q75`.\",\n      \"required\":false,\n      \"default\":\"avg\"\n    }\n  }\n}\n\n\n\n\n3. Upload a GeoJSON Polygon\n\n\nRequest\n\n\nPUT /user/me/files/polygon1.json HTTP/1.1\n\n\n\n\nResponse\n\n\nHeader:\nHTTP/1.1 200 OK\nAccess-Control-Allow-Origin: <Origin>\nAccess-Control-Allow-Credentials: true\n\nBody: none\n\n\n\n\n4. Create a job\n\n\nRequest\n\n\nPOST /jobs HTTP/1.1\nContent-Type: application/json; charset=utf-8\n\nBody:\n{\n  \"process_graph\":{\n    \"process_id\":\"zonal_statistics\",\n    \"args\":{\n      \"imagery\":{\n        \"process_id\":\"filter_daterange\",\n        \"args\":{\n          \"imagery\":{\n            \"process_id\":\"filter_bbox\",\n            \"args\":{\n              \"imagery\":{\n                \"process_id\":\"filter_bands\",\n                \"args\":{\n                  \"imagery\":{\n                    \"product_id\":\"Sentinel2-L1C\"\n                  },\n                  \"bands\":8\n                }\n              },\n              \"left\":16.1,\n              \"right\":16.6,\n              \"top\":48.6,\n              \"bottom\":47.2,\n              \"srs\":\"EPSG:4326\"\n            }\n          },\n          \"from\":\"2017-01-01\",\n          \"to\":\"2017-01-31\"\n        }\n      },\n      \"regions\":\"/users/me/files/\",\n      \"func\":\"avg\"\n    }\n  },\n  \"output\":{\n    \"format\":\"GPKG\"\n  }\n}\n\n\n\n\nResponse\n\n\nHeader:\nHTTP/1.1 200 OK\nContent-Type: application/json; charset=utf-8\nAccess-Control-Allow-Origin: <Origin>\nAccess-Control-Allow-Credentials: true\n\nBody:\n{\n  \"job_id\":\"f6ea12c5e283438a921b525af826da08\",\n  \"status\":\"submitted\",\n  \"submitted\":\"2017-01-01T09:32:12Z\",\n  \"updated\":\"2017-01-01T09:36:18Z\",\n  \"user_id\":\"bd6f9faf93b4\",\n  \"consumed_credits\":0\n}\n\n\n\n\n5. Start batch computation at the back-end\n\n\nRequest\n\n\nPATCH /jobs/f6ea12c5e283438a921b525af826da08/queue HTTP/1.1\n\n\n\n\nResponse\n\n\nHeader:\nHTTP/1.1 200 OK\nAccess-Control-Allow-Origin: <Origin>\nAccess-Control-Allow-Credentials: true\n\nBody: none\n\n\n\n\n6. Check job status twice\n\n\nRequest\n\n\nGET /jobs/f6ea12c5e283438a921b525af826da08 HTTP/1.1\n\n\n\n\nResponse\n\n\nHeader:\nHTTP/1.1 200 OK\nContent-Type: application/json; charset=utf-8\nAccess-Control-Allow-Origin: <Origin>\nAccess-Control-Allow-Credentials: true\n\nBody:\n{\n  \"job_id\":\"f6ea12c5e283438a921b525af826da08\",\n  \"user_id\":\"bd6f9faf93b4\",\n  \"status\":\"running\",\n  \"process_graph\":{\n    \"process_id\":\"zonal_statistics\",\n    \"args\":{\n      \"imagery\":{\n        \"process_id\":\"filter_daterange\",\n        \"args\":{\n          \"imagery\":{\n            \"process_id\":\"filter_bbox\",\n            \"args\":{\n              \"imagery\":{\n                \"process_id\":\"filter_bands\",\n                \"args\":{\n                  \"imagery\":{\n                    \"product_id\":\"Sentinel2-L1C\"\n                  },\n                  \"bands\":8\n                }\n              },\n              \"left\":16.1,\n              \"right\":16.6,\n              \"top\":48.6,\n              \"bottom\":47.2,\n              \"srs\":\"EPSG:4326\"\n            }\n          },\n          \"from\":\"2017-01-01\",\n          \"to\":\"2017-01-31\"\n        }\n      },\n      \"regions\":\"/users/me/files/\",\n      \"func\":\"avg\"\n    }\n  },\n  \"output\":{\n    \"format\":\"GPKG\"\n  },\n  \"submitted\":\"2017-01-01 09:32:12\",\n  \"updated\":\"2017-01-01 09:34:11\",\n  \"consumed_credits\":231\n}\n\n\n\n\nRequest\n\n\nGET /jobs/f6ea12c5e283438a921b525af826da08 HTTP/1.1\n\n\n\n\nResponse\n\n\nHeader:\nHTTP/1.1 200 OK\nContent-Type: application/json; charset=utf-8\nAccess-Control-Allow-Origin: <Origin>\nAccess-Control-Allow-Credentials: true\n\nBody:\n{\n  \"job_id\":\"f6ea12c5e283438a921b525af826da08\",\n  \"user_id\":\"bd6f9faf93b4\",\n  \"status\":\"finished\",\n  \"process_graph\":{\n    \"process_id\":\"zonal_statistics\",\n    \"args\":{\n      \"imagery\":{\n        \"process_id\":\"filter_daterange\",\n        \"args\":{\n          \"imagery\":{\n            \"process_id\":\"filter_bbox\",\n            \"args\":{\n              \"imagery\":{\n                \"process_id\":\"filter_bands\",\n                \"args\":{\n                  \"imagery\":{\n                    \"product_id\":\"Sentinel2-L1C\"\n                  },\n                  \"bands\":8\n                }\n              },\n              \"left\":16.1,\n              \"right\":16.6,\n              \"top\":48.6,\n              \"bottom\":47.2,\n              \"srs\":\"EPSG:4326\"\n            }\n          },\n          \"from\":\"2017-01-01\",\n          \"to\":\"2017-01-31\"\n        }\n      },\n      \"regions\":\"/users/me/files/\",\n      \"func\":\"avg\"\n    }\n  },\n  \"output\":{\n    \"format\":\"GPKG\"\n  },\n  \"submitted\":\"2017-01-01 09:32:12\",\n  \"updated\":\"2017-01-01 09:36:57\",\n  \"consumed_credits\":450\n}\n\n\n\n\n7. Retrieve download links\n\n\nRequest\n\n\nGET /jobs/f6ea12c5e283438a921b525af826da08/download HTTP/1.1\n\n\n\n\nResponse\n\n\nHeader:\nHTTP/1.1 200 OK\nContent-Type: application/json; charset=utf-8\nAccess-Control-Allow-Origin: <Origin>\nAccess-Control-Allow-Credentials: true\n\nBody:\n[\n  \"https://cdn.openeo.org/4854b51643548ab8a858e2b8282711d8/1.gpkg\"\n]\n\n\n\n\n\n\n8. Download file(s)\n\n\nRequest\n\n\nGET https://cdn.openeo.org/4854b51643548ab8a858e2b8282711d8/1.gpkg HTTP/1.1\n\n\n\n\nResponse (GPKG file)\n\n\nomitted",
            "title": "Examples / Proof of Concept"
        },
        {
            "location": "/poc/index.html#proof-of-concept-api-v002",
            "text": "This page gives a detailed description of the openEO proof of concept and gives a list and specification of what needs to be implemented. The proof of concept will consist of   at least three clearly defined example processes (see below),  a prototypical API specification including communication API call sequences of the processes (see below),  implementations of the processes on three back-ends, and  prototypical clients in R, Python and potentially JavaScript.   Below, we define example use cases and how they are translated to sequences of API calls:   Deriving minimum NDVI measurements over pixel time series of Sentinel 2 imagery  Create a monthly aggregated Sentinel 1 product from a custom Python script  Compute time series of zonal (regional) statistics of Sentinel 2 imagery over user-uploaded polygons   Note:  Authentication is not included in these examples. Enabling authentication needs the placeholder  <Origin>  to be set to the requesting host, including protocol, host name/IP and port, e.g.  http://localhost:8080 . This could be done by using the Origin header value from the request.",
            "title": "Proof of Concept (API v0.0.2)"
        },
        {
            "location": "/poc/index.html#use-case-1",
            "text": "",
            "title": "Use Case 1"
        },
        {
            "location": "/poc/index.html#deriving-minimum-ndvi-measurements-over-pixel-time-series-of-sentinel-2-imagery",
            "text": "",
            "title": "Deriving minimum NDVI measurements over pixel time series of Sentinel 2 imagery."
        },
        {
            "location": "/poc/index.html#1-check-whether-sentinel-2a-level-1c-data-is-available-at-the-back-end",
            "text": "Request  GET /data/Sentinel2A-L1C HTTP/1.1  Response  Header:\nHTTP/1.1 200 OK\nContent-Type: application/json; charset=utf-8\nAccess-Control-Allow-Origin: <Origin>\nAccess-Control-Allow-Credentials: true\n\nBody:\n{\n  \"product_id\":\"Sentinel-2A-L1C\",\n  \"description\":\"Sentinel 2 Level-1C: Top-of-atmosphere reflectances in cartographic geometry\",\n  \"source\":\"European Space Agency (ESA)\",\n  \"extent\":[\n    -34,\n    35,\n    39,\n    71\n  ],\n  \"time\":[\n    \"2016-01-01\",\n    \"2017-10-01\"\n  ],\n  \"bands\":[\n    {\n      \"band_id\":\"1\",\n      \"wavelength_nm\":443.9,\n      \"res_m\":60,\n      \"scale\":0.0001,\n      \"offset\":0,\n      \"type\":\"int16\",\n      \"unit\":\"1\"\n    },\n    {\n      \"band_id\":\"2\",\n      \"name\":\"blue\",\n      \"wavelength_nm\":496.6,\n      \"res_m\":10,\n      \"scale\":0.0001,\n      \"offset\":0,\n      \"type\":\"int16\",\n      \"unit\":\"1\"\n    },\n    {\n      \"band_id\":\"3\",\n      \"name\":\"green\",\n      \"wavelength_nm\":560,\n      \"res_m\":10,\n      \"scale\":0.0001,\n      \"offset\":0,\n      \"type\":\"int16\",\n      \"unit\":\"1\"\n    },\n    {\n      \"band_id\":\"4\",\n      \"name\":\"red\",\n      \"wavelength_nm\":664.5,\n      \"res_m\":10,\n      \"scale\":0.0001,\n      \"offset\":0,\n      \"type\":\"int16\",\n      \"unit\":\"1\"\n    },\n    {\n      \"band_id\":\"5\",\n      \"wavelength_nm\":703.9,\n      \"res_m\":20,\n      \"scale\":0.0001,\n      \"offset\":0,\n      \"type\":\"int16\",\n      \"unit\":\"1\"\n    },\n    {\n      \"band_id\":\"6\",\n      \"wavelength_nm\":740.2,\n      \"res_m\":20,\n      \"scale\":0.0001,\n      \"offset\":0,\n      \"type\":\"int16\",\n      \"unit\":\"1\"\n    },\n    {\n      \"band_id\":\"7\",\n      \"wavelength_nm\":782.5,\n      \"res_m\":20,\n      \"scale\":0.0001,\n      \"offset\":0,\n      \"type\":\"int16\",\n      \"unit\":\"1\"\n    },\n    {\n      \"band_id\":\"8\",\n      \"name\":\"nir\",\n      \"wavelength_nm\":835.1,\n      \"res_m\":10,\n      \"scale\":0.0001,\n      \"offset\":0,\n      \"type\":\"int16\",\n      \"unit\":\"1\"\n    },\n    {\n      \"band_id\":\"8a\",\n      \"wavelength_nm\":864.8,\n      \"res_m\":20,\n      \"scale\":0.0001,\n      \"offset\":0,\n      \"type\":\"int16\",\n      \"unit\":\"1\"\n    },\n    {\n      \"band_id\":\"9\",\n      \"wavelength_nm\":945,\n      \"res_m\":60,\n      \"scale\":0.0001,\n      \"offset\":0,\n      \"type\":\"int16\",\n      \"unit\":\"1\"\n    },\n    {\n      \"band_id\":\"10\",\n      \"wavelength_nm\":1373.5,\n      \"res_m\":60,\n      \"scale\":0.0001,\n      \"offset\":0,\n      \"type\":\"int16\",\n      \"unit\":\"1\"\n    },\n    {\n      \"band_id\":\"11\",\n      \"wavelength_nm\":1613.7,\n      \"res_m\":20,\n      \"scale\":0.0001,\n      \"offset\":0,\n      \"type\":\"int16\",\n      \"unit\":\"1\"\n    },\n    {\n      \"band_id\":\"12\",\n      \"wavelength_nm\":2202.4,\n      \"res_m\":20,\n      \"scale\":0.0001,\n      \"offset\":0,\n      \"type\":\"int16\",\n      \"unit\":\"1\"\n    }\n  ]\n}",
            "title": "1. Check whether Sentinel 2A Level 1C data is available at the back-end"
        },
        {
            "location": "/poc/index.html#2-check-that-needed-processes-are-available",
            "text": "Request  GET /processes/filter_bbox HTTP/1.1  Response  Header:\nHTTP/1.1 200 OK\nContent-Type: application/json; charset=utf-8\nAccess-Control-Allow-Origin: <Origin>\nAccess-Control-Allow-Credentials: true\n\nBody:\n{\n  \"process_id\":\"filter_bbox\",\n  \"description\":\"Drops observations from a collection that are located outside of a given bounding box.\",\n  \"args\":{\n    \"imagery\":{\n      \"description\":\"array of input collections with one element\"\n    },\n    \"left\":{\n      \"description\":\"left boundary (longitude / easting)\"\n    },\n    \"right\":{\n      \"description\":\"right boundary (longitude / easting)\"\n    },\n    \"top\":{\n      \"description\":\"top boundary (latitude / northing)\"\n    },\n    \"bottom\":{\n      \"description\":\"bottom boundary (latitude / northing)\"\n    },\n    \"srs\":{\n      \"description\":\"spatial reference system of boundaries as proj4 or EPSG:12345 like string\"\n    }\n  }\n}  Request  GET /processes/filter_daterange HTTP/1.1  Response  Header:\nHTTP/1.1 200 OK\nContent-Type: application/json; charset=utf-8\nAccess-Control-Allow-Origin: <Origin>\nAccess-Control-Allow-Credentials: true\n\nBody:\n{\n  \"process_id\":\"filter_daterange\",\n  \"description\":\"Drops observations from a collection that have been captured before a start or after a given end date.\",\n  \"args\":{\n    \"imagery\":{\n      \"description\":\"array of input collections with one element\"\n    },\n    \"from\":{\n      \"description\":\"start date\"\n    },\n    \"to\":{\n      \"description\":\"end date\"\n    }\n  }\n}  Request  GET /processes/NDVI HTTP/1.1  Response  Header:\nHTTP/1.1 200 OK\nContent-Type: application/json; charset=utf-8\nAccess-Control-Allow-Origin: <Origin>\nAccess-Control-Allow-Credentials: true\n\nBody:\n{\n  \"process_id\":\"NDVI\",\n  \"description\":\"Finds the minimum value of time series for all bands of the input dataset.\",\n  \"args\":{\n    \"imagery\":{\n      \"description\":\"array of input collections with one element\"\n    },\n    \"red\":{\n      \"description\":\"reference to the red band\"\n    },\n    \"nir\":{\n      \"description\":\"reference to the nir band\"\n    }\n  }\n}  Request  GET /processes/min_time HTTP/1.1  Response  Header:\nHTTP/1.1 200 OK\nContent-Type: application/json; charset=utf-8\nAccess-Control-Allow-Origin: <Origin>\nAccess-Control-Allow-Credentials: true\n\nBody:\n{\n  \"process_id\":\"min_time\",\n  \"description\":\"Finds the minimum value of time series for all bands of the input dataset.\",\n  \"args\":{\n    \"imagery\":{\n      \"description\":\"array of input collections with one element\"\n    }\n  }\n}",
            "title": "2. Check that needed processes are available"
        },
        {
            "location": "/poc/index.html#3-create-a-job-at-the-back-end",
            "text": "Request  POST /jobs HTTP/1.1\nContent-Type: application/json; charset=utf-8\n\nBody:\n{\n  \"process_graph\":{\n    \"process_id\":\"min_time\",\n    \"args\":{\n      \"imagery\":{\n        \"process_id\":\"NDVI\",\n        \"args\":{\n          \"imagery\":{\n            \"process_id\":\"filter_daterange\",\n            \"args\":{\n              \"imagery\":{\n                \"process_id\":\"filter_bbox\",\n                \"args\":{\n                  \"imagery\":{\n                    \"product_id\":\"S2_L2A_T32TPS_20M\"\n                  },\n                  \"left\":652000,\n                  \"right\":672000,\n                  \"top\":5161000,\n                  \"bottom\":5181000,\n                  \"srs\":\"EPSG:32632\"\n                }\n              },\n              \"from\":\"2017-01-01\",\n              \"to\":\"2017-01-31\"\n            }\n          },\n          \"red\":\"B04\",\n          \"nir\":\"B8A\"\n        }\n      }\n    }\n  }\n}  Response  Header:\nHTTP/1.1 200 OK\nContent-Type: application/json; charset=utf-8\nAccess-Control-Allow-Origin: <Origin>\nAccess-Control-Allow-Credentials: true\n\nBody:\n{\n  \"job_id\":\"2a8ffb20c2b235a3f3e3351f\",\n  \"status\":\"submitted\",\n  \"submitted\":\"2017-01-01T09:32:12Z\",\n  \"updated\":\"2017-01-01T09:36:18Z\",\n  \"user_id\":\"bd6f9faf93b4\",\n  \"consumed_credits\":0\n}",
            "title": "3. Create a job at the back-end"
        },
        {
            "location": "/poc/index.html#4-create-a-wcs-service",
            "text": "Request  POST /services HTTP/1.1\nContent-Type: application/json; charset=utf-8\n\nBody:\n{\n  \"job_id\":\"2a8ffb20c2b235a3f3e3351f\",\n  \"type\":\"wcs\",\n  \"args\":{\n    \"VERSION\":\"2.0.1\"\n  }\n}  Response  Header:\nHTTP/1.1 200 OK\nContent-Type: application/json; charset=utf-8\nAccess-Control-Allow-Origin: <Origin>\nAccess-Control-Allow-Credentials: true\n\nBody:\n{\n  \"service_id\":\"4dab456f6501bbcd\",\n  \"service_url\":\"https://openeo.org/4dab456f6501bbcd/wcs\",\n  \"service_type\":\"wcs\",\n  \"service_args\":{\n    \"VERSION\":\"2.0.1\"\n  },\n  \"job_id\":\"2a8ffb20c2b235a3f3e3351f\"\n}",
            "title": "4. Create a WCS service"
        },
        {
            "location": "/poc/index.html#5-download-the-data-on-demand-with-wcs",
            "text": "Request  GET https://openeo.org/4dab456f6501bbcd/wcs?SERVICE=WCS&VERSION=2.0.1&REQUEST=GetCapabilities HTTP/1.1  Response  omitted  Request  GET https://openeo.org/4dab456f6501bbcd/wcs?SERVICE=WCS&VERSION=2.0.1&REQUEST=GetCoverage&COVERAGEID=2a8ffb20c2b235a3f3e3351f&FORMAT=image/tiff&SUBSET=x,http://www.opengis.net/def/crs/EPSG/0/4326(16.1,16.5)&SUBSET=y,http://www.opengis.net/def/crs/EPSG/0/4326(47.9,48.6)&&SIZE=x(200)&SIZE=y(200) HTTP/1.1  Response   omitted",
            "title": "5. Download the data on demand with WCS"
        },
        {
            "location": "/poc/index.html#6-stop-the-job-and-the-service",
            "text": "Request  PATCH /jobs/2a8ffb20c2b235a3f3e3351f/cancel HTTP/1.1  Response  Header:\nHTTP/1.1 200 OK\nAccess-Control-Allow-Origin: <Origin>\nAccess-Control-Allow-Credentials: true\n\nBody: none",
            "title": "6. Stop the job (and the service)"
        },
        {
            "location": "/poc/index.html#use-case-2",
            "text": "",
            "title": "Use Case 2"
        },
        {
            "location": "/poc/index.html#create-a-monthly-aggregated-sentinel-1-product-from-a-custom-python-script",
            "text": "",
            "title": "Create a monthly aggregated Sentinel 1 product from a custom Python script."
        },
        {
            "location": "/poc/index.html#1-ask-the-back-end-for-available-sentinel-1-data",
            "text": "Request  GET /data/Sentinel1-L1-IW-GRD HTTP/1.1  Response  Header:\nHTTP/1.1 200 OK\nContent-Type: application/json; charset=utf-8\nAccess-Control-Allow-Origin: <Origin>\nAccess-Control-Allow-Credentials: true\n\nBody:\n{\n  \"product_id\":\"Sentinel1-L1-IW-GRD\",\n  \"description\":\"Sentinel 1 C-band Synthetic Aperture Radar (SAR) Ground Range Data\",\n  \"source\":\"European Space Agency (ESA)\",\n  \"extent\":[\n    -34,\n    35,\n    39,\n    71\n  ],\n  \"time\":[\n    \"2016-01-01\",\n    \"2017-10-01\"\n  ],\n  \"bands\":[\n    {\n      \"band_id\":\"VV\"\n    },\n    {\n      \"band_id\":\"VH\"\n    }\n  ]\n}",
            "title": "1. Ask the back-end for available Sentinel 1 data"
        },
        {
            "location": "/poc/index.html#2-ask-the-back-end-whether-it-supports-python-udfs-of-type-aggregate_time-and-get-details-about-expected-parameters",
            "text": "Request  GET /udf_runtimes HTTP/1.1  Response  Header:\nHTTP/1.1 200 OK\nContent-Type: application/json; charset=utf-8\nAccess-Control-Allow-Origin: <Origin>\nAccess-Control-Allow-Credentials: true\n\nBody:\n{\n  \"Python\":{\n    \"udf_types\":[\n      \"reduce_time\",\n      \"aggregate_time\",\n      \"apply_pixel\"\n    ],\n    \"versions\":{\n      \"3.6.3\":{\n        \"packages\":[\n          \"numpy\",\n          \"scipy\",\n          \"pandas\",\n          \"matplotlib\",\n          \"ipython\",\n          \"jupyter\",\n          \"GDAL\"\n        ]\n      }\n    }\n  }\n}  Request  GET /udf_runtimes/Python/aggregate_time HTTP/1.1  Response  Header:\nHTTP/1.1 200 OK\nContent-Type: application/json; charset=utf-8\nAccess-Control-Allow-Origin: <Origin>\nAccess-Control-Allow-Credentials: true\n\n{\n  \"process_id\":\"/udf/Python/aggregate_time\",\n  \"description\":\"Runs a Python script for each time series of the input dataset.\",\n  \"args\":{\n    \"imagery\":{\n      \"description\":\"array of input collections with one element\"\n    },\n    \"script\":{\n      \"description\":\"Python script that will be executed over all time series, gets time series as (Pandas) DataFrame and expects a new DataFrame as output.\"\n    },\n    \"version\":{\n      \"description\":\"Python version to use, defaults to the latest available version.\",\n      \"required\":false,\n      \"default\":\"latest\"\n    }\n  }\n}",
            "title": "2. Ask the back-end whether it supports Python UDFs of type aggregate_time and get details about expected parameters"
        },
        {
            "location": "/poc/index.html#3-upload-python-script",
            "text": "Request  PUT /users/me/files/s1_aggregate.py HTTP/1.1  Response  Header:\nHTTP/1.1 200 OK\nAccess-Control-Allow-Origin: <Origin>\nAccess-Control-Allow-Credentials: true\n\nBody: none",
            "title": "3. Upload python script"
        },
        {
            "location": "/poc/index.html#4-create-a-job",
            "text": "Request  POST /jobs HTTP/1.1\nContent-Type: application/json; charset=utf-8\n\nBody:\n{\n  \"process_graph\":{\n    \"process_id\":\"/udf/Python/aggregate_time\",\n    \"args\":{\n      \"script\":\"/users/me/files/s1_aggregate.py\",\n      \"imagery\":{\n        \"process_id\":\"filter_daterange\",\n        \"args\":{\n          \"imagery\":{\n            \"process_id\":\"filter_bbox\",\n            \"args\":{\n              \"imagery\":{\n                \"product_id\":\"Sentinel1-L1-IW-GRD\"\n              },\n              \"left\":16.1,\n              \"right\":16.6,\n              \"top\":48.6,\n              \"bottom\":47.2,\n              \"srs\":\"EPSG:4326\"\n            }\n          },\n          \"from\":\"2017-01-01\",\n          \"to\":\"2017-01-31\"\n        }\n      }\n    }\n  }\n}  Response  Header:\nHTTP/1.1 200 OK\nContent-Type: application/json; charset=utf-8\nAccess-Control-Allow-Origin: <Origin>\nAccess-Control-Allow-Credentials: true\n\nBody:\n{\n  \"job_id\":\"3723c32fb7b24698832ca71f2d3f18aa\",\n  \"status\":\"submitted\",\n  \"submitted\":\"2017-01-01T09:32:12Z\",\n  \"updated\":\"2017-01-01T09:36:18Z\",\n  \"user_id\":\"bd6f9faf93b4\",\n  \"consumed_credits\":0\n}",
            "title": "4. Create a job"
        },
        {
            "location": "/poc/index.html#5-create-a-tms-service",
            "text": "Request  POST /services HTTP/1.1\nContent-Type: application/json; charset=utf-8\n\nBody:\n{\n  \"job_id\":\"3723c32fb7b24698832ca71f2d3f18aa\",\n  \"type\":\"tms\"\n}  Response  Header:\nHTTP/1.1 200 OK\nContent-Type: application/json; charset=utf-8\nAccess-Control-Allow-Origin: <Origin>\nAccess-Control-Allow-Credentials: true\n\nBody:\n{\n  \"service_id\":\"9dab4b6f6523\",\n  \"service_url\":\"http://cdn.cloudprovider.com/openeo/services/9dab4b6f6523/tms\",\n  \"service_type\":\"tms\",\n  \"job_id\":\"3723c32fb7b24698832ca71f2d3f18aa\"\n}",
            "title": "5. Create a TMS service"
        },
        {
            "location": "/poc/index.html#6-download-results-as-tms",
            "text": "Example Request  GET http://cdn.cloudprovider.com/openeo/services/9dab4b6f6523/tms/2017-01-01/12/2232/2668/?bands=1 HTTP/1.1  Response  omitted",
            "title": "6. Download results as TMS"
        },
        {
            "location": "/poc/index.html#use-case-3",
            "text": "",
            "title": "Use Case 3"
        },
        {
            "location": "/poc/index.html#compute-time-series-of-zonal-regional-statistics-of-sentinel-2-imagery-over-user-uploaded-polygons",
            "text": "",
            "title": "Compute time series of zonal (regional) statistics of Sentinel 2 imagery over user-uploaded polygons"
        },
        {
            "location": "/poc/index.html#1-check-whether-sentinel-2a-level-1c-data-is-available-at-the-back-end_1",
            "text": "Request  GET /data/Sentinel2A-L1C HTTP/1.1  Response  Header:\nHTTP/1.1 200 OK\nContent-Type: application/json; charset=utf-8\nAccess-Control-Allow-Origin: <Origin>\nAccess-Control-Allow-Credentials: true\n\nBody:\n{\n  \"product_id\":\"Sentinel-2A-L1C\",\n  \"description\":\"Sentinel 2 Level-1C: Top-of-atmosphere reflectances in cartographic geometry\",\n  \"source\":\"European Space Agency (ESA)\",\n  \"extent\":[\n    -34,\n    35,\n    39,\n    71\n  ],\n  \"time\":[\n    \"2016-01-01\",\n    \"2017-10-01\"\n  ],\n  \"bands\":[\n    {\n      \"band_id\":\"1\",\n      \"wavelength_nm\":443.9,\n      \"res_m\":60,\n      \"scale\":0.0001,\n      \"offset\":0,\n      \"type\":\"int16\",\n      \"unit\":\"1\"\n    },\n    {\n      \"band_id\":\"2\",\n      \"name\":\"blue\",\n      \"wavelength_nm\":496.6,\n      \"res_m\":10,\n      \"scale\":0.0001,\n      \"offset\":0,\n      \"type\":\"int16\",\n      \"unit\":\"1\"\n    },\n    {\n      \"band_id\":\"3\",\n      \"name\":\"green\",\n      \"wavelength_nm\":560,\n      \"res_m\":10,\n      \"scale\":0.0001,\n      \"offset\":0,\n      \"type\":\"int16\",\n      \"unit\":\"1\"\n    },\n    {\n      \"band_id\":\"4\",\n      \"name\":\"red\",\n      \"wavelength_nm\":664.5,\n      \"res_m\":10,\n      \"scale\":0.0001,\n      \"offset\":0,\n      \"type\":\"int16\",\n      \"unit\":\"1\"\n    },\n    {\n      \"band_id\":\"5\",\n      \"wavelength_nm\":703.9,\n      \"res_m\":20,\n      \"scale\":0.0001,\n      \"offset\":0,\n      \"type\":\"int16\",\n      \"unit\":\"1\"\n    },\n    {\n      \"band_id\":\"6\",\n      \"wavelength_nm\":740.2,\n      \"res_m\":20,\n      \"scale\":0.0001,\n      \"offset\":0,\n      \"type\":\"int16\",\n      \"unit\":\"1\"\n    },\n    {\n      \"band_id\":\"7\",\n      \"wavelength_nm\":782.5,\n      \"res_m\":20,\n      \"scale\":0.0001,\n      \"offset\":0,\n      \"type\":\"int16\",\n      \"unit\":\"1\"\n    },\n    {\n      \"band_id\":\"8\",\n      \"name\":\"nir\",\n      \"wavelength_nm\":835.1,\n      \"res_m\":10,\n      \"scale\":0.0001,\n      \"offset\":0,\n      \"type\":\"int16\",\n      \"unit\":\"1\"\n    },\n    {\n      \"band_id\":\"8a\",\n      \"wavelength_nm\":864.8,\n      \"res_m\":20,\n      \"scale\":0.0001,\n      \"offset\":0,\n      \"type\":\"int16\",\n      \"unit\":\"1\"\n    },\n    {\n      \"band_id\":\"9\",\n      \"wavelength_nm\":945,\n      \"res_m\":60,\n      \"scale\":0.0001,\n      \"offset\":0,\n      \"type\":\"int16\",\n      \"unit\":\"1\"\n    },\n    {\n      \"band_id\":\"10\",\n      \"wavelength_nm\":1373.5,\n      \"res_m\":60,\n      \"scale\":0.0001,\n      \"offset\":0,\n      \"type\":\"int16\",\n      \"unit\":\"1\"\n    },\n    {\n      \"band_id\":\"11\",\n      \"wavelength_nm\":1613.7,\n      \"res_m\":20,\n      \"scale\":0.0001,\n      \"offset\":0,\n      \"type\":\"int16\",\n      \"unit\":\"1\"\n    },\n    {\n      \"band_id\":\"12\",\n      \"wavelength_nm\":2202.4,\n      \"res_m\":20,\n      \"scale\":0.0001,\n      \"offset\":0,\n      \"type\":\"int16\",\n      \"unit\":\"1\"\n    }\n  ]\n}",
            "title": "1. Check whether Sentinel 2A Level 1C data is available at the back-end"
        },
        {
            "location": "/poc/index.html#2-check-whether-the-back-end-supports-computing-zonal_statistics",
            "text": "Request  GET /processes/zonal_statistics HTTP/1.1  Response  Header:\nHTTP/1.1 200 OK\nContent-Type: application/json; charset=utf-8\nAccess-Control-Allow-Origin: <Origin>\nAccess-Control-Allow-Credentials: true\n\nBody:\n{\n  \"process_id\":\"zonal_statistics\",\n  \"description\":\"Runs a Python script for each time series of the input dataset.\",\n  \"args\":{\n    \"imagery\":{\n      \"description\":\"array of input collections with one element\"\n    },\n    \"regions\":{\n      \"description\":\"Polygon file readable by OGR\"\n    },\n    \"func\":{\n      \"description\":\"Function to apply over the polygons, one of `avg`, `min`, `max`, `median`, `q25`, or `q75`.\",\n      \"required\":false,\n      \"default\":\"avg\"\n    }\n  }\n}",
            "title": "2. Check whether the back-end supports computing zonal_statistics"
        },
        {
            "location": "/poc/index.html#3-upload-a-geojson-polygon",
            "text": "Request  PUT /user/me/files/polygon1.json HTTP/1.1  Response  Header:\nHTTP/1.1 200 OK\nAccess-Control-Allow-Origin: <Origin>\nAccess-Control-Allow-Credentials: true\n\nBody: none",
            "title": "3. Upload a GeoJSON Polygon"
        },
        {
            "location": "/poc/index.html#4-create-a-job_1",
            "text": "Request  POST /jobs HTTP/1.1\nContent-Type: application/json; charset=utf-8\n\nBody:\n{\n  \"process_graph\":{\n    \"process_id\":\"zonal_statistics\",\n    \"args\":{\n      \"imagery\":{\n        \"process_id\":\"filter_daterange\",\n        \"args\":{\n          \"imagery\":{\n            \"process_id\":\"filter_bbox\",\n            \"args\":{\n              \"imagery\":{\n                \"process_id\":\"filter_bands\",\n                \"args\":{\n                  \"imagery\":{\n                    \"product_id\":\"Sentinel2-L1C\"\n                  },\n                  \"bands\":8\n                }\n              },\n              \"left\":16.1,\n              \"right\":16.6,\n              \"top\":48.6,\n              \"bottom\":47.2,\n              \"srs\":\"EPSG:4326\"\n            }\n          },\n          \"from\":\"2017-01-01\",\n          \"to\":\"2017-01-31\"\n        }\n      },\n      \"regions\":\"/users/me/files/\",\n      \"func\":\"avg\"\n    }\n  },\n  \"output\":{\n    \"format\":\"GPKG\"\n  }\n}  Response  Header:\nHTTP/1.1 200 OK\nContent-Type: application/json; charset=utf-8\nAccess-Control-Allow-Origin: <Origin>\nAccess-Control-Allow-Credentials: true\n\nBody:\n{\n  \"job_id\":\"f6ea12c5e283438a921b525af826da08\",\n  \"status\":\"submitted\",\n  \"submitted\":\"2017-01-01T09:32:12Z\",\n  \"updated\":\"2017-01-01T09:36:18Z\",\n  \"user_id\":\"bd6f9faf93b4\",\n  \"consumed_credits\":0\n}",
            "title": "4. Create a job"
        },
        {
            "location": "/poc/index.html#5-start-batch-computation-at-the-back-end",
            "text": "Request  PATCH /jobs/f6ea12c5e283438a921b525af826da08/queue HTTP/1.1  Response  Header:\nHTTP/1.1 200 OK\nAccess-Control-Allow-Origin: <Origin>\nAccess-Control-Allow-Credentials: true\n\nBody: none",
            "title": "5. Start batch computation at the back-end"
        },
        {
            "location": "/poc/index.html#6-check-job-status-twice",
            "text": "Request  GET /jobs/f6ea12c5e283438a921b525af826da08 HTTP/1.1  Response  Header:\nHTTP/1.1 200 OK\nContent-Type: application/json; charset=utf-8\nAccess-Control-Allow-Origin: <Origin>\nAccess-Control-Allow-Credentials: true\n\nBody:\n{\n  \"job_id\":\"f6ea12c5e283438a921b525af826da08\",\n  \"user_id\":\"bd6f9faf93b4\",\n  \"status\":\"running\",\n  \"process_graph\":{\n    \"process_id\":\"zonal_statistics\",\n    \"args\":{\n      \"imagery\":{\n        \"process_id\":\"filter_daterange\",\n        \"args\":{\n          \"imagery\":{\n            \"process_id\":\"filter_bbox\",\n            \"args\":{\n              \"imagery\":{\n                \"process_id\":\"filter_bands\",\n                \"args\":{\n                  \"imagery\":{\n                    \"product_id\":\"Sentinel2-L1C\"\n                  },\n                  \"bands\":8\n                }\n              },\n              \"left\":16.1,\n              \"right\":16.6,\n              \"top\":48.6,\n              \"bottom\":47.2,\n              \"srs\":\"EPSG:4326\"\n            }\n          },\n          \"from\":\"2017-01-01\",\n          \"to\":\"2017-01-31\"\n        }\n      },\n      \"regions\":\"/users/me/files/\",\n      \"func\":\"avg\"\n    }\n  },\n  \"output\":{\n    \"format\":\"GPKG\"\n  },\n  \"submitted\":\"2017-01-01 09:32:12\",\n  \"updated\":\"2017-01-01 09:34:11\",\n  \"consumed_credits\":231\n}  Request  GET /jobs/f6ea12c5e283438a921b525af826da08 HTTP/1.1  Response  Header:\nHTTP/1.1 200 OK\nContent-Type: application/json; charset=utf-8\nAccess-Control-Allow-Origin: <Origin>\nAccess-Control-Allow-Credentials: true\n\nBody:\n{\n  \"job_id\":\"f6ea12c5e283438a921b525af826da08\",\n  \"user_id\":\"bd6f9faf93b4\",\n  \"status\":\"finished\",\n  \"process_graph\":{\n    \"process_id\":\"zonal_statistics\",\n    \"args\":{\n      \"imagery\":{\n        \"process_id\":\"filter_daterange\",\n        \"args\":{\n          \"imagery\":{\n            \"process_id\":\"filter_bbox\",\n            \"args\":{\n              \"imagery\":{\n                \"process_id\":\"filter_bands\",\n                \"args\":{\n                  \"imagery\":{\n                    \"product_id\":\"Sentinel2-L1C\"\n                  },\n                  \"bands\":8\n                }\n              },\n              \"left\":16.1,\n              \"right\":16.6,\n              \"top\":48.6,\n              \"bottom\":47.2,\n              \"srs\":\"EPSG:4326\"\n            }\n          },\n          \"from\":\"2017-01-01\",\n          \"to\":\"2017-01-31\"\n        }\n      },\n      \"regions\":\"/users/me/files/\",\n      \"func\":\"avg\"\n    }\n  },\n  \"output\":{\n    \"format\":\"GPKG\"\n  },\n  \"submitted\":\"2017-01-01 09:32:12\",\n  \"updated\":\"2017-01-01 09:36:57\",\n  \"consumed_credits\":450\n}",
            "title": "6. Check job status twice"
        },
        {
            "location": "/poc/index.html#7-retrieve-download-links",
            "text": "Request  GET /jobs/f6ea12c5e283438a921b525af826da08/download HTTP/1.1  Response  Header:\nHTTP/1.1 200 OK\nContent-Type: application/json; charset=utf-8\nAccess-Control-Allow-Origin: <Origin>\nAccess-Control-Allow-Credentials: true\n\nBody:\n[\n  \"https://cdn.openeo.org/4854b51643548ab8a858e2b8282711d8/1.gpkg\"\n]",
            "title": "7. Retrieve download links"
        },
        {
            "location": "/poc/index.html#8-download-files",
            "text": "Request  GET https://cdn.openeo.org/4854b51643548ab8a858e2b8282711d8/1.gpkg HTTP/1.1  Response (GPKG file)  omitted",
            "title": "8. Download file(s)"
        },
        {
            "location": "/arch/index.html",
            "text": "Architecture\n\n\nThe openEO API defines a language how clients communicate to back-ends in order to analyze large Earth observation datasets. The API will be implemented by drivers for specific back-ends. Some first architecture considerations are listed below.\n\n\n\n\nThe openEO API is a contract between clients and back-ends that describes the communication only\n\n\nEach back-end runs its own API instance including the specific back-end driver. There is no API instance that runs more than one driver.\n\n\nClients in R, Python, and JavaScript connect directly to the back-ends and communicate with the back-ends over \nHTTPS\n according to the openEO API specification.\n\n\nAPI instances can run on back-end servers or additional intermediate layers, which then communicate to back-ends in a back-end specific way.\n\n\nBack-ends may add functionality and extend the API wherever there is need.\n\n\nThere will be a central back-end registry service (openEO Hub), to allow users to search for back-ends with specific functionality and or data. \n\n\nThe openEO API will define \nprofiles\n in order group specific functionality.\n\n\n\n\n\n\nMicroservices\n\n\nTo simplify and structure the development, the API is divided into a few microservices.\n\n\n\n\n\n\n\n\nMicroservice\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nCapabilities\n\n\nThis microservice reports on the capabilities of the back-end, i.e. which API endpoints are implemented, which authentication methods are supported, and whether and how UDFs can be executed at the back-end.\n\n\n\n\n\n\nEO Data Discovery\n\n\nDescribes which datasets and image collections are available at the back-end.\n\n\n\n\n\n\nProcess Discovery\n\n\nProvides services to find out which processes a back-end provides, i.e., what users can do with the available data.\n\n\n\n\n\n\nUDF Runtime Discovery\n\n\nAllows discovering the programming languages and their runtime environments to execute user-defined functions.\n\n\n\n\n\n\nJob Management\n\n\nOrganizes and manages jobs that run processes on back-ends.\n\n\n\n\n\n\nFile Management\n\n\nOrganizes and manages user-uploaded files.\n\n\n\n\n\n\nProcess Graph Management\n\n\nOrganizes and manages user-defined process graphs.\n\n\n\n\n\n\nWeb Service Management\n\n\nWeb services to to access data and job results, e.g. as WCS or WMTS service.\n\n\n\n\n\n\nUser Content\n\n\nUser content and accounting.\n\n\n\n\n\n\nAuthentication\n\n\nAuthentication of users.",
            "title": "Architecture"
        },
        {
            "location": "/arch/index.html#architecture",
            "text": "The openEO API defines a language how clients communicate to back-ends in order to analyze large Earth observation datasets. The API will be implemented by drivers for specific back-ends. Some first architecture considerations are listed below.   The openEO API is a contract between clients and back-ends that describes the communication only  Each back-end runs its own API instance including the specific back-end driver. There is no API instance that runs more than one driver.  Clients in R, Python, and JavaScript connect directly to the back-ends and communicate with the back-ends over  HTTPS  according to the openEO API specification.  API instances can run on back-end servers or additional intermediate layers, which then communicate to back-ends in a back-end specific way.  Back-ends may add functionality and extend the API wherever there is need.  There will be a central back-end registry service (openEO Hub), to allow users to search for back-ends with specific functionality and or data.   The openEO API will define  profiles  in order group specific functionality.",
            "title": "Architecture"
        },
        {
            "location": "/arch/index.html#microservices",
            "text": "To simplify and structure the development, the API is divided into a few microservices.     Microservice  Description      Capabilities  This microservice reports on the capabilities of the back-end, i.e. which API endpoints are implemented, which authentication methods are supported, and whether and how UDFs can be executed at the back-end.    EO Data Discovery  Describes which datasets and image collections are available at the back-end.    Process Discovery  Provides services to find out which processes a back-end provides, i.e., what users can do with the available data.    UDF Runtime Discovery  Allows discovering the programming languages and their runtime environments to execute user-defined functions.    Job Management  Organizes and manages jobs that run processes on back-ends.    File Management  Organizes and manages user-uploaded files.    Process Graph Management  Organizes and manages user-defined process graphs.    Web Service Management  Web services to to access data and job results, e.g. as WCS or WMTS service.    User Content  User content and accounting.    Authentication  Authentication of users.",
            "title": "Microservices"
        },
        {
            "location": "/usermanagement/index.html",
            "text": "User Management and Accounting\n\n\nIn general, the openEO API only defines a minimum subset of user management and accounting functionality. It allows to\n\n\n\n\nauthenticate and authorize\n a user, which may include \nuser registration with OpenID Connect\n,\n\n\nhandle storage space limits (disk quota),\n\n\nmanage billing, which includes to\n\n\nquery the credit a user has available,\n\n\nestimate costs for certain operations,\n\n\nget information about produced costs,\n\n\nlimit costs of certain operations.\n\n\n\n\nTherefore, the API leaves some aspects open that have to be handled by the back-ends separately, including \n\n\n\n\ncredential recovery, e.g. retrieving a forgotten password\n\n\nuser data management, e.g. changing the users payment details or email address\n\n\npayments, i.e. topping up credits for pre-paid services or paying for post-paid services\n\n\nother accounting related tasks, e.g. creating invoices,\n\n\nuser registration (only specified when OpenID Connect is implemented).",
            "title": "User Management and Accounting"
        },
        {
            "location": "/usermanagement/index.html#user-management-and-accounting",
            "text": "In general, the openEO API only defines a minimum subset of user management and accounting functionality. It allows to   authenticate and authorize  a user, which may include  user registration with OpenID Connect ,  handle storage space limits (disk quota),  manage billing, which includes to  query the credit a user has available,  estimate costs for certain operations,  get information about produced costs,  limit costs of certain operations.   Therefore, the API leaves some aspects open that have to be handled by the back-ends separately, including    credential recovery, e.g. retrieving a forgotten password  user data management, e.g. changing the users payment details or email address  payments, i.e. topping up credits for pre-paid services or paying for post-paid services  other accounting related tasks, e.g. creating invoices,  user registration (only specified when OpenID Connect is implemented).",
            "title": "User Management and Accounting"
        },
        {
            "location": "/jobs/index.html",
            "text": "Processing data using a process graph\n\n\nProcess graphs can be executed in three different ways.\n\n\nResults can be pre-computed by creating a \nbatch job\n using  \nPOST /jobs\n.  They are submitted to the back office's processing system, but will remain inactive until \nPOST /jobs/{job_id}/results\n has been called. They will run only once and store results after execution. Results can be downloaded. Batch jobs are typically time consuming such that user interaction is not possible.\n\n\nAnother way of processing and accessing data are \nweb services\n. Web services allow web-based access using different protocols such as \nOGC WMS\n, \nOGC WCS\n or \nXYZ tiles\n. These protocols usually allow users to change the viewing extent or level of detail (zoom level). Therefore, computations may run \non demand\n, i.e. the requested data is calculated during the request. Back-ends should make sure to cache processed data to avoid additional/high costs and waiting times for the user.\n\n\nProcess graphs can also be \nexecuted  synchronously\n (\nPOST /jobs/previews\n). Results are delivered with the request itself and no job is created. Only lightweight computations, for example small previews, should be executed using this approach as timeouts are to be expected for \nlong-polling HTTP requests\n.\n\n\nData processing details\n\n\nHeterogeneous datasets are unified by the back-ends based on the processes in the process graphs. For instance, the difference between a PROBA-V image and a Sentinel image, which have e a different projection and resolution, are automatically resampled and projected by the back-ends as soon as it is required to do so. Clients are not responsible to ensure that the data matches by first applying resampling or projections processes.\n\n\nTemporal references are always specified on the basis of the \nGregorian calendar\n.\n\n\nExamples\n\n\nSynchronously executed jobs\n\n\nRetrieval of a GeoTIFF\n\n\nRequest\n\n\nHeader:\nPOST /preview HTTP/1.1\nContent-Type: application/json; charset=utf-8\n\nBody:\n{\n  \"process_graph\":{\n    \"process_id\":\"min_time\",\n    \"imagery\":{\n      \"process_id\":\"NDVI\",\n      \"imagery\":{\n        \"process_id\":\"filter_daterange\",\n        \"imagery\":{\n          \"process_id\":\"filter_bbox\",\n          \"imagery\":{\n            \"process_id\":\"get_data\",\n            \"data_id\":\"S2_L2A_T32TPS_20M\"\n          },\n          \"left\":652000,\n          \"right\":672000,\n          \"top\":5161000,\n          \"bottom\":5181000,\n          \"srs\":\"EPSG:32632\"\n        },\n        \"from\":\"2017-01-01\",\n        \"to\":\"2017-01-31\"\n      },\n      \"red\":\"B04\",\n      \"nir\":\"B8A\"\n    }\n  },\n  \"output\":{\n    \"format\":\"GTiff\",\n    \"args\":{\n      \"tiled\":true,\n      \"compress\":\"jpeg\",\n      \"photometric\":\"YCBCR\",\n      \"jpeg_quality\":80\n    }\n  }\n}\n\n\n\n\nResponse\n \n\n\nHeader:\nHTTP/1.1 200 OK\nContent-Type: image/tiff\nAccess-Control-Allow-Origin: <Origin>\n\nBody:\nomitted (the GeoTiff file contents)\n\n\n\n\nRetrieval of time series\n\n\nRequest\n\n\nHeader:\nPOST /preview HTTP/1.1\nContent-Type: application/json; charset=utf-8\n\nBody:\n{\n  \"process_graph\":{\n    \"process_id\":\"zonal_statistics\",\n    \"imagery\":{\n      \"process_id\":\"filter_daterange\",\n      \"imagery\":{\n        \"process_id\":\"filter_bbox\",\n        \"imagery\":{\n          \"process_id\":\"filter_bands\",\n          \"imagery\":{\n            \"process_id\":\"get_data\",\n            \"data_id\":\"Sentinel2-L1C\"\n          },\n          \"bands\":8\n        },\n        \"left\":16.1,\n        \"right\":16.6,\n        \"top\":48.6,\n        \"bottom\":47.2,\n        \"srs\":\"EPSG:4326\"\n      },\n      \"from\":\"2017-01-01\",\n      \"to\":\"2017-01-31\"\n    },\n    \"regions\":\"/users/me/files/\",\n    \"func\":\"avg\"\n  },\n  \"output\":{\n    \"format\":\"GPKG\"\n  }\n}\n\n\n\n\nResponse\n \n\n\nHeader:\nHTTP/1.1 200 OK\nContent-Type: application/octet-stream\nAccess-Control-Allow-Origin: <Origin>\n\nBody:\nomitted (the GeoPackage file contents)",
            "title": "Jobs"
        },
        {
            "location": "/jobs/index.html#processing-data-using-a-process-graph",
            "text": "Process graphs can be executed in three different ways.  Results can be pre-computed by creating a  batch job  using   POST /jobs .  They are submitted to the back office's processing system, but will remain inactive until  POST /jobs/{job_id}/results  has been called. They will run only once and store results after execution. Results can be downloaded. Batch jobs are typically time consuming such that user interaction is not possible.  Another way of processing and accessing data are  web services . Web services allow web-based access using different protocols such as  OGC WMS ,  OGC WCS  or  XYZ tiles . These protocols usually allow users to change the viewing extent or level of detail (zoom level). Therefore, computations may run  on demand , i.e. the requested data is calculated during the request. Back-ends should make sure to cache processed data to avoid additional/high costs and waiting times for the user.  Process graphs can also be  executed  synchronously  ( POST /jobs/previews ). Results are delivered with the request itself and no job is created. Only lightweight computations, for example small previews, should be executed using this approach as timeouts are to be expected for  long-polling HTTP requests .",
            "title": "Processing data using a process graph"
        },
        {
            "location": "/jobs/index.html#data-processing-details",
            "text": "Heterogeneous datasets are unified by the back-ends based on the processes in the process graphs. For instance, the difference between a PROBA-V image and a Sentinel image, which have e a different projection and resolution, are automatically resampled and projected by the back-ends as soon as it is required to do so. Clients are not responsible to ensure that the data matches by first applying resampling or projections processes.  Temporal references are always specified on the basis of the  Gregorian calendar .",
            "title": "Data processing details"
        },
        {
            "location": "/jobs/index.html#examples",
            "text": "",
            "title": "Examples"
        },
        {
            "location": "/jobs/index.html#synchronously-executed-jobs",
            "text": "",
            "title": "Synchronously executed jobs"
        },
        {
            "location": "/jobs/index.html#retrieval-of-a-geotiff",
            "text": "Request  Header:\nPOST /preview HTTP/1.1\nContent-Type: application/json; charset=utf-8\n\nBody:\n{\n  \"process_graph\":{\n    \"process_id\":\"min_time\",\n    \"imagery\":{\n      \"process_id\":\"NDVI\",\n      \"imagery\":{\n        \"process_id\":\"filter_daterange\",\n        \"imagery\":{\n          \"process_id\":\"filter_bbox\",\n          \"imagery\":{\n            \"process_id\":\"get_data\",\n            \"data_id\":\"S2_L2A_T32TPS_20M\"\n          },\n          \"left\":652000,\n          \"right\":672000,\n          \"top\":5161000,\n          \"bottom\":5181000,\n          \"srs\":\"EPSG:32632\"\n        },\n        \"from\":\"2017-01-01\",\n        \"to\":\"2017-01-31\"\n      },\n      \"red\":\"B04\",\n      \"nir\":\"B8A\"\n    }\n  },\n  \"output\":{\n    \"format\":\"GTiff\",\n    \"args\":{\n      \"tiled\":true,\n      \"compress\":\"jpeg\",\n      \"photometric\":\"YCBCR\",\n      \"jpeg_quality\":80\n    }\n  }\n}  Response    Header:\nHTTP/1.1 200 OK\nContent-Type: image/tiff\nAccess-Control-Allow-Origin: <Origin>\n\nBody:\nomitted (the GeoTiff file contents)",
            "title": "Retrieval of a GeoTIFF"
        },
        {
            "location": "/jobs/index.html#retrieval-of-time-series",
            "text": "Request  Header:\nPOST /preview HTTP/1.1\nContent-Type: application/json; charset=utf-8\n\nBody:\n{\n  \"process_graph\":{\n    \"process_id\":\"zonal_statistics\",\n    \"imagery\":{\n      \"process_id\":\"filter_daterange\",\n      \"imagery\":{\n        \"process_id\":\"filter_bbox\",\n        \"imagery\":{\n          \"process_id\":\"filter_bands\",\n          \"imagery\":{\n            \"process_id\":\"get_data\",\n            \"data_id\":\"Sentinel2-L1C\"\n          },\n          \"bands\":8\n        },\n        \"left\":16.1,\n        \"right\":16.6,\n        \"top\":48.6,\n        \"bottom\":47.2,\n        \"srs\":\"EPSG:4326\"\n      },\n      \"from\":\"2017-01-01\",\n      \"to\":\"2017-01-31\"\n    },\n    \"regions\":\"/users/me/files/\",\n    \"func\":\"avg\"\n  },\n  \"output\":{\n    \"format\":\"GPKG\"\n  }\n}  Response    Header:\nHTTP/1.1 200 OK\nContent-Type: application/octet-stream\nAccess-Control-Allow-Origin: <Origin>\n\nBody:\nomitted (the GeoPackage file contents)",
            "title": "Retrieval of time series"
        },
        {
            "location": "/udfs/index.html",
            "text": "User-defined functions\n\n\nWork in progress!\n\n\nThe abbreviation \nUDF\n stands for \nuser-defined function\n. With this concept, users are able to upload custom code and have it executed e.g. for every pixel of a scene, allowing custom calculations on server-side data.\n\n\nUser-defined functions (UDFs) can be exposed to the data in different ways. This includes which parts of the data are passed to the function, how the function execution is parallelized, and how the expected output is structured.",
            "title": "UDFs"
        },
        {
            "location": "/udfs/index.html#user-defined-functions",
            "text": "Work in progress!  The abbreviation  UDF  stands for  user-defined function . With this concept, users are able to upload custom code and have it executed e.g. for every pixel of a scene, allowing custom calculations on server-side data.  User-defined functions (UDFs) can be exposed to the data in different ways. This includes which parts of the data are passed to the function, how the function execution is parallelized, and how the expected output is structured.",
            "title": "User-defined functions"
        },
        {
            "location": "/processes/index.html",
            "text": "Processes\n\n\nA process is defined in the API specification. It is an operation that performs a specific task on a set of parameters and returns a result. It's definition includes a name, a set of parameters, a return type, a set of possible errors or exceptions and some other metadata. \n\n\n\n\nThe name is the identifier for the process and MUST never contain a forward slash \n/\n. \n\n\nEach parameter has a name and the valid content follows a schema.\n\n\nThe content returned by a process also follows a schema.\n\n\nThe schema usually defines the data type and a format according to JSON schema. There are openEO specific formats defined below.\n\n\n\n\nopenEO specific formats\n\n\nTBD\n\n\nCore Processes\n\n\nThere are some processes that we define to be core processes that should be implemented by all back-ends:\n\n\n\n\nget_data\n\n\nfilter_bands\n\n\nfilter_daterange\n\n\nprocess_graph\n\n\nto be continued...\n\n\n\n\nNote:\n Currently there are only few defined processes. Those are currently only meant as an example how future documentation of processes may look like.\n\n\nget_data\n\n\nFilters and selects a single dataset provided by the back-end.\n\n\nArguments\n\n\nAny of the properties of a dataset, e.g.\n\n\n\n\ndata_id\n: Filter by data id\n\n\nextent\n: Filter by extent\n\n\ntime\n: Filter by time\n\n\nbands\n: Filter by band ids\n\n\n...\n\n\n\n\nThe back-end provider decides which of the potential datasets is the most relevant one to be selected.\n\n\nExamples\n\n\n{\n  \"process_id\": \"get_data\",\n  \"data_id\":\"Sentinel2A-L1C\"\n}\n\n\n\n\n{\n  \"process_id\": \"get_data\",\n  \"platform\": \"landsat-7\",\n  \"sensor\": \"modis\", \n  \"derived_from\": null\n}\n\n\n\n\nfilter_bands\n\n\nAllows to extract one or multiple bands of multi-band raster image collection. Bands can be chosen either by band id, band name or by wavelength.\n\n\nArguments\n\n\n\n\nimagery\n \n(required)\n: Image collection to filter\n\n\n\n\nAnd one of:\n\n\n\n\nbands\n: string or array of strings containing band ids.\n\n\nnames\n: string or array of strings containing band names.\n\n\nwavelengths\n: number or two-element array of numbers containing a wavelength or a minimum and maximum wavelength respectively.\n\n\n\n\nExamples\n\n\n{\n  \"process_id\":\"filter_bands\",\n  \"imagery\":{\n    \"process_id\":\"get_data\",\n    \"data_id\":\"Sentinel2A-L1C\"\n  },\n  \"bands\":\"1\"\n}\n\n\n\n\n{\n  \"process_id\":\"filter_bands\",\n  \"imagery\":{\n    \"process_id\":\"get_data\",\n    \"data_id\":\"Sentinel2A-L1C\"\n  },\n  \"wavelengths\":[\n    1300,\n    2000\n  ]\n}\n\n\n\n\nfilter_daterange\n\n\nAllows to filter an image collection by temporal extent.\n\n\nArguments\n\n\n\n\nimagery\n \n(required)\n: Image collection to filter\n\n\n\n\nAnd at least one of:\n\n\n\n\nfrom\n: Includes all data newer than the specified ISO 8601 date or date-time with simultaneous consideration of \nto\n.\n\n\nto\n: Includes all data older than the specified ISO 8601 date or date-time with simultaneous consideration of \nfrom\n.\n\n\n\n\nExamples\n\n\n{\n  \"process_id\":\"filter_daterange\",\n  \"imagery\":{\n    \"process_id\":\"get_data\",\n    \"data_id\":\"Sentinel2A-L1C\"\n  },\n  \"from\":\"2017-01-01\",\n  \"to\":\"2017-01-31\"\n}\n\n\n\n\nprocess_graph\n\n\nAnother process graph can be referenced with the process \nprocess_graph\n. This could even be an externally hosted process graph.\n\n\nArguments\n\n\n\n\nimagery\n \n(required)\n: Image collection to apply the process graph to\n\n\nuri\n \n(required)\n: An URI to a process graph.\n\n\n\n\nExamples\n\n\n{\n  \"process_id\":\"process_graph\",\n  \"imagery\":{\n    \"process_id\":\"get_data\",\n    \"data_id\":\"Sentinel2A-L1C\"\n  },\n  \"uri\":\"http://otherhost.org/api/v1/users/12345/process_graphs/abcdef\"\n}\n\n\n\n\nto be continued...",
            "title": "Processes"
        },
        {
            "location": "/processes/index.html#processes",
            "text": "A process is defined in the API specification. It is an operation that performs a specific task on a set of parameters and returns a result. It's definition includes a name, a set of parameters, a return type, a set of possible errors or exceptions and some other metadata.    The name is the identifier for the process and MUST never contain a forward slash  / .   Each parameter has a name and the valid content follows a schema.  The content returned by a process also follows a schema.  The schema usually defines the data type and a format according to JSON schema. There are openEO specific formats defined below.",
            "title": "Processes"
        },
        {
            "location": "/processes/index.html#openeo-specific-formats",
            "text": "TBD",
            "title": "openEO specific formats"
        },
        {
            "location": "/processes/index.html#core-processes",
            "text": "There are some processes that we define to be core processes that should be implemented by all back-ends:   get_data  filter_bands  filter_daterange  process_graph  to be continued...   Note:  Currently there are only few defined processes. Those are currently only meant as an example how future documentation of processes may look like.",
            "title": "Core Processes"
        },
        {
            "location": "/processes/index.html#get_data",
            "text": "Filters and selects a single dataset provided by the back-end.",
            "title": "get_data"
        },
        {
            "location": "/processes/index.html#arguments",
            "text": "Any of the properties of a dataset, e.g.   data_id : Filter by data id  extent : Filter by extent  time : Filter by time  bands : Filter by band ids  ...   The back-end provider decides which of the potential datasets is the most relevant one to be selected.",
            "title": "Arguments"
        },
        {
            "location": "/processes/index.html#examples",
            "text": "{\n  \"process_id\": \"get_data\",\n  \"data_id\":\"Sentinel2A-L1C\"\n}  {\n  \"process_id\": \"get_data\",\n  \"platform\": \"landsat-7\",\n  \"sensor\": \"modis\", \n  \"derived_from\": null\n}",
            "title": "Examples"
        },
        {
            "location": "/processes/index.html#filter_bands",
            "text": "Allows to extract one or multiple bands of multi-band raster image collection. Bands can be chosen either by band id, band name or by wavelength.",
            "title": "filter_bands"
        },
        {
            "location": "/processes/index.html#arguments_1",
            "text": "imagery   (required) : Image collection to filter   And one of:   bands : string or array of strings containing band ids.  names : string or array of strings containing band names.  wavelengths : number or two-element array of numbers containing a wavelength or a minimum and maximum wavelength respectively.",
            "title": "Arguments"
        },
        {
            "location": "/processes/index.html#examples_1",
            "text": "{\n  \"process_id\":\"filter_bands\",\n  \"imagery\":{\n    \"process_id\":\"get_data\",\n    \"data_id\":\"Sentinel2A-L1C\"\n  },\n  \"bands\":\"1\"\n}  {\n  \"process_id\":\"filter_bands\",\n  \"imagery\":{\n    \"process_id\":\"get_data\",\n    \"data_id\":\"Sentinel2A-L1C\"\n  },\n  \"wavelengths\":[\n    1300,\n    2000\n  ]\n}",
            "title": "Examples"
        },
        {
            "location": "/processes/index.html#filter_daterange",
            "text": "Allows to filter an image collection by temporal extent.",
            "title": "filter_daterange"
        },
        {
            "location": "/processes/index.html#arguments_2",
            "text": "imagery   (required) : Image collection to filter   And at least one of:   from : Includes all data newer than the specified ISO 8601 date or date-time with simultaneous consideration of  to .  to : Includes all data older than the specified ISO 8601 date or date-time with simultaneous consideration of  from .",
            "title": "Arguments"
        },
        {
            "location": "/processes/index.html#examples_2",
            "text": "{\n  \"process_id\":\"filter_daterange\",\n  \"imagery\":{\n    \"process_id\":\"get_data\",\n    \"data_id\":\"Sentinel2A-L1C\"\n  },\n  \"from\":\"2017-01-01\",\n  \"to\":\"2017-01-31\"\n}",
            "title": "Examples"
        },
        {
            "location": "/processes/index.html#process_graph",
            "text": "Another process graph can be referenced with the process  process_graph . This could even be an externally hosted process graph.",
            "title": "process_graph"
        },
        {
            "location": "/processes/index.html#arguments_3",
            "text": "imagery   (required) : Image collection to apply the process graph to  uri   (required) : An URI to a process graph.",
            "title": "Arguments"
        },
        {
            "location": "/processes/index.html#examples_3",
            "text": "{\n  \"process_id\":\"process_graph\",\n  \"imagery\":{\n    \"process_id\":\"get_data\",\n    \"data_id\":\"Sentinel2A-L1C\"\n  },\n  \"uri\":\"http://otherhost.org/api/v1/users/12345/process_graphs/abcdef\"\n}",
            "title": "Examples"
        },
        {
            "location": "/processes/index.html#to-be-continued",
            "text": "",
            "title": "to be continued..."
        },
        {
            "location": "/processgraphs/index.html",
            "text": "Process graphs\n\n\nA process graph includes specific process calls, i.e. references to one or more processes including specific values for input arguments similar to a function call in programming. However, process graphs can chain multiple processes. In particular, arguments of processes in general can be again (recursive) process graphs, input datasets, or simple scalar or array values.\n\n\nSchematic definition\n\n\nProcess\n\n\nA single process in a process graph is defined as follows:\n\n\n<Process> := {\n  \"process_id\": <string>,\n  \"description\": <string>,\n  \"<ArgumentName>\": <Value>,\n  ...\n}\n\n\n\n\nA process MUST always contain a key-value-pair named \nprocess_id\n and MAY contain a \nprocess_description\n.  It MAY hold an arbitrary number of additional elements as arguments for the process.\n\n\nprocess_id\n can currently contain three types of processes:\n\n\n\n\nBackend-defined processes, which are listed at \nGET /processes\n, e.g. \nfilter_bands\n.\n\n\nUser-defined process graphs, which are listed at \nGET /users/{user_id}/process_graphs\n. \n  They are prefixed with \n/user/\n, e.g. \n/user/my_process_graph\n.\n\n\nUser-defined functions (UDF) are prefixed with \n/udf\n and additionally contain the runtime and the process name separated by \n/\n, e.g. \n/udf/Python/apply_pixel\n.\n\n\n\n\nArguments\n\n\nA process can have an arbitrary number of arguments.\n\n\nThe key \n<ArgumentName>\n can be any valid JSON key, but it is RECOMMENDED to use \nsnake case\n and limit the characters to \na-z\n, \n0-9\n and the \n_\n. \n<ArgumentName>\n MUST NOT use the names \nprocess_id\n or \nprocess_description\n as it would result in a naming conflict.\n\n\nA value is defined as follows:\n\n\n<Value> := <string|number|array|boolean|null|object|Process>\n\n\n\n\nNote:\n The specified data types except \nProcess\n (see definition above) are the native data types supported by JSON. Some limitations apply:\n\n\n\n\nAn array MUST always contain \none data type only\n and is allowed to contain the data types allowed for \n<Value>\n.\n\n\nObjects are not allowed to have a key with the name \nprocess_id\n except for an object of type \nProcess\n.\n\n\n\n\nNote:\n The expected names of arguments are defined by the process descriptions, which can be discovered with calls to \nGET /processes\n and \nGET /udf_runtimes/{lang}/{udf_type}\n. Therefore, the key name for a key-value-pair holding an image collection as value doesn't necessarily need to be named \nimagery\n. The name depends on the name of the corresponding process argument the image collection is assigned to. Example 2 demonstrates this by using \ncollection\n as a key once. \n\n\nExamples\n\n\nExample 1:\n A full process graph definition.\n\n\n{\n  \"process_id\":\"min_time\",\n  \"imagery\":{\n    \"process_id\":\"/udf/Python/custom_ndvi\",\n    \"imagery\":{\n      \"process_id\":\"filter_daterange\",\n      \"imagery\":{\n        \"process_id\":\"filter_bbox\",\n        \"imagery\":{\n          \"process_id\":\"get_data\",\n          \"data_id\":\"S2_L2A_T32TPS_20M\"\n        },\n        \"left\":652000,\n        \"right\":672000,\n        \"top\":5161000,\n        \"bottom\":5181000,\n        \"srs\":\"EPSG:32632\"\n      },\n      \"from\":\"2017-01-01\",\n      \"to\":\"2017-01-31\"\n    },\n    \"red\":\"B04\",\n    \"nir\":\"B8A\"\n  }\n}\n\n\n\n\nExample 2:\n If a process needs multiple processes as input, it is allowed to use arrays of the respective types.\n\n\n{\n  \"imagery\":{\n    \"process_id\":\"union\",\n    \"collection\":[\n      {\n        \"process_id\":\"filter_bands\",\n        \"imagery\":{\n          \"process_id\":\"get_data\",\n          \"data_id\":\"Sentinel2-L1C\"\n        },\n        \"bands\":8\n      },\n      {\n        \"process_id\":\"filter_bands\",\n        \"imagery\":{\n          \"process_id\":\"get_data\",\n          \"data_id\":\"Sentinel2-L1C\"\n        },\n        \"bands\":5\n      }\n    ]\n  }\n}",
            "title": "Process Graphs"
        },
        {
            "location": "/processgraphs/index.html#process-graphs",
            "text": "A process graph includes specific process calls, i.e. references to one or more processes including specific values for input arguments similar to a function call in programming. However, process graphs can chain multiple processes. In particular, arguments of processes in general can be again (recursive) process graphs, input datasets, or simple scalar or array values.",
            "title": "Process graphs"
        },
        {
            "location": "/processgraphs/index.html#schematic-definition",
            "text": "",
            "title": "Schematic definition"
        },
        {
            "location": "/processgraphs/index.html#process",
            "text": "A single process in a process graph is defined as follows:  <Process> := {\n  \"process_id\": <string>,\n  \"description\": <string>,\n  \"<ArgumentName>\": <Value>,\n  ...\n}  A process MUST always contain a key-value-pair named  process_id  and MAY contain a  process_description .  It MAY hold an arbitrary number of additional elements as arguments for the process.  process_id  can currently contain three types of processes:   Backend-defined processes, which are listed at  GET /processes , e.g.  filter_bands .  User-defined process graphs, which are listed at  GET /users/{user_id}/process_graphs . \n  They are prefixed with  /user/ , e.g.  /user/my_process_graph .  User-defined functions (UDF) are prefixed with  /udf  and additionally contain the runtime and the process name separated by  / , e.g.  /udf/Python/apply_pixel .   Arguments  A process can have an arbitrary number of arguments.  The key  <ArgumentName>  can be any valid JSON key, but it is RECOMMENDED to use  snake case  and limit the characters to  a-z ,  0-9  and the  _ .  <ArgumentName>  MUST NOT use the names  process_id  or  process_description  as it would result in a naming conflict.  A value is defined as follows:  <Value> := <string|number|array|boolean|null|object|Process>  Note:  The specified data types except  Process  (see definition above) are the native data types supported by JSON. Some limitations apply:   An array MUST always contain  one data type only  and is allowed to contain the data types allowed for  <Value> .  Objects are not allowed to have a key with the name  process_id  except for an object of type  Process .   Note:  The expected names of arguments are defined by the process descriptions, which can be discovered with calls to  GET /processes  and  GET /udf_runtimes/{lang}/{udf_type} . Therefore, the key name for a key-value-pair holding an image collection as value doesn't necessarily need to be named  imagery . The name depends on the name of the corresponding process argument the image collection is assigned to. Example 2 demonstrates this by using  collection  as a key once.",
            "title": "Process"
        },
        {
            "location": "/processgraphs/index.html#examples",
            "text": "Example 1:  A full process graph definition.  {\n  \"process_id\":\"min_time\",\n  \"imagery\":{\n    \"process_id\":\"/udf/Python/custom_ndvi\",\n    \"imagery\":{\n      \"process_id\":\"filter_daterange\",\n      \"imagery\":{\n        \"process_id\":\"filter_bbox\",\n        \"imagery\":{\n          \"process_id\":\"get_data\",\n          \"data_id\":\"S2_L2A_T32TPS_20M\"\n        },\n        \"left\":652000,\n        \"right\":672000,\n        \"top\":5161000,\n        \"bottom\":5181000,\n        \"srs\":\"EPSG:32632\"\n      },\n      \"from\":\"2017-01-01\",\n      \"to\":\"2017-01-31\"\n    },\n    \"red\":\"B04\",\n    \"nir\":\"B8A\"\n  }\n}  Example 2:  If a process needs multiple processes as input, it is allowed to use arrays of the respective types.  {\n  \"imagery\":{\n    \"process_id\":\"union\",\n    \"collection\":[\n      {\n        \"process_id\":\"filter_bands\",\n        \"imagery\":{\n          \"process_id\":\"get_data\",\n          \"data_id\":\"Sentinel2-L1C\"\n        },\n        \"bands\":8\n      },\n      {\n        \"process_id\":\"filter_bands\",\n        \"imagery\":{\n          \"process_id\":\"get_data\",\n          \"data_id\":\"Sentinel2-L1C\"\n        },\n        \"bands\":5\n      }\n    ]\n  }\n}",
            "title": "Examples"
        },
        {
            "location": "/apiguidelines/index.html",
            "text": "General API Guidelines\n\n\nTo provide the smoothest possible experience, it's important to have these APIs follow consistent design guidelines, thus making using them easy and intuitive.\n\n\nLanguage\n\n\nLanguage\n\n\nGenerally, English language MUST be used for all names, documentation etc.\n\n\nIn the specification the key words \u201cMUST\u201d, \u201cMUST NOT\u201d, \u201cREQUIRED\u201d, \u201cSHALL\u201d, \u201cSHALL NOT\u201d, \u201cSHOULD\u201d, \u201cSHOULD NOT\u201d, \u201cRECOMMENDED\u201d, \u201cMAY\u201d, and \u201cOPTIONAL\u201d in this document are to be interpreted as described in \nRFC 2119\n.\n\n\nCasing\n\n\nAll names SHOULD be written in snake case, i.e. words are separated with one\u00a0underscore\u00a0character (_) and no spaces, with all letters lowercased. Example: \nhello_world\n. This applies particularly to endpoints and JSON property names. HTTP header fields follow their respective casing conventions, e.g. \nContent-Type\n or \nOpenEO-Costs\n, despite being case-insensitive according to \nRFC 7230\n.\n\n\nTechnical requirements\n\n\nHTTP\n\n\nThe API developed by the openEO project uses \nHTTP REST Level 2\n for communication between client and back-end server.\n\n\nPublic APIs MUST be available via HTTPS only and all inbound calls MUST be HTTPS. \n\n\nVerbs\n\n\nEndpoints SHOULD use meaningful HTTP verbs (e.g. GET, POST, PUT, PATCH, DELETE).\n\n\nIf there is a need to transfer big chunks of data via GET requests, POST requests MAY be used as a replacement as they support to send data via request body.\n\n\nUnless otherwise stated, PATCH requests are only defined to work on the direct (first-level) children of the full JSON object. Therefore, changing a property on a deeper level of the full JSON object always requires to send the whole JSON object defined by the first-level property.\n\n\nResource naming\n\n\nNaming of endpoints SHOULD follow the REST principles. Therefore, endpoints SHOULD be centered around resources. Resource identifiers MUST be named with a noun in plural form except for single actions that can not be modelled with the regular HTTP verbs. Single actions MUST be single endpoint with a single HTTP verb (POST is RECOMMENDED) and no other endpoints beneath it.\n\n\nCross-Origin Resource Sharing (CORS)\n\n\nAll back-end providers SHOULD support CORS. More information can be found in the \ncorresponding section\n.\n\n\nStatus codes and error handling\n\n\nThe success of requests MUST be indicated using \nHTTP status codes\n according to \nRFC 7231\n. More information can be found in the section about \nstatus und error handling\n.\n\n\nRequests and response formats\n\n\nJSON\n\n\nWeb-based communication, especially when a mobile or other low-bandwidth client is involved, has moved quickly in the direction of JSON for a variety of reasons, including its tendency to be lighter weight and its ease of consumption with JavaScript-based clients. Therefore, services SHOULD use JSON as the default encoding. Other response formats can be requested using \nContent Negotiation\n.\n\n\nClients and servers MUST NOT rely on the order in which properties appears in JSON responses. When supported by the service, clients MAY request that array elements be returned in a specific order.\n\n\nCollections SHOULD NOT include nested JSON objects if those information can be requested from the individual resources.\n\n\nTemporal data\n\n\nDate, time, intervals and durations MUST be formatted according to ISO 8601 if there is an appropriate encoding available in the standard.",
            "title": "Specification Guidelines"
        },
        {
            "location": "/apiguidelines/index.html#general-api-guidelines",
            "text": "To provide the smoothest possible experience, it's important to have these APIs follow consistent design guidelines, thus making using them easy and intuitive.",
            "title": "General API Guidelines"
        },
        {
            "location": "/apiguidelines/index.html#language",
            "text": "",
            "title": "Language"
        },
        {
            "location": "/apiguidelines/index.html#language_1",
            "text": "Generally, English language MUST be used for all names, documentation etc.  In the specification the key words \u201cMUST\u201d, \u201cMUST NOT\u201d, \u201cREQUIRED\u201d, \u201cSHALL\u201d, \u201cSHALL NOT\u201d, \u201cSHOULD\u201d, \u201cSHOULD NOT\u201d, \u201cRECOMMENDED\u201d, \u201cMAY\u201d, and \u201cOPTIONAL\u201d in this document are to be interpreted as described in  RFC 2119 .",
            "title": "Language"
        },
        {
            "location": "/apiguidelines/index.html#casing",
            "text": "All names SHOULD be written in snake case, i.e. words are separated with one\u00a0underscore\u00a0character (_) and no spaces, with all letters lowercased. Example:  hello_world . This applies particularly to endpoints and JSON property names. HTTP header fields follow their respective casing conventions, e.g.  Content-Type  or  OpenEO-Costs , despite being case-insensitive according to  RFC 7230 .",
            "title": "Casing"
        },
        {
            "location": "/apiguidelines/index.html#technical-requirements",
            "text": "",
            "title": "Technical requirements"
        },
        {
            "location": "/apiguidelines/index.html#http",
            "text": "The API developed by the openEO project uses  HTTP REST Level 2  for communication between client and back-end server.  Public APIs MUST be available via HTTPS only and all inbound calls MUST be HTTPS.",
            "title": "HTTP"
        },
        {
            "location": "/apiguidelines/index.html#verbs",
            "text": "Endpoints SHOULD use meaningful HTTP verbs (e.g. GET, POST, PUT, PATCH, DELETE).  If there is a need to transfer big chunks of data via GET requests, POST requests MAY be used as a replacement as they support to send data via request body.  Unless otherwise stated, PATCH requests are only defined to work on the direct (first-level) children of the full JSON object. Therefore, changing a property on a deeper level of the full JSON object always requires to send the whole JSON object defined by the first-level property.",
            "title": "Verbs"
        },
        {
            "location": "/apiguidelines/index.html#resource-naming",
            "text": "Naming of endpoints SHOULD follow the REST principles. Therefore, endpoints SHOULD be centered around resources. Resource identifiers MUST be named with a noun in plural form except for single actions that can not be modelled with the regular HTTP verbs. Single actions MUST be single endpoint with a single HTTP verb (POST is RECOMMENDED) and no other endpoints beneath it.",
            "title": "Resource naming"
        },
        {
            "location": "/apiguidelines/index.html#cross-origin-resource-sharing-cors",
            "text": "All back-end providers SHOULD support CORS. More information can be found in the  corresponding section .",
            "title": "Cross-Origin Resource Sharing (CORS)"
        },
        {
            "location": "/apiguidelines/index.html#status-codes-and-error-handling",
            "text": "The success of requests MUST be indicated using  HTTP status codes  according to  RFC 7231 . More information can be found in the section about  status und error handling .",
            "title": "Status codes and error handling"
        },
        {
            "location": "/apiguidelines/index.html#requests-and-response-formats",
            "text": "",
            "title": "Requests and response formats"
        },
        {
            "location": "/apiguidelines/index.html#json",
            "text": "Web-based communication, especially when a mobile or other low-bandwidth client is involved, has moved quickly in the direction of JSON for a variety of reasons, including its tendency to be lighter weight and its ease of consumption with JavaScript-based clients. Therefore, services SHOULD use JSON as the default encoding. Other response formats can be requested using  Content Negotiation .  Clients and servers MUST NOT rely on the order in which properties appears in JSON responses. When supported by the service, clients MAY request that array elements be returned in a specific order.  Collections SHOULD NOT include nested JSON objects if those information can be requested from the individual resources.",
            "title": "JSON"
        },
        {
            "location": "/apiguidelines/index.html#temporal-data",
            "text": "Date, time, intervals and durations MUST be formatted according to ISO 8601 if there is an appropriate encoding available in the standard.",
            "title": "Temporal data"
        },
        {
            "location": "/errors/index.html",
            "text": "Status and error handling\n\n\nThe success of requests MUST be indicated using \nHTTP status codes\n according to \nRFC 7231\n.\n\n\nIf the API responds with a status code between 100 and 399 the back-end indicates that the request has been handled successfully.\n\n\nIn general an error is communicated with a status code between 400 and 599. Client errors are defined as a client passing invalid data to the service and the service\u00a0\ncorrectly\n rejecting that data. Examples include invalid credentials, incorrect parameters, unknown versions, or similar. These are generally \"4xx\" HTTP error codes and are the result of a client passing incorrect or invalid data. Client errors do\u00a0\nnot\n\u00a0contribute to overall API availability. \n\n\nServer errors are defined as the server failing to correctly return in response to a valid client request. These are generally \"5xx\" HTTP error codes. Server errors \ndo\n contribute to the overall API availability. Calls that fail due to rate limiting or quota failures MUST NOT count as server errors. \n\n\nJSON error object\n\n\nA JSON error object SHOULD be sent with all responses that have a status code between 400 and 599.\n\n\n{\n  \"id\": \"936DA01F-9ABD-4D9D-80C7-02AF85C822A8\",\n  \"code\": 123,\n  \"message\": \"A sample error message.\",\n  \"url\": \"http://www.openeo.org/docs/errors/123\"\n}\n\n\n\n\nSending \ncode\n and \nmessage\n is REQUIRED. \n\n\n\n\n\n\nA back-end MAY add a free-form \nid\n (unique identifier) to the error response to be able to log and track errors with further non-disclosable details.\n\n\n\n\n\n\nThe \ncode\n is either one of the standardized openEO error codes below or a proprietary error code with a number greater than 10000.\n\n\n\n\n\n\nThe \nmessage\n explains the reason the server is rejecting the request. For \"4xx\" error codes the message explains how the client needs to modify the request.\n\n\n\n\n\n\nBy default the message MUST be sent in English language. Content Negotiation is used to localize the error messages: If an \nAcceppt-Language\n header is sent by the client and a translation is available, the message should be translated accordingly and the \nContent-Language\n header must be present in the response. See \"\nHow to localize your API\n\" for more information.\n\n\n\n\nurl\n is an OPTIONAL attribute and contains a link to a resource that is explaining the error and potential solutions in-depth.\n\n\n\n\nStandardized status codes\n\n\nThe openEO API usually uses the following HTTP status codes for successful requests: \n\n\n\n\n200 OK\n:\n  Indicates a successful request \nwith\n a response body being sent.\n\n\n201 Created\n\n  Indicates a successful request that successfully created a new resource. Sends a \nLocation\n header to the newly created resource \nwithout\n a response body.\n\n\n202 Accepted\n\n  Indicates a successful request that successfully queued the creation of a new resource, but it has not been created yet. The response is sent \nwithout\n a response body.\n\n\n204 No Content\n:\n  Indicates a successful request \nwithout\n a response body being sent.\n\n\n\n\nThe openEO API often uses the following HTTP status codes for failed requests: \n\n\n\n\n\n\n400 Bad request\n:\n  The back-end responds with this error code whenever the error has its origin on client side and no other HTTP status code in the 400 range is suitable.\n\n\n\n\n\n\n401 Unauthorized\n:\n  The client \ndid not\n provide any authorization details (usually using the Authorization header), but authorization is required for this request to be processed.\n\n\n\n\n\n\n403 Forbidden\n:\n  The client \ndid\n provide authorization details (usually using the Authorization header), but the provided credentials or the authorization token is invalid or has expired.\n\n\n\n\n\n\n404 Not Found\n:\n  The resource specified by the path does not exist, i.e. one of the the resources belonging to the specified identifiers are not available at the back-end.\n  \nNote:\n Unsupported endpoints MUST use HTTP status code 501.\n\n\n\n\n\n\n500 Internal Server Error\n:\n  The error has its origin on server side and no other status code in the 500 range is suitable.\n\n\n\n\n\n\n501 Not implemented\n:\n  An endpoint is specified in the openEO API, but is not supported.\n\n\n\n\n\n\nIf a HTTP status code in the 400 range is returned, the client SHOULD NOT repeat the request without modifications. For HTTP status code in the 500 range, the client MAY repeat the same request later.\n\n\nAll HTTP status codes defined in RFC 7231 in the 400 and 500 ranges can be used as openEO error code in addition to the most used status codes mentioned here. Responding with openEO error codes 400 and 500 SHOULD be avoided in favor of any more specific standardized or proprietary openEO error code.\n\n\nGeneral error codes (xxx)\n\n\n\n\n\n\n\n\nopenEO Error Code\n\n\nDescription\n\n\nMessage\n\n\nHTTP Status Code\n\n\n\n\n\n\n\n\n\n\n404\n\n\nTo be used if the value of a path parameter is invalid, i.e. the requested resource is not available. \nNote:\n Unsupported endpoints MUST use code 501.\n\n\nNot Found.\n\n\n404\n\n\n\n\n\n\n501\n\n\nThe back-end responds with this error code whenever an endpoint is specified in the openEO API, but is not supported.\n\n\nNot implemented.\n\n\n501\n\n\n\n\n\n\n503\n\n\n\n\nService unavailable.\n\n\n503\n\n\n\n\n\n\n601\n\n\n\n\nParameter \nX\n is invalid.\n\n\n400\n\n\n\n\n\n\n611\n\n\nInvalid or unsupported CRS specified.\n\n\nInvalid CRS specified.\n\n\n400\n\n\n\n\n\n\n612\n\n\n\n\nCoordinate is out of bounds.\n\n\n400\n\n\n\n\n\n\n\n\nCapabilities (11xx)\n\n\nNone yet.\n\n\nData and process discovery (12xx)\n\n\nNone yet.\n\n\nUDFs (13xx)\n\n\n\n\n\n\n\n\nopenEO Error Code\n\n\nDescription\n\n\nMessage\n\n\nHTTP Status Code\n\n\n\n\n\n\n\n\n\n\n1301\n\n\n\n\nUDF programming language not supported.\n\n\n400 / 404\n\n\n\n\n\n\n1302\n\n\n\n\nUDF type not supported.\n\n\n400 / 404\n\n\n\n\n\n\n\n\nFile Handling (14xx)\n\n\n\n\n\n\n\n\nopenEO Error Code\n\n\nDescription\n\n\nMessage\n\n\nHTTP Status Code\n\n\n\n\n\n\n\n\n\n\n1401\n\n\nServer couldn't store file due to server-side reasons.\n\n\nUnable to store file.\n\n\n500\n\n\n\n\n\n\n1402\n\n\nThe storage quota has been exceeded by the user.\n\n\nInsufficient Storage.\n\n\n400\n\n\n\n\n\n\n1410\n\n\nFile format, file extension or mime type is not allowed.\n\n\nFile type not allowed.\n\n\n400\n\n\n\n\n\n\n1411\n\n\nFile exceeds allowed maximum file size.\n\n\nFile size it too large.\n\n\n400\n\n\n\n\n\n\n1412\n\n\nThe content of the file is invalid.\n\n\nFile content is invalid.\n\n\n400\n\n\n\n\n\n\n1413\n\n\nThe file is locked by a running job or another process.\n\n\nFile is locked.\n\n\n400\n\n\n\n\n\n\n\n\nProcess graphs (2xxx)\n\n\n\n\n\n\n\n\nopenEO Error Code\n\n\nDescription\n\n\nMessage\n\n\nHTTP Status Code\n\n\n\n\n\n\n\n\n\n\n2001\n\n\n\n\nNo process graph specified.\n\n\n400\n\n\n\n\n\n\n2002\n\n\n\n\nProcess graph structure is invalid.\n\n\n400\n\n\n\n\n\n\n2003\n\n\n\n\nThe array \nX\n contains values of multiple types.\n\n\n400\n\n\n\n\n\n\n2101\n\n\n\n\nProcess \nX\n is not supported.\n\n\n400\n\n\n\n\n\n\n2102\n\n\n\n\nProcess argument \nX\n is not supported.\n\n\n400\n\n\n\n\n\n\n2103\n\n\n\n\nInvalid value \nY\n for the process argument \nX\n specified.\n\n\n400\n\n\n\n\n\n\n2104\n\n\n\n\nRequired process argument \nX\n is missing.\n\n\n400\n\n\n\n\n\n\n\n\nJobs (3xxx)\n\n\n\n\n\n\n\n\nopenEO Error Code\n\n\nDescription\n\n\nMessage\n\n\nHTTP Status Code\n\n\n\n\n\n\n\n\n\n\n408\n\n\nThe (synchronous) request timed out.\n\n\nRequest timed out.\n\n\n408\n\n\n\n\n\n\n3001\n\n\n\n\nOutput format not supported.\n\n\n400\n\n\n\n\n\n\n3002\n\n\n\n\nOutput format argument \nX\n is not supported.\n\n\n400\n\n\n\n\n\n\n3003\n\n\n\n\nInvalid value \nY\n for the output format argument \nX\n specified.\n\n\n400\n\n\n\n\n\n\n3004\n\n\n\n\nData can't be transformed into the requested output format.\n\n\n400\n\n\n\n\n\n\n3005\n\n\nThe job is currently locked due to an enabled service or a running batch computation and can't be modified meanwhile.\n\n\nJob is locked.\n\n\n400\n\n\n\n\n\n\n3006\n\n\nThe job has not finished computing the results yet. Try again later.\n\n\nResults are not finished yet.\n\n\n400\n\n\n\n\n\n\n\n\nAuthorization, user content and billing (401-403, 4xxx)\n\n\n\n\n\n\n\n\nopenEO Error Code\n\n\nDescription\n\n\nMessage\n\n\nHTTP Status Code\n\n\n\n\n\n\n\n\n\n\n401\n\n\nThe back-end responds with this error code whenever the HTTP status code 401 is appropriate (see above) and no other openEO error code in the 4000 range is suitable.\n\n\nUnauthorized.\n\n\n401\n\n\n\n\n\n\n402\n\n\nThe budget required to fulfil the request are insufficient.\n\n\nPayment required.\n\n\n402\n\n\n\n\n\n\n403\n\n\nThe back-end responds with this error code whenever the HTTP status code 403 is appropriate (see above) and no other openEO error code in the 4000 range is suitable.\n\n\nForbidden.\n\n\n403\n\n\n\n\n\n\n4001\n\n\nThe specified password is not considered secure by the policy of the back-end provider or no password was given at all. The user needs to specify a different password to proceed.\n\n\nPassword does not meet the requirements.\n\n\n400\n\n\n\n\n\n\n4031\n\n\nInvalid authentication scheme (e.g. Bearer).\n\n\n\n\n403\n\n\n\n\n\n\n4032\n\n\nAuthorization token invalid or expired.\n\n\n\n\n403\n\n\n\n\n\n\n4033\n\n\n\n\nCredentials are not correct.\n\n\n403\n\n\n\n\n\n\n\n\nWeb services (5xxx)\n\n\n\n\n\n\n\n\nopenEO Error Code\n\n\nDescription\n\n\nMessage\n\n\nHTTP Status Code\n\n\n\n\n\n\n\n\n\n\n5001\n\n\n\n\nService type is not supported.\n\n\n400\n\n\n\n\n\n\n5101\n\n\nInvalid job id specified.\n\n\nJob does not exist.\n\n\n400\n\n\n\n\n\n\n5102\n\n\n\n\nService argument \nX\n is not supported.\n\n\n400\n\n\n\n\n\n\n5103\n\n\n\n\nInvalid value \nY\n for the service argument \nX\n specified.\n\n\n400\n\n\n\n\n\n\n5104\n\n\n\n\nRequired service argument \nX\n is missing.\n\n\n400",
            "title": "Error handling"
        },
        {
            "location": "/errors/index.html#status-and-error-handling",
            "text": "The success of requests MUST be indicated using  HTTP status codes  according to  RFC 7231 .  If the API responds with a status code between 100 and 399 the back-end indicates that the request has been handled successfully.  In general an error is communicated with a status code between 400 and 599. Client errors are defined as a client passing invalid data to the service and the service\u00a0 correctly  rejecting that data. Examples include invalid credentials, incorrect parameters, unknown versions, or similar. These are generally \"4xx\" HTTP error codes and are the result of a client passing incorrect or invalid data. Client errors do\u00a0 not \u00a0contribute to overall API availability.   Server errors are defined as the server failing to correctly return in response to a valid client request. These are generally \"5xx\" HTTP error codes. Server errors  do  contribute to the overall API availability. Calls that fail due to rate limiting or quota failures MUST NOT count as server errors.",
            "title": "Status and error handling"
        },
        {
            "location": "/errors/index.html#json-error-object",
            "text": "A JSON error object SHOULD be sent with all responses that have a status code between 400 and 599.  {\n  \"id\": \"936DA01F-9ABD-4D9D-80C7-02AF85C822A8\",\n  \"code\": 123,\n  \"message\": \"A sample error message.\",\n  \"url\": \"http://www.openeo.org/docs/errors/123\"\n}  Sending  code  and  message  is REQUIRED.     A back-end MAY add a free-form  id  (unique identifier) to the error response to be able to log and track errors with further non-disclosable details.    The  code  is either one of the standardized openEO error codes below or a proprietary error code with a number greater than 10000.    The  message  explains the reason the server is rejecting the request. For \"4xx\" error codes the message explains how the client needs to modify the request.    By default the message MUST be sent in English language. Content Negotiation is used to localize the error messages: If an  Acceppt-Language  header is sent by the client and a translation is available, the message should be translated accordingly and the  Content-Language  header must be present in the response. See \" How to localize your API \" for more information.   url  is an OPTIONAL attribute and contains a link to a resource that is explaining the error and potential solutions in-depth.",
            "title": "JSON error object"
        },
        {
            "location": "/errors/index.html#standardized-status-codes",
            "text": "The openEO API usually uses the following HTTP status codes for successful requests:    200 OK :\n  Indicates a successful request  with  a response body being sent.  201 Created \n  Indicates a successful request that successfully created a new resource. Sends a  Location  header to the newly created resource  without  a response body.  202 Accepted \n  Indicates a successful request that successfully queued the creation of a new resource, but it has not been created yet. The response is sent  without  a response body.  204 No Content :\n  Indicates a successful request  without  a response body being sent.   The openEO API often uses the following HTTP status codes for failed requests:     400 Bad request :\n  The back-end responds with this error code whenever the error has its origin on client side and no other HTTP status code in the 400 range is suitable.    401 Unauthorized :\n  The client  did not  provide any authorization details (usually using the Authorization header), but authorization is required for this request to be processed.    403 Forbidden :\n  The client  did  provide authorization details (usually using the Authorization header), but the provided credentials or the authorization token is invalid or has expired.    404 Not Found :\n  The resource specified by the path does not exist, i.e. one of the the resources belonging to the specified identifiers are not available at the back-end.\n   Note:  Unsupported endpoints MUST use HTTP status code 501.    500 Internal Server Error :\n  The error has its origin on server side and no other status code in the 500 range is suitable.    501 Not implemented :\n  An endpoint is specified in the openEO API, but is not supported.    If a HTTP status code in the 400 range is returned, the client SHOULD NOT repeat the request without modifications. For HTTP status code in the 500 range, the client MAY repeat the same request later.  All HTTP status codes defined in RFC 7231 in the 400 and 500 ranges can be used as openEO error code in addition to the most used status codes mentioned here. Responding with openEO error codes 400 and 500 SHOULD be avoided in favor of any more specific standardized or proprietary openEO error code.",
            "title": "Standardized status codes"
        },
        {
            "location": "/errors/index.html#general-error-codes-xxx",
            "text": "openEO Error Code  Description  Message  HTTP Status Code      404  To be used if the value of a path parameter is invalid, i.e. the requested resource is not available.  Note:  Unsupported endpoints MUST use code 501.  Not Found.  404    501  The back-end responds with this error code whenever an endpoint is specified in the openEO API, but is not supported.  Not implemented.  501    503   Service unavailable.  503    601   Parameter  X  is invalid.  400    611  Invalid or unsupported CRS specified.  Invalid CRS specified.  400    612   Coordinate is out of bounds.  400",
            "title": "General error codes (xxx)"
        },
        {
            "location": "/errors/index.html#capabilities-11xx",
            "text": "None yet.",
            "title": "Capabilities (11xx)"
        },
        {
            "location": "/errors/index.html#data-and-process-discovery-12xx",
            "text": "None yet.",
            "title": "Data and process discovery (12xx)"
        },
        {
            "location": "/errors/index.html#udfs-13xx",
            "text": "openEO Error Code  Description  Message  HTTP Status Code      1301   UDF programming language not supported.  400 / 404    1302   UDF type not supported.  400 / 404",
            "title": "UDFs (13xx)"
        },
        {
            "location": "/errors/index.html#file-handling-14xx",
            "text": "openEO Error Code  Description  Message  HTTP Status Code      1401  Server couldn't store file due to server-side reasons.  Unable to store file.  500    1402  The storage quota has been exceeded by the user.  Insufficient Storage.  400    1410  File format, file extension or mime type is not allowed.  File type not allowed.  400    1411  File exceeds allowed maximum file size.  File size it too large.  400    1412  The content of the file is invalid.  File content is invalid.  400    1413  The file is locked by a running job or another process.  File is locked.  400",
            "title": "File Handling (14xx)"
        },
        {
            "location": "/errors/index.html#process-graphs-2xxx",
            "text": "openEO Error Code  Description  Message  HTTP Status Code      2001   No process graph specified.  400    2002   Process graph structure is invalid.  400    2003   The array  X  contains values of multiple types.  400    2101   Process  X  is not supported.  400    2102   Process argument  X  is not supported.  400    2103   Invalid value  Y  for the process argument  X  specified.  400    2104   Required process argument  X  is missing.  400",
            "title": "Process graphs (2xxx)"
        },
        {
            "location": "/errors/index.html#jobs-3xxx",
            "text": "openEO Error Code  Description  Message  HTTP Status Code      408  The (synchronous) request timed out.  Request timed out.  408    3001   Output format not supported.  400    3002   Output format argument  X  is not supported.  400    3003   Invalid value  Y  for the output format argument  X  specified.  400    3004   Data can't be transformed into the requested output format.  400    3005  The job is currently locked due to an enabled service or a running batch computation and can't be modified meanwhile.  Job is locked.  400    3006  The job has not finished computing the results yet. Try again later.  Results are not finished yet.  400",
            "title": "Jobs (3xxx)"
        },
        {
            "location": "/errors/index.html#authorization-user-content-and-billing-401-403-4xxx",
            "text": "openEO Error Code  Description  Message  HTTP Status Code      401  The back-end responds with this error code whenever the HTTP status code 401 is appropriate (see above) and no other openEO error code in the 4000 range is suitable.  Unauthorized.  401    402  The budget required to fulfil the request are insufficient.  Payment required.  402    403  The back-end responds with this error code whenever the HTTP status code 403 is appropriate (see above) and no other openEO error code in the 4000 range is suitable.  Forbidden.  403    4001  The specified password is not considered secure by the policy of the back-end provider or no password was given at all. The user needs to specify a different password to proceed.  Password does not meet the requirements.  400    4031  Invalid authentication scheme (e.g. Bearer).   403    4032  Authorization token invalid or expired.   403    4033   Credentials are not correct.  403",
            "title": "Authorization, user content and billing (401-403, 4xxx)"
        },
        {
            "location": "/errors/index.html#web-services-5xxx",
            "text": "openEO Error Code  Description  Message  HTTP Status Code      5001   Service type is not supported.  400    5101  Invalid job id specified.  Job does not exist.  400    5102   Service argument  X  is not supported.  400    5103   Invalid value  Y  for the service argument  X  specified.  400    5104   Required service argument  X  is missing.  400",
            "title": "Web services (5xxx)"
        },
        {
            "location": "/cors/index.html",
            "text": "Cross-Origin Resource Sharing (CORS)\n\n\n\n\nCross-origin resource sharing (CORS) is a mechanism that allows restricted resources [...] on a web page to be requested from another domain outside the domain from which the first resource was served. [...]\nCORS defines a way in which a browser and server can interact to determine whether or not it is safe to allow the cross-origin request. It allows for more freedom and functionality than purely same-origin requests, but is more secure than simply allowing all cross-origin requests.\n\n\n\n\nSource: \nhttps://en.wikipedia.org/wiki/Cross-origin_resource_sharing\n\n\nopenEO-based back-ends are usually hosted on a different domain / host than the client that is requesting data from the back-end. Therefore most requests to the back-end are blocked by all modern browsers. This leads to the problem that the JavaScript library (and the Web Editor) can't access any back-end. Therefore, all back-end providers SHOULD support CORS. Without supporting CORS users can't access the back-end with browser-based clients, i.e. the \nJavaScript client\n. \nCORS is a recommendation of the W3C organization.\n The following chapters will explain how back-end providers can implement CORS support.\n\n\n1. Supporting the OPTIONS method\n\n\nAll endpoints must respond to the \nOPTIONS\n HTTP method. This is a response for the preflight requests made by the browsers. It needs to respond with a status code of \n204\n and send the HTTP headers shown in the table below. No body needs to be provided.\n\n\n\n\n\n\n\n\nName\n\n\nDescription\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nAccess-Control-Allow-Origin\n\n\nAllowed origin for the request, including protocol, host and port. It is RECOMMENDED to return the value of the request's origin header. If no \nOrigin\n is sent to the back-end CORS headers SCHOULD NOT be sent at all.\n\n\nhttp://client.isp.com:80\n\n\n\n\n\n\nAccess-Control-Allow-Credentials\n\n\nIf authorization is implemented by the back-end the value MUST be \ntrue\n.\n\n\ntrue\n\n\n\n\n\n\nAccess-Control-Allow-Headers\n\n\nComma-separated list of HTTP headers allowed to be send. MUST contain at least \nAuthorization\n if authorization is implemented by the back-end.\n\n\nAuthorization, Content-Type\n\n\n\n\n\n\nAccess-Control-Allow-Methods\n\n\nComma-separated list of HTTP methods allowed to be requested. Back-ends MUST list all implemented HTTP methods for the endpoint here.\n\n\nOPTIONS, GET, POST, PATCH, PUT, DELETE\n\n\n\n\n\n\nContent-Type\n\n\nSHOULD return the content type delivered by the request that the permission is requested for.\n\n\napplication/json\n\n\n\n\n\n\n\n\nExample request and response\n\n\nRequest:\n\n\nOPTIONS /api/v1/jobs HTTP/1.1\nHost: openeo.cloudprovider.com\nOrigin: http://client.org:8080\nAccess-Control-Request-Method: POST \nAccess-Control-Request-Headers: Authorization, Content-Type\n\n\n\n\nResponse:\n\n\nHTTP/1.1 204 No Content\nAccess-Control-Allow-Origin: http://client.org:8080\nAccess-Control-Allow-Credentials: true\nAccess-Control-Allow-Methods: OPTIONS, GET, POST, PATCH, PUT, DELETE\nAccess-Control-Allow-Headers: Authorization, Content-Type\nContent-Type: application/json\n\n\n\n\n2. Sending CORS headers for every endpoint\n\n\nThe following headers MUST be included with every response:\n\n\n\n\n\n\n\n\nName\n\n\nDescription\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nAccess-Control-Allow-Origin\n\n\nAllowed origin for the request, including protocol, host and port. It is RECOMMENDED to return the value of the request's origin header. If no \nOrigin\n is sent to the back-end CORS headers SHOULD NOT be sent at all.\n\n\nhttp://client.isp.com:80\n\n\n\n\n\n\nAccess-Control-Allow-Credentials\n\n\nIf authorization is implemented by the back-end the value MUST be \ntrue\n.\n\n\ntrue\n\n\n\n\n\n\n\n\nRemarks\n\n\nMost server can send the required headers and the responses to the OPTIONS requests globally. Otherwise you may want to use a proxy server to add the headers and OPTIONS responses.",
            "title": "CORS"
        },
        {
            "location": "/cors/index.html#cross-origin-resource-sharing-cors",
            "text": "Cross-origin resource sharing (CORS) is a mechanism that allows restricted resources [...] on a web page to be requested from another domain outside the domain from which the first resource was served. [...]\nCORS defines a way in which a browser and server can interact to determine whether or not it is safe to allow the cross-origin request. It allows for more freedom and functionality than purely same-origin requests, but is more secure than simply allowing all cross-origin requests.   Source:  https://en.wikipedia.org/wiki/Cross-origin_resource_sharing  openEO-based back-ends are usually hosted on a different domain / host than the client that is requesting data from the back-end. Therefore most requests to the back-end are blocked by all modern browsers. This leads to the problem that the JavaScript library (and the Web Editor) can't access any back-end. Therefore, all back-end providers SHOULD support CORS. Without supporting CORS users can't access the back-end with browser-based clients, i.e. the  JavaScript client .  CORS is a recommendation of the W3C organization.  The following chapters will explain how back-end providers can implement CORS support.",
            "title": "Cross-Origin Resource Sharing (CORS)"
        },
        {
            "location": "/cors/index.html#1-supporting-the-options-method",
            "text": "All endpoints must respond to the  OPTIONS  HTTP method. This is a response for the preflight requests made by the browsers. It needs to respond with a status code of  204  and send the HTTP headers shown in the table below. No body needs to be provided.     Name  Description  Example      Access-Control-Allow-Origin  Allowed origin for the request, including protocol, host and port. It is RECOMMENDED to return the value of the request's origin header. If no  Origin  is sent to the back-end CORS headers SCHOULD NOT be sent at all.  http://client.isp.com:80    Access-Control-Allow-Credentials  If authorization is implemented by the back-end the value MUST be  true .  true    Access-Control-Allow-Headers  Comma-separated list of HTTP headers allowed to be send. MUST contain at least  Authorization  if authorization is implemented by the back-end.  Authorization, Content-Type    Access-Control-Allow-Methods  Comma-separated list of HTTP methods allowed to be requested. Back-ends MUST list all implemented HTTP methods for the endpoint here.  OPTIONS, GET, POST, PATCH, PUT, DELETE    Content-Type  SHOULD return the content type delivered by the request that the permission is requested for.  application/json",
            "title": "1. Supporting the OPTIONS method"
        },
        {
            "location": "/cors/index.html#example-request-and-response",
            "text": "Request:  OPTIONS /api/v1/jobs HTTP/1.1\nHost: openeo.cloudprovider.com\nOrigin: http://client.org:8080\nAccess-Control-Request-Method: POST \nAccess-Control-Request-Headers: Authorization, Content-Type  Response:  HTTP/1.1 204 No Content\nAccess-Control-Allow-Origin: http://client.org:8080\nAccess-Control-Allow-Credentials: true\nAccess-Control-Allow-Methods: OPTIONS, GET, POST, PATCH, PUT, DELETE\nAccess-Control-Allow-Headers: Authorization, Content-Type\nContent-Type: application/json",
            "title": "Example request and response"
        },
        {
            "location": "/cors/index.html#2-sending-cors-headers-for-every-endpoint",
            "text": "The following headers MUST be included with every response:     Name  Description  Example      Access-Control-Allow-Origin  Allowed origin for the request, including protocol, host and port. It is RECOMMENDED to return the value of the request's origin header. If no  Origin  is sent to the back-end CORS headers SHOULD NOT be sent at all.  http://client.isp.com:80    Access-Control-Allow-Credentials  If authorization is implemented by the back-end the value MUST be  true .  true",
            "title": "2. Sending CORS headers for every endpoint"
        },
        {
            "location": "/cors/index.html#remarks",
            "text": "Most server can send the required headers and the responses to the OPTIONS requests globally. Otherwise you may want to use a proxy server to add the headers and OPTIONS responses.",
            "title": "Remarks"
        },
        {
            "location": "/apireference/index.html",
            "text": "Placeholder for generated API specification.",
            "title": "API Reference"
        }
    ]
}