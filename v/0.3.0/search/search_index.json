{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"openEO - Concepts and API Reference Work in progress, please contribute by adding issues . openEO develops an open API that connects clients like R, Python and JavaScript to big Earth observation cloud back-ends in a simple and unified way. The following pages introduce the core concepts of the project. Make sure to introduce yourself to the major technical terms used in the openEO project by reading the glossary . The openEO API defines a HTTP API that lets cloud back-ends with large Earth observation datasets communicate with front end analysis applications in an interoperable way. This documentation describes important API concepts and design decisions and gives a complete API reference documentation . As an overview, the openEO API specifies how to discover which Earth observation data and processes are available at cloud back-ends, execute (chained) processes on back-ends, run user-defined functions (UDFs) on back-ends where UDFs can be exposed to the data in different ways, download (intermediate) results as web services, and manage user content including accounting. The API is defined as an OpenAPI 3.0 JSON file. openEO , A Common, Open Source Interface between Earth Observation Data Infrastructures and Front-End Applications is a H2020 project funded under call EO-2-2017: EO Big Data Shift, under proposal number 776242. It will run from Oct 2017 to Sept 2020. This project has received funding from the European Union\u2019s Horizon 2020 research and innovation programme under grant agreement No 776242. The contents of this website reflects only the authors\u2019 view; the European Commission is not responsible for any use that may be made of the information it provides.","title":"Introduction"},{"location":"#openeo-concepts-and-api-reference","text":"Work in progress, please contribute by adding issues . openEO develops an open API that connects clients like R, Python and JavaScript to big Earth observation cloud back-ends in a simple and unified way. The following pages introduce the core concepts of the project. Make sure to introduce yourself to the major technical terms used in the openEO project by reading the glossary . The openEO API defines a HTTP API that lets cloud back-ends with large Earth observation datasets communicate with front end analysis applications in an interoperable way. This documentation describes important API concepts and design decisions and gives a complete API reference documentation . As an overview, the openEO API specifies how to discover which Earth observation data and processes are available at cloud back-ends, execute (chained) processes on back-ends, run user-defined functions (UDFs) on back-ends where UDFs can be exposed to the data in different ways, download (intermediate) results as web services, and manage user content including accounting. The API is defined as an OpenAPI 3.0 JSON file. openEO , A Common, Open Source Interface between Earth Observation Data Infrastructures and Front-End Applications is a H2020 project funded under call EO-2-2017: EO Big Data Shift, under proposal number 776242. It will run from Oct 2017 to Sept 2020. This project has received funding from the European Union\u2019s Horizon 2020 research and innovation programme under grant agreement No 776242. The contents of this website reflects only the authors\u2019 view; the European Commission is not responsible for any use that may be made of the information it provides.","title":"openEO - Concepts and API Reference"},{"location":"apireference-subscriptions/","text":"Placeholder for generated API specification for subscriptions.","title":"Subscriptions API Reference"},{"location":"apireference/","text":"Placeholder for generated API specification.","title":"Core API Reference"},{"location":"arch/","text":"Architecture The openEO API defines a language how clients communicate to back-ends in order to analyze large Earth observation datasets. The API will be implemented by drivers for specific back-ends. Some first architecture considerations are listed below. The openEO API is a contract between clients and back-ends that describes the communication only Each back-end runs its own API instance including the specific back-end driver. There is no API instance that runs more than one driver. Clients in R, Python, and JavaScript connect directly to the back-ends and communicate with the back-ends over HTTPS according to the openEO API specification. API instances can run on back-end servers or additional intermediate layers, which then communicate to back-ends in a back-end specific way. Back-ends may add functionality and extend the API wherever there is need. There will be a central back-end registry service (openEO Hub), to allow users to search for back-ends with specific functionality and or data. The openEO API may define profiles in order group specific functionality. Figure: Architecture - openEO API shown in dark blue Microservices To simplify and structure the development, the API is divided into a few microservices. Microservice Description Capabilities This microservice reports on the capabilities of the back-end, i.e. which API endpoints are implemented, which authentication methods are supported, and whether and how UDFs can be executed at the back-end. EO Data Discovery Describes which datasets and image collections are available at the back-end. Process Discovery Provides services to find out which processes a back-end provides, i.e., what users can do with the available data. UDF Runtime Discovery Allows discovering the programming languages and their runtime environments to execute user-defined functions. Job Management Organizes and manages jobs that run processes on back-ends. File Management Organizes and manages user-uploaded files. Process Graph Management Organizes and manages user-defined process graphs. Web Service Management Web services to to access data and job results, e.g. as WCS or WMTS service. User Content User content and accounting. Authentication Authentication of users.","title":"Architecture"},{"location":"arch/#architecture","text":"The openEO API defines a language how clients communicate to back-ends in order to analyze large Earth observation datasets. The API will be implemented by drivers for specific back-ends. Some first architecture considerations are listed below. The openEO API is a contract between clients and back-ends that describes the communication only Each back-end runs its own API instance including the specific back-end driver. There is no API instance that runs more than one driver. Clients in R, Python, and JavaScript connect directly to the back-ends and communicate with the back-ends over HTTPS according to the openEO API specification. API instances can run on back-end servers or additional intermediate layers, which then communicate to back-ends in a back-end specific way. Back-ends may add functionality and extend the API wherever there is need. There will be a central back-end registry service (openEO Hub), to allow users to search for back-ends with specific functionality and or data. The openEO API may define profiles in order group specific functionality. Figure: Architecture - openEO API shown in dark blue","title":"Architecture"},{"location":"arch/#microservices","text":"To simplify and structure the development, the API is divided into a few microservices. Microservice Description Capabilities This microservice reports on the capabilities of the back-end, i.e. which API endpoints are implemented, which authentication methods are supported, and whether and how UDFs can be executed at the back-end. EO Data Discovery Describes which datasets and image collections are available at the back-end. Process Discovery Provides services to find out which processes a back-end provides, i.e., what users can do with the available data. UDF Runtime Discovery Allows discovering the programming languages and their runtime environments to execute user-defined functions. Job Management Organizes and manages jobs that run processes on back-ends. File Management Organizes and manages user-uploaded files. Process Graph Management Organizes and manages user-defined process graphs. Web Service Management Web services to to access data and job results, e.g. as WCS or WMTS service. User Content User content and accounting. Authentication Authentication of users.","title":"Microservices"},{"location":"codeofconduct/","text":"Contributor Code of Conduct As contributors and maintainers of this project, we pledge to respect all people who contribute through reporting issues, posting feature requests, updating documentation, submitting pull requests or patches, and other activities. We are committed to making participation in this project a harassment-free experience for everyone, regardless of level of experience, gender, gender identity and expression, sexual orientation, disability, personal appearance, body size, race, ethnicity, age, or religion. Examples of unacceptable behavior by participants include the use of sexual language or imagery, derogatory comments or personal attacks, trolling, public or private harassment, insults, or other unprofessional conduct. Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct. Project maintainers who do not follow the Code of Conduct may be removed from the project team. Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by opening an issue or contacting one or more of the project maintainers. This Code of Conduct is adapted from the Contributor Covenant , version 1.0.0, available at http://contributor-covenant.org/version/1/0/0/ .","title":"Contributor Code of Conduct"},{"location":"codeofconduct/#contributor-code-of-conduct","text":"As contributors and maintainers of this project, we pledge to respect all people who contribute through reporting issues, posting feature requests, updating documentation, submitting pull requests or patches, and other activities. We are committed to making participation in this project a harassment-free experience for everyone, regardless of level of experience, gender, gender identity and expression, sexual orientation, disability, personal appearance, body size, race, ethnicity, age, or religion. Examples of unacceptable behavior by participants include the use of sexual language or imagery, derogatory comments or personal attacks, trolling, public or private harassment, insults, or other unprofessional conduct. Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct. Project maintainers who do not follow the Code of Conduct may be removed from the project team. Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by opening an issue or contacting one or more of the project maintainers. This Code of Conduct is adapted from the Contributor Covenant , version 1.0.0, available at http://contributor-covenant.org/version/1/0/0/ .","title":"Contributor Code of Conduct"},{"location":"cors/","text":"Cross-Origin Resource Sharing (CORS) Cross-origin resource sharing (CORS) is a mechanism that allows restricted resources [...] on a web page to be requested from another domain outside the domain from which the first resource was served. [...] CORS defines a way in which a browser and server can interact to determine whether or not it is safe to allow the cross-origin request. It allows for more freedom and functionality than purely same-origin requests, but is more secure than simply allowing all cross-origin requests. Source: https://en.wikipedia.org/wiki/Cross-origin_resource_sharing openEO-based back-ends are usually hosted on a different domain / host than the client that is requesting data from the back-end. Therefore most requests to the back-end are blocked by all modern browsers. This leads to the problem that the JavaScript library (and the Web Editor) can't access any back-end. Therefore, all back-end providers SHOULD support CORS. Without supporting CORS users can't access the back-end with browser-based clients, i.e. the JavaScript client . CORS is a recommendation of the W3C organization. The following chapters will explain how back-end providers can implement CORS support. 1. Supporting the OPTIONS method All endpoints must respond to the OPTIONS HTTP method. This is a response for the preflight requests made by the browsers. It needs to respond with a status code of 204 and send the HTTP headers shown in the table below. No body needs to be provided. Name Description Example Access-Control-Allow-Origin Allowed origin for the request, including protocol, host and port. It is RECOMMENDED to return the value of the request's origin header. If no Origin is sent to the back-end CORS headers SCHOULD NOT be sent at all. http://client.isp.com:80 Access-Control-Allow-Credentials If authorization is implemented by the back-end the value MUST be true . true Access-Control-Allow-Headers Comma-separated list of HTTP headers allowed to be send. MUST contain at least Authorization if authorization is implemented by the back-end. Authorization, Content-Type Access-Control-Allow-Methods Comma-separated list of HTTP methods allowed to be requested. Back-ends MUST list all implemented HTTP methods for the endpoint here. OPTIONS, GET, POST, PATCH, PUT, DELETE Content-Type SHOULD return the content type delivered by the request that the permission is requested for. application/json Example request and response Request: OPTIONS /api/v1/jobs HTTP / 1.1 Host : openeo.cloudprovider.com Origin : http://client.org:8080 Access-Control-Request-Method : POST Access-Control-Request-Headers : Authorization, Content-Type Response: HTTP / 1.1 204 No Content Access-Control-Allow-Origin : http://client.org:8080 Access-Control-Allow-Credentials : true Access-Control-Allow-Methods : OPTIONS, GET, POST, PATCH, PUT, DELETE Access-Control-Allow-Headers : Authorization, Content-Type Content-Type : application/json 2. Sending CORS headers for every endpoint The following headers MUST be included with every response: Name Description Example Access-Control-Allow-Origin Allowed origin for the request, including protocol, host and port. It is RECOMMENDED to return the value of the request's origin header. If no Origin is sent to the back-end CORS headers SHOULD NOT be sent at all. http://client.isp.com:80 Access-Control-Allow-Credentials If authorization is implemented by the back-end the value MUST be true . true Hint Most server can send the required headers and the responses to the OPTIONS requests globally. Otherwise you may want to use a proxy server to add the headers and OPTIONS responses.","title":"CORS"},{"location":"cors/#cross-origin-resource-sharing-cors","text":"Cross-origin resource sharing (CORS) is a mechanism that allows restricted resources [...] on a web page to be requested from another domain outside the domain from which the first resource was served. [...] CORS defines a way in which a browser and server can interact to determine whether or not it is safe to allow the cross-origin request. It allows for more freedom and functionality than purely same-origin requests, but is more secure than simply allowing all cross-origin requests. Source: https://en.wikipedia.org/wiki/Cross-origin_resource_sharing openEO-based back-ends are usually hosted on a different domain / host than the client that is requesting data from the back-end. Therefore most requests to the back-end are blocked by all modern browsers. This leads to the problem that the JavaScript library (and the Web Editor) can't access any back-end. Therefore, all back-end providers SHOULD support CORS. Without supporting CORS users can't access the back-end with browser-based clients, i.e. the JavaScript client . CORS is a recommendation of the W3C organization. The following chapters will explain how back-end providers can implement CORS support.","title":"Cross-Origin Resource Sharing (CORS)"},{"location":"cors/#1-supporting-the-options-method","text":"All endpoints must respond to the OPTIONS HTTP method. This is a response for the preflight requests made by the browsers. It needs to respond with a status code of 204 and send the HTTP headers shown in the table below. No body needs to be provided. Name Description Example Access-Control-Allow-Origin Allowed origin for the request, including protocol, host and port. It is RECOMMENDED to return the value of the request's origin header. If no Origin is sent to the back-end CORS headers SCHOULD NOT be sent at all. http://client.isp.com:80 Access-Control-Allow-Credentials If authorization is implemented by the back-end the value MUST be true . true Access-Control-Allow-Headers Comma-separated list of HTTP headers allowed to be send. MUST contain at least Authorization if authorization is implemented by the back-end. Authorization, Content-Type Access-Control-Allow-Methods Comma-separated list of HTTP methods allowed to be requested. Back-ends MUST list all implemented HTTP methods for the endpoint here. OPTIONS, GET, POST, PATCH, PUT, DELETE Content-Type SHOULD return the content type delivered by the request that the permission is requested for. application/json","title":"1. Supporting the OPTIONS method"},{"location":"cors/#example-request-and-response","text":"Request: OPTIONS /api/v1/jobs HTTP / 1.1 Host : openeo.cloudprovider.com Origin : http://client.org:8080 Access-Control-Request-Method : POST Access-Control-Request-Headers : Authorization, Content-Type Response: HTTP / 1.1 204 No Content Access-Control-Allow-Origin : http://client.org:8080 Access-Control-Allow-Credentials : true Access-Control-Allow-Methods : OPTIONS, GET, POST, PATCH, PUT, DELETE Access-Control-Allow-Headers : Authorization, Content-Type Content-Type : application/json","title":"Example request and response"},{"location":"cors/#2-sending-cors-headers-for-every-endpoint","text":"The following headers MUST be included with every response: Name Description Example Access-Control-Allow-Origin Allowed origin for the request, including protocol, host and port. It is RECOMMENDED to return the value of the request's origin header. If no Origin is sent to the back-end CORS headers SHOULD NOT be sent at all. http://client.isp.com:80 Access-Control-Allow-Credentials If authorization is implemented by the back-end the value MUST be true . true Hint Most server can send the required headers and the responses to the OPTIONS requests globally. Otherwise you may want to use a proxy server to add the headers and OPTIONS responses.","title":"2. Sending CORS headers for every endpoint"},{"location":"errors/","text":"Status and error handling The success of requests MUST be indicated using HTTP status codes according to RFC 7231 . If the API responds with a status code between 100 and 399 the back-end indicates that the request has been handled successfully. In general an error is communicated with a status code between 400 and 599. Client errors are defined as a client passing invalid data to the service and the service correctly rejecting that data. Examples include invalid credentials, incorrect parameters, unknown versions, or similar. These are generally \"4xx\" HTTP error codes and are the result of a client passing incorrect or invalid data. Client errors do not contribute to overall API availability. Server errors are defined as the server failing to correctly return in response to a valid client request. These are generally \"5xx\" HTTP error codes. Server errors do contribute to the overall API availability. Calls that fail due to rate limiting or quota failures MUST NOT count as server errors. JSON error object A JSON error object SHOULD be sent with all responses that have a status code between 400 and 599. { \"id\" : \"936DA01F-9ABD-4D9D-80C7-02AF85C822A8\" , \"code\" : 123 , \"message\" : \"A sample error message.\" , \"url\" : \"http://www.openeo.org/docs/errors/123\" } Sending code and message is REQUIRED. A back-end MAY add a free-form id (unique identifier) to the error response to be able to log and track errors with further non-disclosable details. The code is either one of the standardized openEO error codes below or a proprietary error code with a number greater than 10000. The message explains the reason the server is rejecting the request. For \"4xx\" error codes the message explains how the client needs to modify the request. By default the message MUST be sent in English language. Content Negotiation is used to localize the error messages: If an Acceppt-Language header is sent by the client and a translation is available, the message should be translated accordingly and the Content-Language header must be present in the response. See \" How to localize your API \" for more information. url is an OPTIONAL attribute and contains a link to a resource that is explaining the error and potential solutions in-depth. Standardized status codes The openEO API usually uses the following HTTP status codes for successful requests: 200 OK : Indicates a successful request with a response body being sent. 201 Created Indicates a successful request that successfully created a new resource. Sends a Location header to the newly created resource without a response body. 202 Accepted Indicates a successful request that successfully queued the creation of a new resource, but it has not been created yet. The response is sent without a response body. 204 No Content : Indicates a successful request without a response body being sent. The openEO API often uses the following HTTP status codes for failed requests: 400 Bad request : The back-end responds with this error code whenever the error has its origin on client side and no other HTTP status code in the 400 range is suitable. 401 Unauthorized : The client did not provide any authorization details (usually using the Authorization header), but authorization is required for this request to be processed. 403 Forbidden : The client did provide authorization details (usually using the Authorization header), but the provided credentials or the authorization token is invalid or has expired. 404 Not Found : The resource specified by the path does not exist, i.e. one of the the resources belonging to the specified identifiers are not available at the back-end. Note: Unsupported endpoints MUST use HTTP status code 501. 500 Internal Server Error : The error has its origin on server side and no other status code in the 500 range is suitable. 501 Not implemented : An endpoint is specified in the openEO API, but is not supported. If a HTTP status code in the 400 range is returned, the client SHOULD NOT repeat the request without modifications. For HTTP status code in the 500 range, the client MAY repeat the same request later. All HTTP status codes defined in RFC 7231 in the 400 and 500 ranges can be used as openEO error code in addition to the most used status codes mentioned here. Responding with openEO error codes 400 and 500 SHOULD be avoided in favor of any more specific standardized or proprietary openEO error code. General error codes (xxx) openEO Error Code Description Message HTTP Status Code 404 To be used if the value of a path parameter is invalid, i.e. the requested resource is not available. Note: Unsupported endpoints MUST use code 501. Not Found. 404 501 The back-end responds with this error code whenever an endpoint is specified in the openEO API, but is not supported. Not implemented. 501 503 Service unavailable. 503 601 Parameter X is invalid. 400 611 Invalid or unsupported CRS specified. Invalid CRS specified. 400 612 Coordinate is out of bounds. 400 Capabilities (11xx) None yet. Data and process discovery (12xx) None yet. UDFs (13xx) openEO Error Code Description Message HTTP Status Code 1301 UDF programming language not supported. 400 / 404 1302 UDF type not supported. 400 / 404 File Handling (14xx) openEO Error Code Description Message HTTP Status Code 1401 Server couldn't store file due to server-side reasons. Unable to store file. 500 1402 The storage quota has been exceeded by the user. Insufficient Storage. 400 1410 File format, file extension or mime type is not allowed. File type not allowed. 400 1411 File exceeds allowed maximum file size. File size it too large. 400 1412 The content of the file is invalid. File content is invalid. 400 1413 The file is locked by a running job or another process. File is locked. 400 Process graphs (2xxx) openEO Error Code Description Message HTTP Status Code 2001 No process graph specified. 400 2002 Process graph structure is invalid. 400 2003 The array X contains values of multiple types. 400 2101 Process X is not supported. 400 2102 Process argument X is not supported. 400 2103 Invalid value Y for the process argument X specified. 400 2104 Required process argument X is missing. 400 Jobs (3xxx) openEO Error Code Description Message HTTP Status Code 408 The (synchronous) request timed out. Request timed out. 408 3001 Output format not supported. 400 3002 Output format argument X is not supported. 400 3003 Invalid value Y for the output format argument X specified. 400 3004 Data can't be transformed into the requested output format. 400 3005 The job is currently locked due to an enabled service or a running batch computation and can't be modified meanwhile. Job is locked. 400 3006 The job has not finished computing the results yet. Try again later. Results are not finished yet. 400 Authorization, user content and billing (401-403, 4xxx) openEO Error Code Description Message HTTP Status Code 401 The back-end responds with this error code whenever the HTTP status code 401 is appropriate (see above) and no other openEO error code in the 4000 range is suitable. Unauthorized. 401 402 The budget required to fulfil the request are insufficient. Payment required. 402 403 The back-end responds with this error code whenever the HTTP status code 403 is appropriate (see above) and no other openEO error code in the 4000 range is suitable. Forbidden. 403 4001 The specified password is not considered secure by the policy of the back-end provider or no password was given at all. The user needs to specify a different password to proceed. Password does not meet the requirements. 400 4031 Invalid authentication scheme (e.g. Bearer). 403 4032 Authorization token invalid or expired. 403 4033 Credentials are not correct. 403 Web services (5xxx) openEO Error Code Description Message HTTP Status Code 5001 Service type is not supported. 400 5101 Invalid job id specified. Job does not exist. 400 5102 Service argument X is not supported. 400 5103 Invalid value Y for the service argument X specified. 400 5104 Required service argument X is missing. 400","title":"Error Handling"},{"location":"errors/#status-and-error-handling","text":"The success of requests MUST be indicated using HTTP status codes according to RFC 7231 . If the API responds with a status code between 100 and 399 the back-end indicates that the request has been handled successfully. In general an error is communicated with a status code between 400 and 599. Client errors are defined as a client passing invalid data to the service and the service correctly rejecting that data. Examples include invalid credentials, incorrect parameters, unknown versions, or similar. These are generally \"4xx\" HTTP error codes and are the result of a client passing incorrect or invalid data. Client errors do not contribute to overall API availability. Server errors are defined as the server failing to correctly return in response to a valid client request. These are generally \"5xx\" HTTP error codes. Server errors do contribute to the overall API availability. Calls that fail due to rate limiting or quota failures MUST NOT count as server errors.","title":"Status and error handling"},{"location":"errors/#json-error-object","text":"A JSON error object SHOULD be sent with all responses that have a status code between 400 and 599. { \"id\" : \"936DA01F-9ABD-4D9D-80C7-02AF85C822A8\" , \"code\" : 123 , \"message\" : \"A sample error message.\" , \"url\" : \"http://www.openeo.org/docs/errors/123\" } Sending code and message is REQUIRED. A back-end MAY add a free-form id (unique identifier) to the error response to be able to log and track errors with further non-disclosable details. The code is either one of the standardized openEO error codes below or a proprietary error code with a number greater than 10000. The message explains the reason the server is rejecting the request. For \"4xx\" error codes the message explains how the client needs to modify the request. By default the message MUST be sent in English language. Content Negotiation is used to localize the error messages: If an Acceppt-Language header is sent by the client and a translation is available, the message should be translated accordingly and the Content-Language header must be present in the response. See \" How to localize your API \" for more information. url is an OPTIONAL attribute and contains a link to a resource that is explaining the error and potential solutions in-depth.","title":"JSON error object"},{"location":"errors/#standardized-status-codes","text":"The openEO API usually uses the following HTTP status codes for successful requests: 200 OK : Indicates a successful request with a response body being sent. 201 Created Indicates a successful request that successfully created a new resource. Sends a Location header to the newly created resource without a response body. 202 Accepted Indicates a successful request that successfully queued the creation of a new resource, but it has not been created yet. The response is sent without a response body. 204 No Content : Indicates a successful request without a response body being sent. The openEO API often uses the following HTTP status codes for failed requests: 400 Bad request : The back-end responds with this error code whenever the error has its origin on client side and no other HTTP status code in the 400 range is suitable. 401 Unauthorized : The client did not provide any authorization details (usually using the Authorization header), but authorization is required for this request to be processed. 403 Forbidden : The client did provide authorization details (usually using the Authorization header), but the provided credentials or the authorization token is invalid or has expired. 404 Not Found : The resource specified by the path does not exist, i.e. one of the the resources belonging to the specified identifiers are not available at the back-end. Note: Unsupported endpoints MUST use HTTP status code 501. 500 Internal Server Error : The error has its origin on server side and no other status code in the 500 range is suitable. 501 Not implemented : An endpoint is specified in the openEO API, but is not supported. If a HTTP status code in the 400 range is returned, the client SHOULD NOT repeat the request without modifications. For HTTP status code in the 500 range, the client MAY repeat the same request later. All HTTP status codes defined in RFC 7231 in the 400 and 500 ranges can be used as openEO error code in addition to the most used status codes mentioned here. Responding with openEO error codes 400 and 500 SHOULD be avoided in favor of any more specific standardized or proprietary openEO error code.","title":"Standardized status codes"},{"location":"errors/#general-error-codes-xxx","text":"openEO Error Code Description Message HTTP Status Code 404 To be used if the value of a path parameter is invalid, i.e. the requested resource is not available. Note: Unsupported endpoints MUST use code 501. Not Found. 404 501 The back-end responds with this error code whenever an endpoint is specified in the openEO API, but is not supported. Not implemented. 501 503 Service unavailable. 503 601 Parameter X is invalid. 400 611 Invalid or unsupported CRS specified. Invalid CRS specified. 400 612 Coordinate is out of bounds. 400","title":"General error codes (xxx)"},{"location":"errors/#capabilities-11xx","text":"None yet.","title":"Capabilities (11xx)"},{"location":"errors/#data-and-process-discovery-12xx","text":"None yet.","title":"Data and process discovery (12xx)"},{"location":"errors/#udfs-13xx","text":"openEO Error Code Description Message HTTP Status Code 1301 UDF programming language not supported. 400 / 404 1302 UDF type not supported. 400 / 404","title":"UDFs (13xx)"},{"location":"errors/#file-handling-14xx","text":"openEO Error Code Description Message HTTP Status Code 1401 Server couldn't store file due to server-side reasons. Unable to store file. 500 1402 The storage quota has been exceeded by the user. Insufficient Storage. 400 1410 File format, file extension or mime type is not allowed. File type not allowed. 400 1411 File exceeds allowed maximum file size. File size it too large. 400 1412 The content of the file is invalid. File content is invalid. 400 1413 The file is locked by a running job or another process. File is locked. 400","title":"File Handling (14xx)"},{"location":"errors/#process-graphs-2xxx","text":"openEO Error Code Description Message HTTP Status Code 2001 No process graph specified. 400 2002 Process graph structure is invalid. 400 2003 The array X contains values of multiple types. 400 2101 Process X is not supported. 400 2102 Process argument X is not supported. 400 2103 Invalid value Y for the process argument X specified. 400 2104 Required process argument X is missing. 400","title":"Process graphs (2xxx)"},{"location":"errors/#jobs-3xxx","text":"openEO Error Code Description Message HTTP Status Code 408 The (synchronous) request timed out. Request timed out. 408 3001 Output format not supported. 400 3002 Output format argument X is not supported. 400 3003 Invalid value Y for the output format argument X specified. 400 3004 Data can't be transformed into the requested output format. 400 3005 The job is currently locked due to an enabled service or a running batch computation and can't be modified meanwhile. Job is locked. 400 3006 The job has not finished computing the results yet. Try again later. Results are not finished yet. 400","title":"Jobs (3xxx)"},{"location":"errors/#authorization-user-content-and-billing-401-403-4xxx","text":"openEO Error Code Description Message HTTP Status Code 401 The back-end responds with this error code whenever the HTTP status code 401 is appropriate (see above) and no other openEO error code in the 4000 range is suitable. Unauthorized. 401 402 The budget required to fulfil the request are insufficient. Payment required. 402 403 The back-end responds with this error code whenever the HTTP status code 403 is appropriate (see above) and no other openEO error code in the 4000 range is suitable. Forbidden. 403 4001 The specified password is not considered secure by the policy of the back-end provider or no password was given at all. The user needs to specify a different password to proceed. Password does not meet the requirements. 400 4031 Invalid authentication scheme (e.g. Bearer). 403 4032 Authorization token invalid or expired. 403 4033 Credentials are not correct. 403","title":"Authorization, user content and billing (401-403, 4xxx)"},{"location":"errors/#web-services-5xxx","text":"openEO Error Code Description Message HTTP Status Code 5001 Service type is not supported. 400 5101 Invalid job id specified. Job does not exist. 400 5102 Service argument X is not supported. 400 5103 Invalid value Y for the service argument X specified. 400 5104 Required service argument X is missing. 400","title":"Web services (5xxx)"},{"location":"examples-poc/","text":"Proof of Concept (API v0.0.2) This page gives a detailed description of the openEO proof of concept and gives a list and specification of what needs to be implemented. The proof of concept will consist of at least three clearly defined example processes (see below), a prototypical API specification including communication API call sequences of the processes (see below), implementations of the processes on three back-ends, and prototypical clients in R, Python and potentially JavaScript. Below, we define example use cases and how they are translated to sequences of API calls: Deriving minimum NDVI measurements over pixel time series of Sentinel 2 imagery Create a monthly aggregated Sentinel 1 product from a custom Python script Compute time series of zonal (regional) statistics of Sentinel 2 imagery over user-uploaded polygons Note Authentication is not included in these examples. Enabling authentication needs the placeholder <Origin> to be set to the requesting host, including protocol, host name/IP and port, e.g. http://localhost:8080 . This could be done by using the Origin header value from the request. Use Case 1 Deriving minimum NDVI measurements over pixel time series of Sentinel 2 imagery. 1. Check whether Sentinel 2A Level 1C data is available at the back-end Request GET /data/Sentinel2A-L1C HTTP / 1.1 Response HTTP / 1.1 200 OK Content-Type : application/json; charset=utf-8 Access-Control-Allow-Origin : <Origin> Access-Control-Allow-Credentials : true { \"product_id\" : \"Sentinel-2A-L1C\" , \"description\" : \"Sentinel 2 Level-1C: Top-of-atmosphere reflectances in cartographic geometry\" , \"source\" : \"European Space Agency (ESA)\" , \"extent\" :[ -34 , 35 , 39 , 71 ], \"time\" :[ \"2016-01-01\" , \"2017-10-01\" ], \"bands\" :[ { \"band_id\" : \"1\" , \"wavelength_nm\" : 443.9 , \"res_m\" : 60 , \"scale\" : 0.0001 , \"offset\" : 0 , \"type\" : \"int16\" , \"unit\" : \"1\" }, { \"band_id\" : \"2\" , \"name\" : \"blue\" , \"wavelength_nm\" : 496.6 , \"res_m\" : 10 , \"scale\" : 0.0001 , \"offset\" : 0 , \"type\" : \"int16\" , \"unit\" : \"1\" }, { \"band_id\" : \"3\" , \"name\" : \"green\" , \"wavelength_nm\" : 560 , \"res_m\" : 10 , \"scale\" : 0.0001 , \"offset\" : 0 , \"type\" : \"int16\" , \"unit\" : \"1\" }, { \"band_id\" : \"4\" , \"name\" : \"red\" , \"wavelength_nm\" : 664.5 , \"res_m\" : 10 , \"scale\" : 0.0001 , \"offset\" : 0 , \"type\" : \"int16\" , \"unit\" : \"1\" }, { \"band_id\" : \"5\" , \"wavelength_nm\" : 703.9 , \"res_m\" : 20 , \"scale\" : 0.0001 , \"offset\" : 0 , \"type\" : \"int16\" , \"unit\" : \"1\" }, { \"band_id\" : \"6\" , \"wavelength_nm\" : 740.2 , \"res_m\" : 20 , \"scale\" : 0.0001 , \"offset\" : 0 , \"type\" : \"int16\" , \"unit\" : \"1\" }, { \"band_id\" : \"7\" , \"wavelength_nm\" : 782.5 , \"res_m\" : 20 , \"scale\" : 0.0001 , \"offset\" : 0 , \"type\" : \"int16\" , \"unit\" : \"1\" }, { \"band_id\" : \"8\" , \"name\" : \"nir\" , \"wavelength_nm\" : 835.1 , \"res_m\" : 10 , \"scale\" : 0.0001 , \"offset\" : 0 , \"type\" : \"int16\" , \"unit\" : \"1\" }, { \"band_id\" : \"8a\" , \"wavelength_nm\" : 864.8 , \"res_m\" : 20 , \"scale\" : 0.0001 , \"offset\" : 0 , \"type\" : \"int16\" , \"unit\" : \"1\" }, { \"band_id\" : \"9\" , \"wavelength_nm\" : 945 , \"res_m\" : 60 , \"scale\" : 0.0001 , \"offset\" : 0 , \"type\" : \"int16\" , \"unit\" : \"1\" }, { \"band_id\" : \"10\" , \"wavelength_nm\" : 1373.5 , \"res_m\" : 60 , \"scale\" : 0.0001 , \"offset\" : 0 , \"type\" : \"int16\" , \"unit\" : \"1\" }, { \"band_id\" : \"11\" , \"wavelength_nm\" : 1613.7 , \"res_m\" : 20 , \"scale\" : 0.0001 , \"offset\" : 0 , \"type\" : \"int16\" , \"unit\" : \"1\" }, { \"band_id\" : \"12\" , \"wavelength_nm\" : 2202.4 , \"res_m\" : 20 , \"scale\" : 0.0001 , \"offset\" : 0 , \"type\" : \"int16\" , \"unit\" : \"1\" } ] } 2. Check that needed processes are available Request GET /processes/filter_bbox HTTP / 1.1 Response HTTP / 1.1 200 OK Content-Type : application/json; charset=utf-8 Access-Control-Allow-Origin : <Origin> Access-Control-Allow-Credentials : true { \"process_id\" : \"filter_bbox\" , \"description\" : \"Drops observations from a collection that are located outside of a given bounding box.\" , \"args\" :{ \"imagery\" :{ \"description\" : \"array of input collections with one element\" }, \"left\" :{ \"description\" : \"left boundary (longitude / easting)\" }, \"right\" :{ \"description\" : \"right boundary (longitude / easting)\" }, \"top\" :{ \"description\" : \"top boundary (latitude / northing)\" }, \"bottom\" :{ \"description\" : \"bottom boundary (latitude / northing)\" }, \"srs\" :{ \"description\" : \"spatial reference system of boundaries as proj4 or EPSG:12345 like string\" } } } Request GET /processes/filter_daterange HTTP / 1.1 Response HTTP / 1.1 200 OK Content-Type : application/json; charset=utf-8 Access-Control-Allow-Origin : <Origin> Access-Control-Allow-Credentials : true { \"process_id\" : \"filter_daterange\" , \"description\" : \"Drops observations from a collection that have been captured before a start or after a given end date.\" , \"args\" :{ \"imagery\" :{ \"description\" : \"array of input collections with one element\" }, \"from\" :{ \"description\" : \"start date\" }, \"to\" :{ \"description\" : \"end date\" } } } Request GET /processes/NDVI HTTP / 1.1 Response HTTP / 1.1 200 OK Content-Type : application/json; charset=utf-8 Access-Control-Allow-Origin : <Origin> Access-Control-Allow-Credentials : true { \"process_id\" : \"NDVI\" , \"description\" : \"Finds the minimum value of time series for all bands of the input dataset.\" , \"args\" :{ \"imagery\" :{ \"description\" : \"array of input collections with one element\" }, \"red\" :{ \"description\" : \"reference to the red band\" }, \"nir\" :{ \"description\" : \"reference to the nir band\" } } } Request GET /processes/min_time HTTP / 1.1 Response HTTP / 1.1 200 OK Content-Type : application/json; charset=utf-8 Access-Control-Allow-Origin : <Origin> Access-Control-Allow-Credentials : true { \"process_id\" : \"min_time\" , \"description\" : \"Finds the minimum value of time series for all bands of the input dataset.\" , \"args\" :{ \"imagery\" :{ \"description\" : \"array of input collections with one element\" } } } 3. Create a job at the back-end Request POST /jobs HTTP / 1.1 Content-Type : application/json; charset=utf-8 { \"process_graph\" :{ \"process_id\" : \"min_time\" , \"args\" :{ \"imagery\" :{ \"process_id\" : \"NDVI\" , \"args\" :{ \"imagery\" :{ \"process_id\" : \"filter_daterange\" , \"args\" :{ \"imagery\" :{ \"process_id\" : \"filter_bbox\" , \"args\" :{ \"imagery\" :{ \"product_id\" : \"S2_L2A_T32TPS_20M\" }, \"left\" : 652000 , \"right\" : 672000 , \"top\" : 5161000 , \"bottom\" : 5181000 , \"srs\" : \"EPSG:32632\" } }, \"from\" : \"2017-01-01\" , \"to\" : \"2017-01-31\" } }, \"red\" : \"B04\" , \"nir\" : \"B8A\" } } } } } Response HTTP / 1.1 200 OK Content-Type : application/json; charset=utf-8 Access-Control-Allow-Origin : <Origin> Access-Control-Allow-Credentials : true { \"job_id\" : \"2a8ffb20c2b235a3f3e3351f\" , \"status\" : \"submitted\" , \"submitted\" : \"2017-01-01T09:32:12Z\" , \"updated\" : \"2017-01-01T09:36:18Z\" , \"user_id\" : \"bd6f9faf93b4\" , \"consumed_credits\" : 0 } 4. Create a WCS service Request POST /services HTTP / 1.1 Content-Type : application/json; charset=utf-8 { \"job_id\" : \"2a8ffb20c2b235a3f3e3351f\" , \"type\" : \"wcs\" , \"args\" :{ \"VERSION\" : \"2.0.1\" } } Response HTTP / 1.1 200 OK Content-Type : application/json; charset=utf-8 Access-Control-Allow-Origin : <Origin> Access-Control-Allow-Credentials : true { \"service_id\" : \"4dab456f6501bbcd\" , \"service_url\" : \"https://openeo.org/4dab456f6501bbcd/wcs\" , \"service_type\" : \"wcs\" , \"service_args\" :{ \"VERSION\" : \"2.0.1\" }, \"job_id\" : \"2a8ffb20c2b235a3f3e3351f\" } 5. Download the data on demand with WCS Request GET https://openeo.org/4dab456f6501bbcd/wcs?SERVICE=WCS&VERSION=2.0.1&REQUEST=GetCapabilities HTTP / 1.1 Response omitted Request GET https://openeo.org/4dab456f6501bbcd/wcs?SERVICE=WCS&VERSION=2.0.1&REQUEST=GetCoverage&COVERAGEID=2a8ffb20c2b235a3f3e3351f&FORMAT=image/tiff&SUBSET=x,http://www.opengis.net/def/crs/EPSG/0/4326(16.1,16.5)&SUBSET=y,http://www.opengis.net/def/crs/EPSG/0/4326(47.9,48.6)&&SIZE=x(200)&SIZE=y(200) HTTP / 1.1 Response omitted 6. Stop the job (and the service) Request PATCH /jobs/2a8ffb20c2b235a3f3e3351f/cancel HTTP / 1.1 Response HTTP / 1.1 200 OK Access-Control-Allow-Origin : <Origin> Access-Control-Allow-Credentials : true Use Case 2 Create a monthly aggregated Sentinel 1 product from a custom Python script. 1. Ask the back-end for available Sentinel 1 data Request GET /data/Sentinel1-L1-IW-GRD HTTP / 1.1 Response HTTP / 1.1 200 OK Content-Type : application/json; charset=utf-8 Access-Control-Allow-Origin : <Origin> Access-Control-Allow-Credentials : true { \"product_id\" : \"Sentinel1-L1-IW-GRD\" , \"description\" : \"Sentinel 1 C-band Synthetic Aperture Radar (SAR) Ground Range Data\" , \"source\" : \"European Space Agency (ESA)\" , \"extent\" :[ -34 , 35 , 39 , 71 ], \"time\" :[ \"2016-01-01\" , \"2017-10-01\" ], \"bands\" :[ { \"band_id\" : \"VV\" }, { \"band_id\" : \"VH\" } ] } 2. Ask the back-end whether it supports Python UDFs of type aggregate_time and get details about expected parameters Request GET /udf_runtimes HTTP / 1.1 Response HTTP / 1.1 200 OK Content-Type : application/json; charset=utf-8 Access-Control-Allow-Origin : <Origin> Access-Control-Allow-Credentials : true { \"Python\" :{ \"udf_types\" :[ \"reduce_time\" , \"aggregate_time\" , \"apply_pixel\" ], \"versions\" :{ \"3.6.3\" :{ \"packages\" :[ \"numpy\" , \"scipy\" , \"pandas\" , \"matplotlib\" , \"ipython\" , \"jupyter\" , \"GDAL\" ] } } } } Request GET /udf_runtimes/Python/aggregate_time HTTP / 1.1 Response HTTP / 1.1 200 OK Content-Type : application/json; charset=utf-8 Access-Control-Allow-Origin : <Origin> Access-Control-Allow-Credentials : true { \"process_id\" : \"/udf/Python/aggregate_time\" , \"description\" : \"Runs a Python script for each time series of the input dataset.\" , \"args\" :{ \"imagery\" :{ \"description\" : \"array of input collections with one element\" }, \"script\" :{ \"description\" : \"Python script that will be executed over all time series, gets time series as (Pandas) DataFrame and expects a new DataFrame as output.\" }, \"version\" :{ \"description\" : \"Python version to use, defaults to the latest available version.\" , \"required\" : false , \"default\" : \"latest\" } } } 3. Upload python script Request PUT /users/me/files/s1_aggregate.py HTTP / 1.1 Response HTTP / 1.1 200 OK Access-Control-Allow-Origin : <Origin> Access-Control-Allow-Credentials : true 4. Create a job Request POST /jobs HTTP / 1.1 Content-Type : application/json; charset=utf-8 { \"process_graph\" :{ \"process_id\" : \"/udf/Python/aggregate_time\" , \"args\" :{ \"script\" : \"/users/me/files/s1_aggregate.py\" , \"imagery\" :{ \"process_id\" : \"filter_daterange\" , \"args\" :{ \"imagery\" :{ \"process_id\" : \"filter_bbox\" , \"args\" :{ \"imagery\" :{ \"product_id\" : \"Sentinel1-L1-IW-GRD\" }, \"left\" : 16.1 , \"right\" : 16.6 , \"top\" : 48.6 , \"bottom\" : 47.2 , \"srs\" : \"EPSG:4326\" } }, \"from\" : \"2017-01-01\" , \"to\" : \"2017-01-31\" } } } } } Response HTTP / 1.1 200 OK Content-Type : application/json; charset=utf-8 Access-Control-Allow-Origin : <Origin> Access-Control-Allow-Credentials : true { \"job_id\" : \"3723c32fb7b24698832ca71f2d3f18aa\" , \"status\" : \"submitted\" , \"submitted\" : \"2017-01-01T09:32:12Z\" , \"updated\" : \"2017-01-01T09:36:18Z\" , \"user_id\" : \"bd6f9faf93b4\" , \"consumed_credits\" : 0 } 5. Create a TMS service Request POST /services HTTP / 1.1 Content-Type : application/json; charset=utf-8 { \"job_id\" : \"3723c32fb7b24698832ca71f2d3f18aa\" , \"type\" : \"tms\" } Response HTTP / 1.1 200 OK Content-Type : application/json; charset=utf-8 Access-Control-Allow-Origin : <Origin> Access-Control-Allow-Credentials : true { \"service_id\" : \"9dab4b6f6523\" , \"service_url\" : \"http://cdn.cloudprovider.com/openeo/services/9dab4b6f6523/tms\" , \"service_type\" : \"tms\" , \"job_id\" : \"3723c32fb7b24698832ca71f2d3f18aa\" } 6. Download results as TMS Example Request GET http://cdn.cloudprovider.com/openeo/services/9dab4b6f6523/tms/2017-01-01/12/2232/2668/?bands=1 HTTP / 1.1 Response omitted Use Case 3 Compute time series of zonal (regional) statistics of Sentinel 2 imagery over user-uploaded polygons 1. Check whether Sentinel 2A Level 1C data is available at the back-end Request GET /data/Sentinel2A-L1C HTTP / 1.1 Response HTTP / 1.1 200 OK Content-Type : application/json; charset=utf-8 Access-Control-Allow-Origin : <Origin> Access-Control-Allow-Credentials : true { \"product_id\" : \"Sentinel-2A-L1C\" , \"description\" : \"Sentinel 2 Level-1C: Top-of-atmosphere reflectances in cartographic geometry\" , \"source\" : \"European Space Agency (ESA)\" , \"extent\" :[ -34 , 35 , 39 , 71 ], \"time\" :[ \"2016-01-01\" , \"2017-10-01\" ], \"bands\" :[ { \"band_id\" : \"1\" , \"wavelength_nm\" : 443.9 , \"res_m\" : 60 , \"scale\" : 0.0001 , \"offset\" : 0 , \"type\" : \"int16\" , \"unit\" : \"1\" }, { \"band_id\" : \"2\" , \"name\" : \"blue\" , \"wavelength_nm\" : 496.6 , \"res_m\" : 10 , \"scale\" : 0.0001 , \"offset\" : 0 , \"type\" : \"int16\" , \"unit\" : \"1\" }, { \"band_id\" : \"3\" , \"name\" : \"green\" , \"wavelength_nm\" : 560 , \"res_m\" : 10 , \"scale\" : 0.0001 , \"offset\" : 0 , \"type\" : \"int16\" , \"unit\" : \"1\" }, { \"band_id\" : \"4\" , \"name\" : \"red\" , \"wavelength_nm\" : 664.5 , \"res_m\" : 10 , \"scale\" : 0.0001 , \"offset\" : 0 , \"type\" : \"int16\" , \"unit\" : \"1\" }, { \"band_id\" : \"5\" , \"wavelength_nm\" : 703.9 , \"res_m\" : 20 , \"scale\" : 0.0001 , \"offset\" : 0 , \"type\" : \"int16\" , \"unit\" : \"1\" }, { \"band_id\" : \"6\" , \"wavelength_nm\" : 740.2 , \"res_m\" : 20 , \"scale\" : 0.0001 , \"offset\" : 0 , \"type\" : \"int16\" , \"unit\" : \"1\" }, { \"band_id\" : \"7\" , \"wavelength_nm\" : 782.5 , \"res_m\" : 20 , \"scale\" : 0.0001 , \"offset\" : 0 , \"type\" : \"int16\" , \"unit\" : \"1\" }, { \"band_id\" : \"8\" , \"name\" : \"nir\" , \"wavelength_nm\" : 835.1 , \"res_m\" : 10 , \"scale\" : 0.0001 , \"offset\" : 0 , \"type\" : \"int16\" , \"unit\" : \"1\" }, { \"band_id\" : \"8a\" , \"wavelength_nm\" : 864.8 , \"res_m\" : 20 , \"scale\" : 0.0001 , \"offset\" : 0 , \"type\" : \"int16\" , \"unit\" : \"1\" }, { \"band_id\" : \"9\" , \"wavelength_nm\" : 945 , \"res_m\" : 60 , \"scale\" : 0.0001 , \"offset\" : 0 , \"type\" : \"int16\" , \"unit\" : \"1\" }, { \"band_id\" : \"10\" , \"wavelength_nm\" : 1373.5 , \"res_m\" : 60 , \"scale\" : 0.0001 , \"offset\" : 0 , \"type\" : \"int16\" , \"unit\" : \"1\" }, { \"band_id\" : \"11\" , \"wavelength_nm\" : 1613.7 , \"res_m\" : 20 , \"scale\" : 0.0001 , \"offset\" : 0 , \"type\" : \"int16\" , \"unit\" : \"1\" }, { \"band_id\" : \"12\" , \"wavelength_nm\" : 2202.4 , \"res_m\" : 20 , \"scale\" : 0.0001 , \"offset\" : 0 , \"type\" : \"int16\" , \"unit\" : \"1\" } ] } 2. Check whether the back-end supports computing zonal_statistics Request GET /processes/zonal_statistics HTTP / 1.1 Response HTTP / 1.1 200 OK Content-Type : application/json; charset=utf-8 Access-Control-Allow-Origin : <Origin> Access-Control-Allow-Credentials : true { \"process_id\" : \"zonal_statistics\" , \"description\" : \"Runs a Python script for each time series of the input dataset.\" , \"args\" :{ \"imagery\" :{ \"description\" : \"array of input collections with one element\" }, \"regions\" :{ \"description\" : \"Polygon file readable by OGR\" }, \"func\" :{ \"description\" : \"Function to apply over the polygons, one of `avg`, `min`, `max`, `median`, `q25`, or `q75`.\" , \"required\" : false , \"default\" : \"avg\" } } } 3. Upload a GeoJSON Polygon Request PUT /user/me/files/polygon1.json HTTP / 1.1 Response HTTP / 1.1 200 OK Access-Control-Allow-Origin : <Origin> Access-Control-Allow-Credentials : true 4. Create a job Request POST /jobs HTTP / 1.1 Content-Type : application/json; charset=utf-8 { \"process_graph\" :{ \"process_id\" : \"zonal_statistics\" , \"args\" :{ \"imagery\" :{ \"process_id\" : \"filter_daterange\" , \"args\" :{ \"imagery\" :{ \"process_id\" : \"filter_bbox\" , \"args\" :{ \"imagery\" :{ \"process_id\" : \"filter_bands\" , \"args\" :{ \"imagery\" :{ \"product_id\" : \"Sentinel2-L1C\" }, \"bands\" : 8 } }, \"left\" : 16.1 , \"right\" : 16.6 , \"top\" : 48.6 , \"bottom\" : 47.2 , \"srs\" : \"EPSG:4326\" } }, \"from\" : \"2017-01-01\" , \"to\" : \"2017-01-31\" } }, \"regions\" : \"/users/me/files/\" , \"func\" : \"avg\" } }, \"output\" :{ \"format\" : \"GPKG\" } } Response HTTP / 1.1 200 OK Content-Type : application/json; charset=utf-8 Access-Control-Allow-Origin : <Origin> Access-Control-Allow-Credentials : true { \"job_id\" : \"f6ea12c5e283438a921b525af826da08\" , \"status\" : \"submitted\" , \"submitted\" : \"2017-01-01T09:32:12Z\" , \"updated\" : \"2017-01-01T09:36:18Z\" , \"user_id\" : \"bd6f9faf93b4\" , \"consumed_credits\" : 0 } 5. Start batch computation at the back-end Request PATCH /jobs/f6ea12c5e283438a921b525af826da08/queue HTTP / 1.1 Response HTTP / 1.1 200 OK Access-Control-Allow-Origin : <Origin> Access-Control-Allow-Credentials : true 6. Check job status twice Request GET /jobs/f6ea12c5e283438a921b525af826da08 HTTP / 1.1 Response HTTP / 1.1 200 OK Content-Type : application/json; charset=utf-8 Access-Control-Allow-Origin : <Origin> Access-Control-Allow-Credentials : true { \"job_id\" : \"f6ea12c5e283438a921b525af826da08\" , \"user_id\" : \"bd6f9faf93b4\" , \"status\" : \"running\" , \"process_graph\" :{ \"process_id\" : \"zonal_statistics\" , \"args\" :{ \"imagery\" :{ \"process_id\" : \"filter_daterange\" , \"args\" :{ \"imagery\" :{ \"process_id\" : \"filter_bbox\" , \"args\" :{ \"imagery\" :{ \"process_id\" : \"filter_bands\" , \"args\" :{ \"imagery\" :{ \"product_id\" : \"Sentinel2-L1C\" }, \"bands\" : 8 } }, \"left\" : 16.1 , \"right\" : 16.6 , \"top\" : 48.6 , \"bottom\" : 47.2 , \"srs\" : \"EPSG:4326\" } }, \"from\" : \"2017-01-01\" , \"to\" : \"2017-01-31\" } }, \"regions\" : \"/users/me/files/\" , \"func\" : \"avg\" } }, \"output\" :{ \"format\" : \"GPKG\" }, \"submitted\" : \"2017-01-01 09:32:12\" , \"updated\" : \"2017-01-01 09:34:11\" , \"consumed_credits\" : 231 } Request GET /jobs/f6ea12c5e283438a921b525af826da08 HTTP / 1.1 Response HTTP / 1.1 200 OK Content-Type : application/json; charset=utf-8 Access-Control-Allow-Origin : <Origin> Access-Control-Allow-Credentials : true { \"job_id\" : \"f6ea12c5e283438a921b525af826da08\" , \"user_id\" : \"bd6f9faf93b4\" , \"status\" : \"finished\" , \"process_graph\" :{ \"process_id\" : \"zonal_statistics\" , \"args\" :{ \"imagery\" :{ \"process_id\" : \"filter_daterange\" , \"args\" :{ \"imagery\" :{ \"process_id\" : \"filter_bbox\" , \"args\" :{ \"imagery\" :{ \"process_id\" : \"filter_bands\" , \"args\" :{ \"imagery\" :{ \"product_id\" : \"Sentinel2-L1C\" }, \"bands\" : 8 } }, \"left\" : 16.1 , \"right\" : 16.6 , \"top\" : 48.6 , \"bottom\" : 47.2 , \"srs\" : \"EPSG:4326\" } }, \"from\" : \"2017-01-01\" , \"to\" : \"2017-01-31\" } }, \"regions\" : \"/users/me/files/\" , \"func\" : \"avg\" } }, \"output\" :{ \"format\" : \"GPKG\" }, \"submitted\" : \"2017-01-01 09:32:12\" , \"updated\" : \"2017-01-01 09:36:57\" , \"consumed_credits\" : 450 } 7. Retrieve download links Request GET /jobs/f6ea12c5e283438a921b525af826da08/download HTTP / 1.1 Response HTTP / 1.1 200 OK Content-Type : application/json; charset=utf-8 Access-Control-Allow-Origin : <Origin> Access-Control-Allow-Credentials : true [ \"https://cdn.openeo.org/4854b51643548ab8a858e2b8282711d8/1.gpkg\" ] 8. Download file(s) Request GET https://cdn.openeo.org/4854b51643548ab8a858e2b8282711d8/1.gpkg HTTP / 1.1 Response (GPKG file) omitted","title":"Examples (proof-of-concept)"},{"location":"examples-poc/#proof-of-concept-api-v002","text":"This page gives a detailed description of the openEO proof of concept and gives a list and specification of what needs to be implemented. The proof of concept will consist of at least three clearly defined example processes (see below), a prototypical API specification including communication API call sequences of the processes (see below), implementations of the processes on three back-ends, and prototypical clients in R, Python and potentially JavaScript. Below, we define example use cases and how they are translated to sequences of API calls: Deriving minimum NDVI measurements over pixel time series of Sentinel 2 imagery Create a monthly aggregated Sentinel 1 product from a custom Python script Compute time series of zonal (regional) statistics of Sentinel 2 imagery over user-uploaded polygons Note Authentication is not included in these examples. Enabling authentication needs the placeholder <Origin> to be set to the requesting host, including protocol, host name/IP and port, e.g. http://localhost:8080 . This could be done by using the Origin header value from the request.","title":"Proof of Concept (API v0.0.2)"},{"location":"examples-poc/#use-case-1","text":"","title":"Use Case 1"},{"location":"examples-poc/#deriving-minimum-ndvi-measurements-over-pixel-time-series-of-sentinel-2-imagery","text":"","title":"Deriving minimum NDVI measurements over pixel time series of Sentinel 2 imagery."},{"location":"examples-poc/#1-check-whether-sentinel-2a-level-1c-data-is-available-at-the-back-end","text":"Request GET /data/Sentinel2A-L1C HTTP / 1.1 Response HTTP / 1.1 200 OK Content-Type : application/json; charset=utf-8 Access-Control-Allow-Origin : <Origin> Access-Control-Allow-Credentials : true { \"product_id\" : \"Sentinel-2A-L1C\" , \"description\" : \"Sentinel 2 Level-1C: Top-of-atmosphere reflectances in cartographic geometry\" , \"source\" : \"European Space Agency (ESA)\" , \"extent\" :[ -34 , 35 , 39 , 71 ], \"time\" :[ \"2016-01-01\" , \"2017-10-01\" ], \"bands\" :[ { \"band_id\" : \"1\" , \"wavelength_nm\" : 443.9 , \"res_m\" : 60 , \"scale\" : 0.0001 , \"offset\" : 0 , \"type\" : \"int16\" , \"unit\" : \"1\" }, { \"band_id\" : \"2\" , \"name\" : \"blue\" , \"wavelength_nm\" : 496.6 , \"res_m\" : 10 , \"scale\" : 0.0001 , \"offset\" : 0 , \"type\" : \"int16\" , \"unit\" : \"1\" }, { \"band_id\" : \"3\" , \"name\" : \"green\" , \"wavelength_nm\" : 560 , \"res_m\" : 10 , \"scale\" : 0.0001 , \"offset\" : 0 , \"type\" : \"int16\" , \"unit\" : \"1\" }, { \"band_id\" : \"4\" , \"name\" : \"red\" , \"wavelength_nm\" : 664.5 , \"res_m\" : 10 , \"scale\" : 0.0001 , \"offset\" : 0 , \"type\" : \"int16\" , \"unit\" : \"1\" }, { \"band_id\" : \"5\" , \"wavelength_nm\" : 703.9 , \"res_m\" : 20 , \"scale\" : 0.0001 , \"offset\" : 0 , \"type\" : \"int16\" , \"unit\" : \"1\" }, { \"band_id\" : \"6\" , \"wavelength_nm\" : 740.2 , \"res_m\" : 20 , \"scale\" : 0.0001 , \"offset\" : 0 , \"type\" : \"int16\" , \"unit\" : \"1\" }, { \"band_id\" : \"7\" , \"wavelength_nm\" : 782.5 , \"res_m\" : 20 , \"scale\" : 0.0001 , \"offset\" : 0 , \"type\" : \"int16\" , \"unit\" : \"1\" }, { \"band_id\" : \"8\" , \"name\" : \"nir\" , \"wavelength_nm\" : 835.1 , \"res_m\" : 10 , \"scale\" : 0.0001 , \"offset\" : 0 , \"type\" : \"int16\" , \"unit\" : \"1\" }, { \"band_id\" : \"8a\" , \"wavelength_nm\" : 864.8 , \"res_m\" : 20 , \"scale\" : 0.0001 , \"offset\" : 0 , \"type\" : \"int16\" , \"unit\" : \"1\" }, { \"band_id\" : \"9\" , \"wavelength_nm\" : 945 , \"res_m\" : 60 , \"scale\" : 0.0001 , \"offset\" : 0 , \"type\" : \"int16\" , \"unit\" : \"1\" }, { \"band_id\" : \"10\" , \"wavelength_nm\" : 1373.5 , \"res_m\" : 60 , \"scale\" : 0.0001 , \"offset\" : 0 , \"type\" : \"int16\" , \"unit\" : \"1\" }, { \"band_id\" : \"11\" , \"wavelength_nm\" : 1613.7 , \"res_m\" : 20 , \"scale\" : 0.0001 , \"offset\" : 0 , \"type\" : \"int16\" , \"unit\" : \"1\" }, { \"band_id\" : \"12\" , \"wavelength_nm\" : 2202.4 , \"res_m\" : 20 , \"scale\" : 0.0001 , \"offset\" : 0 , \"type\" : \"int16\" , \"unit\" : \"1\" } ] }","title":"1. Check whether Sentinel 2A Level 1C data is available at the back-end"},{"location":"examples-poc/#2-check-that-needed-processes-are-available","text":"Request GET /processes/filter_bbox HTTP / 1.1 Response HTTP / 1.1 200 OK Content-Type : application/json; charset=utf-8 Access-Control-Allow-Origin : <Origin> Access-Control-Allow-Credentials : true { \"process_id\" : \"filter_bbox\" , \"description\" : \"Drops observations from a collection that are located outside of a given bounding box.\" , \"args\" :{ \"imagery\" :{ \"description\" : \"array of input collections with one element\" }, \"left\" :{ \"description\" : \"left boundary (longitude / easting)\" }, \"right\" :{ \"description\" : \"right boundary (longitude / easting)\" }, \"top\" :{ \"description\" : \"top boundary (latitude / northing)\" }, \"bottom\" :{ \"description\" : \"bottom boundary (latitude / northing)\" }, \"srs\" :{ \"description\" : \"spatial reference system of boundaries as proj4 or EPSG:12345 like string\" } } } Request GET /processes/filter_daterange HTTP / 1.1 Response HTTP / 1.1 200 OK Content-Type : application/json; charset=utf-8 Access-Control-Allow-Origin : <Origin> Access-Control-Allow-Credentials : true { \"process_id\" : \"filter_daterange\" , \"description\" : \"Drops observations from a collection that have been captured before a start or after a given end date.\" , \"args\" :{ \"imagery\" :{ \"description\" : \"array of input collections with one element\" }, \"from\" :{ \"description\" : \"start date\" }, \"to\" :{ \"description\" : \"end date\" } } } Request GET /processes/NDVI HTTP / 1.1 Response HTTP / 1.1 200 OK Content-Type : application/json; charset=utf-8 Access-Control-Allow-Origin : <Origin> Access-Control-Allow-Credentials : true { \"process_id\" : \"NDVI\" , \"description\" : \"Finds the minimum value of time series for all bands of the input dataset.\" , \"args\" :{ \"imagery\" :{ \"description\" : \"array of input collections with one element\" }, \"red\" :{ \"description\" : \"reference to the red band\" }, \"nir\" :{ \"description\" : \"reference to the nir band\" } } } Request GET /processes/min_time HTTP / 1.1 Response HTTP / 1.1 200 OK Content-Type : application/json; charset=utf-8 Access-Control-Allow-Origin : <Origin> Access-Control-Allow-Credentials : true { \"process_id\" : \"min_time\" , \"description\" : \"Finds the minimum value of time series for all bands of the input dataset.\" , \"args\" :{ \"imagery\" :{ \"description\" : \"array of input collections with one element\" } } }","title":"2. Check that needed processes are available"},{"location":"examples-poc/#3-create-a-job-at-the-back-end","text":"Request POST /jobs HTTP / 1.1 Content-Type : application/json; charset=utf-8 { \"process_graph\" :{ \"process_id\" : \"min_time\" , \"args\" :{ \"imagery\" :{ \"process_id\" : \"NDVI\" , \"args\" :{ \"imagery\" :{ \"process_id\" : \"filter_daterange\" , \"args\" :{ \"imagery\" :{ \"process_id\" : \"filter_bbox\" , \"args\" :{ \"imagery\" :{ \"product_id\" : \"S2_L2A_T32TPS_20M\" }, \"left\" : 652000 , \"right\" : 672000 , \"top\" : 5161000 , \"bottom\" : 5181000 , \"srs\" : \"EPSG:32632\" } }, \"from\" : \"2017-01-01\" , \"to\" : \"2017-01-31\" } }, \"red\" : \"B04\" , \"nir\" : \"B8A\" } } } } } Response HTTP / 1.1 200 OK Content-Type : application/json; charset=utf-8 Access-Control-Allow-Origin : <Origin> Access-Control-Allow-Credentials : true { \"job_id\" : \"2a8ffb20c2b235a3f3e3351f\" , \"status\" : \"submitted\" , \"submitted\" : \"2017-01-01T09:32:12Z\" , \"updated\" : \"2017-01-01T09:36:18Z\" , \"user_id\" : \"bd6f9faf93b4\" , \"consumed_credits\" : 0 }","title":"3. Create a job at the back-end"},{"location":"examples-poc/#4-create-a-wcs-service","text":"Request POST /services HTTP / 1.1 Content-Type : application/json; charset=utf-8 { \"job_id\" : \"2a8ffb20c2b235a3f3e3351f\" , \"type\" : \"wcs\" , \"args\" :{ \"VERSION\" : \"2.0.1\" } } Response HTTP / 1.1 200 OK Content-Type : application/json; charset=utf-8 Access-Control-Allow-Origin : <Origin> Access-Control-Allow-Credentials : true { \"service_id\" : \"4dab456f6501bbcd\" , \"service_url\" : \"https://openeo.org/4dab456f6501bbcd/wcs\" , \"service_type\" : \"wcs\" , \"service_args\" :{ \"VERSION\" : \"2.0.1\" }, \"job_id\" : \"2a8ffb20c2b235a3f3e3351f\" }","title":"4. Create a WCS service"},{"location":"examples-poc/#5-download-the-data-on-demand-with-wcs","text":"Request GET https://openeo.org/4dab456f6501bbcd/wcs?SERVICE=WCS&VERSION=2.0.1&REQUEST=GetCapabilities HTTP / 1.1 Response omitted Request GET https://openeo.org/4dab456f6501bbcd/wcs?SERVICE=WCS&VERSION=2.0.1&REQUEST=GetCoverage&COVERAGEID=2a8ffb20c2b235a3f3e3351f&FORMAT=image/tiff&SUBSET=x,http://www.opengis.net/def/crs/EPSG/0/4326(16.1,16.5)&SUBSET=y,http://www.opengis.net/def/crs/EPSG/0/4326(47.9,48.6)&&SIZE=x(200)&SIZE=y(200) HTTP / 1.1 Response omitted","title":"5. Download the data on demand with WCS"},{"location":"examples-poc/#6-stop-the-job-and-the-service","text":"Request PATCH /jobs/2a8ffb20c2b235a3f3e3351f/cancel HTTP / 1.1 Response HTTP / 1.1 200 OK Access-Control-Allow-Origin : <Origin> Access-Control-Allow-Credentials : true","title":"6. Stop the job (and the service)"},{"location":"examples-poc/#use-case-2","text":"","title":"Use Case 2"},{"location":"examples-poc/#create-a-monthly-aggregated-sentinel-1-product-from-a-custom-python-script","text":"","title":"Create a monthly aggregated Sentinel 1 product from a custom Python script."},{"location":"examples-poc/#1-ask-the-back-end-for-available-sentinel-1-data","text":"Request GET /data/Sentinel1-L1-IW-GRD HTTP / 1.1 Response HTTP / 1.1 200 OK Content-Type : application/json; charset=utf-8 Access-Control-Allow-Origin : <Origin> Access-Control-Allow-Credentials : true { \"product_id\" : \"Sentinel1-L1-IW-GRD\" , \"description\" : \"Sentinel 1 C-band Synthetic Aperture Radar (SAR) Ground Range Data\" , \"source\" : \"European Space Agency (ESA)\" , \"extent\" :[ -34 , 35 , 39 , 71 ], \"time\" :[ \"2016-01-01\" , \"2017-10-01\" ], \"bands\" :[ { \"band_id\" : \"VV\" }, { \"band_id\" : \"VH\" } ] }","title":"1. Ask the back-end for available Sentinel 1 data"},{"location":"examples-poc/#2-ask-the-back-end-whether-it-supports-python-udfs-of-type-aggregate_time-and-get-details-about-expected-parameters","text":"Request GET /udf_runtimes HTTP / 1.1 Response HTTP / 1.1 200 OK Content-Type : application/json; charset=utf-8 Access-Control-Allow-Origin : <Origin> Access-Control-Allow-Credentials : true { \"Python\" :{ \"udf_types\" :[ \"reduce_time\" , \"aggregate_time\" , \"apply_pixel\" ], \"versions\" :{ \"3.6.3\" :{ \"packages\" :[ \"numpy\" , \"scipy\" , \"pandas\" , \"matplotlib\" , \"ipython\" , \"jupyter\" , \"GDAL\" ] } } } } Request GET /udf_runtimes/Python/aggregate_time HTTP / 1.1 Response HTTP / 1.1 200 OK Content-Type : application/json; charset=utf-8 Access-Control-Allow-Origin : <Origin> Access-Control-Allow-Credentials : true { \"process_id\" : \"/udf/Python/aggregate_time\" , \"description\" : \"Runs a Python script for each time series of the input dataset.\" , \"args\" :{ \"imagery\" :{ \"description\" : \"array of input collections with one element\" }, \"script\" :{ \"description\" : \"Python script that will be executed over all time series, gets time series as (Pandas) DataFrame and expects a new DataFrame as output.\" }, \"version\" :{ \"description\" : \"Python version to use, defaults to the latest available version.\" , \"required\" : false , \"default\" : \"latest\" } } }","title":"2. Ask the back-end whether it supports Python UDFs of type aggregate_time and get details about expected parameters"},{"location":"examples-poc/#3-upload-python-script","text":"Request PUT /users/me/files/s1_aggregate.py HTTP / 1.1 Response HTTP / 1.1 200 OK Access-Control-Allow-Origin : <Origin> Access-Control-Allow-Credentials : true","title":"3. Upload python script"},{"location":"examples-poc/#4-create-a-job","text":"Request POST /jobs HTTP / 1.1 Content-Type : application/json; charset=utf-8 { \"process_graph\" :{ \"process_id\" : \"/udf/Python/aggregate_time\" , \"args\" :{ \"script\" : \"/users/me/files/s1_aggregate.py\" , \"imagery\" :{ \"process_id\" : \"filter_daterange\" , \"args\" :{ \"imagery\" :{ \"process_id\" : \"filter_bbox\" , \"args\" :{ \"imagery\" :{ \"product_id\" : \"Sentinel1-L1-IW-GRD\" }, \"left\" : 16.1 , \"right\" : 16.6 , \"top\" : 48.6 , \"bottom\" : 47.2 , \"srs\" : \"EPSG:4326\" } }, \"from\" : \"2017-01-01\" , \"to\" : \"2017-01-31\" } } } } } Response HTTP / 1.1 200 OK Content-Type : application/json; charset=utf-8 Access-Control-Allow-Origin : <Origin> Access-Control-Allow-Credentials : true { \"job_id\" : \"3723c32fb7b24698832ca71f2d3f18aa\" , \"status\" : \"submitted\" , \"submitted\" : \"2017-01-01T09:32:12Z\" , \"updated\" : \"2017-01-01T09:36:18Z\" , \"user_id\" : \"bd6f9faf93b4\" , \"consumed_credits\" : 0 }","title":"4. Create a job"},{"location":"examples-poc/#5-create-a-tms-service","text":"Request POST /services HTTP / 1.1 Content-Type : application/json; charset=utf-8 { \"job_id\" : \"3723c32fb7b24698832ca71f2d3f18aa\" , \"type\" : \"tms\" } Response HTTP / 1.1 200 OK Content-Type : application/json; charset=utf-8 Access-Control-Allow-Origin : <Origin> Access-Control-Allow-Credentials : true { \"service_id\" : \"9dab4b6f6523\" , \"service_url\" : \"http://cdn.cloudprovider.com/openeo/services/9dab4b6f6523/tms\" , \"service_type\" : \"tms\" , \"job_id\" : \"3723c32fb7b24698832ca71f2d3f18aa\" }","title":"5. Create a TMS service"},{"location":"examples-poc/#6-download-results-as-tms","text":"Example Request GET http://cdn.cloudprovider.com/openeo/services/9dab4b6f6523/tms/2017-01-01/12/2232/2668/?bands=1 HTTP / 1.1 Response omitted","title":"6. Download results as TMS"},{"location":"examples-poc/#use-case-3","text":"","title":"Use Case 3"},{"location":"examples-poc/#compute-time-series-of-zonal-regional-statistics-of-sentinel-2-imagery-over-user-uploaded-polygons","text":"","title":"Compute time series of zonal (regional) statistics of Sentinel 2 imagery over user-uploaded polygons"},{"location":"examples-poc/#1-check-whether-sentinel-2a-level-1c-data-is-available-at-the-back-end_1","text":"Request GET /data/Sentinel2A-L1C HTTP / 1.1 Response HTTP / 1.1 200 OK Content-Type : application/json; charset=utf-8 Access-Control-Allow-Origin : <Origin> Access-Control-Allow-Credentials : true { \"product_id\" : \"Sentinel-2A-L1C\" , \"description\" : \"Sentinel 2 Level-1C: Top-of-atmosphere reflectances in cartographic geometry\" , \"source\" : \"European Space Agency (ESA)\" , \"extent\" :[ -34 , 35 , 39 , 71 ], \"time\" :[ \"2016-01-01\" , \"2017-10-01\" ], \"bands\" :[ { \"band_id\" : \"1\" , \"wavelength_nm\" : 443.9 , \"res_m\" : 60 , \"scale\" : 0.0001 , \"offset\" : 0 , \"type\" : \"int16\" , \"unit\" : \"1\" }, { \"band_id\" : \"2\" , \"name\" : \"blue\" , \"wavelength_nm\" : 496.6 , \"res_m\" : 10 , \"scale\" : 0.0001 , \"offset\" : 0 , \"type\" : \"int16\" , \"unit\" : \"1\" }, { \"band_id\" : \"3\" , \"name\" : \"green\" , \"wavelength_nm\" : 560 , \"res_m\" : 10 , \"scale\" : 0.0001 , \"offset\" : 0 , \"type\" : \"int16\" , \"unit\" : \"1\" }, { \"band_id\" : \"4\" , \"name\" : \"red\" , \"wavelength_nm\" : 664.5 , \"res_m\" : 10 , \"scale\" : 0.0001 , \"offset\" : 0 , \"type\" : \"int16\" , \"unit\" : \"1\" }, { \"band_id\" : \"5\" , \"wavelength_nm\" : 703.9 , \"res_m\" : 20 , \"scale\" : 0.0001 , \"offset\" : 0 , \"type\" : \"int16\" , \"unit\" : \"1\" }, { \"band_id\" : \"6\" , \"wavelength_nm\" : 740.2 , \"res_m\" : 20 , \"scale\" : 0.0001 , \"offset\" : 0 , \"type\" : \"int16\" , \"unit\" : \"1\" }, { \"band_id\" : \"7\" , \"wavelength_nm\" : 782.5 , \"res_m\" : 20 , \"scale\" : 0.0001 , \"offset\" : 0 , \"type\" : \"int16\" , \"unit\" : \"1\" }, { \"band_id\" : \"8\" , \"name\" : \"nir\" , \"wavelength_nm\" : 835.1 , \"res_m\" : 10 , \"scale\" : 0.0001 , \"offset\" : 0 , \"type\" : \"int16\" , \"unit\" : \"1\" }, { \"band_id\" : \"8a\" , \"wavelength_nm\" : 864.8 , \"res_m\" : 20 , \"scale\" : 0.0001 , \"offset\" : 0 , \"type\" : \"int16\" , \"unit\" : \"1\" }, { \"band_id\" : \"9\" , \"wavelength_nm\" : 945 , \"res_m\" : 60 , \"scale\" : 0.0001 , \"offset\" : 0 , \"type\" : \"int16\" , \"unit\" : \"1\" }, { \"band_id\" : \"10\" , \"wavelength_nm\" : 1373.5 , \"res_m\" : 60 , \"scale\" : 0.0001 , \"offset\" : 0 , \"type\" : \"int16\" , \"unit\" : \"1\" }, { \"band_id\" : \"11\" , \"wavelength_nm\" : 1613.7 , \"res_m\" : 20 , \"scale\" : 0.0001 , \"offset\" : 0 , \"type\" : \"int16\" , \"unit\" : \"1\" }, { \"band_id\" : \"12\" , \"wavelength_nm\" : 2202.4 , \"res_m\" : 20 , \"scale\" : 0.0001 , \"offset\" : 0 , \"type\" : \"int16\" , \"unit\" : \"1\" } ] }","title":"1. Check whether Sentinel 2A Level 1C data is available at the back-end"},{"location":"examples-poc/#2-check-whether-the-back-end-supports-computing-zonal_statistics","text":"Request GET /processes/zonal_statistics HTTP / 1.1 Response HTTP / 1.1 200 OK Content-Type : application/json; charset=utf-8 Access-Control-Allow-Origin : <Origin> Access-Control-Allow-Credentials : true { \"process_id\" : \"zonal_statistics\" , \"description\" : \"Runs a Python script for each time series of the input dataset.\" , \"args\" :{ \"imagery\" :{ \"description\" : \"array of input collections with one element\" }, \"regions\" :{ \"description\" : \"Polygon file readable by OGR\" }, \"func\" :{ \"description\" : \"Function to apply over the polygons, one of `avg`, `min`, `max`, `median`, `q25`, or `q75`.\" , \"required\" : false , \"default\" : \"avg\" } } }","title":"2. Check whether the back-end supports computing zonal_statistics"},{"location":"examples-poc/#3-upload-a-geojson-polygon","text":"Request PUT /user/me/files/polygon1.json HTTP / 1.1 Response HTTP / 1.1 200 OK Access-Control-Allow-Origin : <Origin> Access-Control-Allow-Credentials : true","title":"3. Upload a GeoJSON Polygon"},{"location":"examples-poc/#4-create-a-job_1","text":"Request POST /jobs HTTP / 1.1 Content-Type : application/json; charset=utf-8 { \"process_graph\" :{ \"process_id\" : \"zonal_statistics\" , \"args\" :{ \"imagery\" :{ \"process_id\" : \"filter_daterange\" , \"args\" :{ \"imagery\" :{ \"process_id\" : \"filter_bbox\" , \"args\" :{ \"imagery\" :{ \"process_id\" : \"filter_bands\" , \"args\" :{ \"imagery\" :{ \"product_id\" : \"Sentinel2-L1C\" }, \"bands\" : 8 } }, \"left\" : 16.1 , \"right\" : 16.6 , \"top\" : 48.6 , \"bottom\" : 47.2 , \"srs\" : \"EPSG:4326\" } }, \"from\" : \"2017-01-01\" , \"to\" : \"2017-01-31\" } }, \"regions\" : \"/users/me/files/\" , \"func\" : \"avg\" } }, \"output\" :{ \"format\" : \"GPKG\" } } Response HTTP / 1.1 200 OK Content-Type : application/json; charset=utf-8 Access-Control-Allow-Origin : <Origin> Access-Control-Allow-Credentials : true { \"job_id\" : \"f6ea12c5e283438a921b525af826da08\" , \"status\" : \"submitted\" , \"submitted\" : \"2017-01-01T09:32:12Z\" , \"updated\" : \"2017-01-01T09:36:18Z\" , \"user_id\" : \"bd6f9faf93b4\" , \"consumed_credits\" : 0 }","title":"4. Create a job"},{"location":"examples-poc/#5-start-batch-computation-at-the-back-end","text":"Request PATCH /jobs/f6ea12c5e283438a921b525af826da08/queue HTTP / 1.1 Response HTTP / 1.1 200 OK Access-Control-Allow-Origin : <Origin> Access-Control-Allow-Credentials : true","title":"5. Start batch computation at the back-end"},{"location":"examples-poc/#6-check-job-status-twice","text":"Request GET /jobs/f6ea12c5e283438a921b525af826da08 HTTP / 1.1 Response HTTP / 1.1 200 OK Content-Type : application/json; charset=utf-8 Access-Control-Allow-Origin : <Origin> Access-Control-Allow-Credentials : true { \"job_id\" : \"f6ea12c5e283438a921b525af826da08\" , \"user_id\" : \"bd6f9faf93b4\" , \"status\" : \"running\" , \"process_graph\" :{ \"process_id\" : \"zonal_statistics\" , \"args\" :{ \"imagery\" :{ \"process_id\" : \"filter_daterange\" , \"args\" :{ \"imagery\" :{ \"process_id\" : \"filter_bbox\" , \"args\" :{ \"imagery\" :{ \"process_id\" : \"filter_bands\" , \"args\" :{ \"imagery\" :{ \"product_id\" : \"Sentinel2-L1C\" }, \"bands\" : 8 } }, \"left\" : 16.1 , \"right\" : 16.6 , \"top\" : 48.6 , \"bottom\" : 47.2 , \"srs\" : \"EPSG:4326\" } }, \"from\" : \"2017-01-01\" , \"to\" : \"2017-01-31\" } }, \"regions\" : \"/users/me/files/\" , \"func\" : \"avg\" } }, \"output\" :{ \"format\" : \"GPKG\" }, \"submitted\" : \"2017-01-01 09:32:12\" , \"updated\" : \"2017-01-01 09:34:11\" , \"consumed_credits\" : 231 } Request GET /jobs/f6ea12c5e283438a921b525af826da08 HTTP / 1.1 Response HTTP / 1.1 200 OK Content-Type : application/json; charset=utf-8 Access-Control-Allow-Origin : <Origin> Access-Control-Allow-Credentials : true { \"job_id\" : \"f6ea12c5e283438a921b525af826da08\" , \"user_id\" : \"bd6f9faf93b4\" , \"status\" : \"finished\" , \"process_graph\" :{ \"process_id\" : \"zonal_statistics\" , \"args\" :{ \"imagery\" :{ \"process_id\" : \"filter_daterange\" , \"args\" :{ \"imagery\" :{ \"process_id\" : \"filter_bbox\" , \"args\" :{ \"imagery\" :{ \"process_id\" : \"filter_bands\" , \"args\" :{ \"imagery\" :{ \"product_id\" : \"Sentinel2-L1C\" }, \"bands\" : 8 } }, \"left\" : 16.1 , \"right\" : 16.6 , \"top\" : 48.6 , \"bottom\" : 47.2 , \"srs\" : \"EPSG:4326\" } }, \"from\" : \"2017-01-01\" , \"to\" : \"2017-01-31\" } }, \"regions\" : \"/users/me/files/\" , \"func\" : \"avg\" } }, \"output\" :{ \"format\" : \"GPKG\" }, \"submitted\" : \"2017-01-01 09:32:12\" , \"updated\" : \"2017-01-01 09:36:57\" , \"consumed_credits\" : 450 }","title":"6. Check job status twice"},{"location":"examples-poc/#7-retrieve-download-links","text":"Request GET /jobs/f6ea12c5e283438a921b525af826da08/download HTTP / 1.1 Response HTTP / 1.1 200 OK Content-Type : application/json; charset=utf-8 Access-Control-Allow-Origin : <Origin> Access-Control-Allow-Credentials : true [ \"https://cdn.openeo.org/4854b51643548ab8a858e2b8282711d8/1.gpkg\" ]","title":"7. Retrieve download links"},{"location":"examples-poc/#8-download-files","text":"Request GET https://cdn.openeo.org/4854b51643548ab8a858e2b8282711d8/1.gpkg HTTP / 1.1 Response (GPKG file) omitted","title":"8. Download file(s)"},{"location":"gettingstarted-backends/","text":"Getting started for back-end providers As a back-end provider who wants to provide its datasets, processes and infrastructure to a broader audience through a standardized interface you may want to implement a driver for openEO. First of all, you should go through the list of openEO repositories and check whether there is already a back-end driver that suits your needs. In this case you don't need to develop your own driver, but \"only\" need to ingest your data, adopt your required processes and set-up the infrastructure. Please follow the documentation for the individual driver you want to use. If your preferred technology has no back-end driver yet, you may consider writing your own driver. All software written for openEO should follow the software development guidelines . You certainly need to understand the architecture of openEO and concepts behind jobs , processes and process graphs . This helps you read and understand the API specification . Technical API related documents like CORS and error handing should be read, too. If you do not want to start from scratch, you could try to generate a server stub from the OpenAPI 3.0 -based API specification with the Swagger code generator . If you are using Python or NodeJS to implement your driver you may re-use some common modules of existing driver implementations: Python Driver Commons NodeJS Driver Commons (TBD) You can implement a back-end in iterations. It is recommended to start by implementing the Capabilities microservice. EO Data Discovery , Process Discovery are important for the client libraries to be available, too. Afterwards you should implement Job Management or synchronous data processing . All other microservices can be added later and are not strictly required to run openEO services. Keep in mind that you don't need to implement all endpoints in the first iteration and that you can specify in the Capabilities, which endpoints you are supporting. For example, you could start by implementing the following endpoints in the first iteration: Capabilities: GET / and GET /output_formats Data discovery: GET /data and GET /data/{data_id} Process discovery: GET /processes Data processing: POST /preview Authentication (if required): GET /credentials/basic Afterwards you can already start experimenting with your first process graphs and process EO data with our client libraries on your back-end. More information will follow soon, for example about back-end compliance testing.","title":"Back-end Providers"},{"location":"gettingstarted-backends/#getting-started-for-back-end-providers","text":"As a back-end provider who wants to provide its datasets, processes and infrastructure to a broader audience through a standardized interface you may want to implement a driver for openEO. First of all, you should go through the list of openEO repositories and check whether there is already a back-end driver that suits your needs. In this case you don't need to develop your own driver, but \"only\" need to ingest your data, adopt your required processes and set-up the infrastructure. Please follow the documentation for the individual driver you want to use. If your preferred technology has no back-end driver yet, you may consider writing your own driver. All software written for openEO should follow the software development guidelines . You certainly need to understand the architecture of openEO and concepts behind jobs , processes and process graphs . This helps you read and understand the API specification . Technical API related documents like CORS and error handing should be read, too. If you do not want to start from scratch, you could try to generate a server stub from the OpenAPI 3.0 -based API specification with the Swagger code generator . If you are using Python or NodeJS to implement your driver you may re-use some common modules of existing driver implementations: Python Driver Commons NodeJS Driver Commons (TBD) You can implement a back-end in iterations. It is recommended to start by implementing the Capabilities microservice. EO Data Discovery , Process Discovery are important for the client libraries to be available, too. Afterwards you should implement Job Management or synchronous data processing . All other microservices can be added later and are not strictly required to run openEO services. Keep in mind that you don't need to implement all endpoints in the first iteration and that you can specify in the Capabilities, which endpoints you are supporting. For example, you could start by implementing the following endpoints in the first iteration: Capabilities: GET / and GET /output_formats Data discovery: GET /data and GET /data/{data_id} Process discovery: GET /processes Data processing: POST /preview Authentication (if required): GET /credentials/basic Afterwards you can already start experimenting with your first process graphs and process EO data with our client libraries on your back-end. More information will follow soon, for example about back-end compliance testing.","title":"Getting started for back-end providers"},{"location":"gettingstarted-clients/","text":"Getting started for client developers For easy access to openEO back-ends it is essential to provide client libraries for users in their well-known programming languages or working environments. This can be either a client library for a specific programming language that hides the technical details of the openEO API or an application with a user interface, e.g. a GIS software plugin or a web-based tool. All software written for openEO should follow the software development guidelines . Client library developers If your preferred programming list is not part of the available client libraries you may consider writing your own client library. Our client libraries are basically translating the openEO API into native concepts of the programming languages. Working with openEO should feel like being a first-class citizen of the programming language. Get started by reading the guidelines to develop client libraries , which have been written to ensure the client libraries provide a consistent feel and behavior across programming languages. You certainly need to understand the concepts behind jobs , processes and process graphs . This helps you understand the API specification and related documents. If you do not want to start from scratch, you could try to generate a client library stub from the OpenAPI 3.0 -based API specification with the Swagger code generator . Make sure the generated code complies to the client library guidelines mentioned above. More information will follow soon, for example about client testing. Applications and Software plugins Standalone applications and software plugins written in a certain programming language could use the existing client libraries to facilitate access to openEO back-ends. Web applications potentially could use the JavaScript client to access openEO back-ends. Back-Ends may also provide standardized web interfaces such as OGC WMS or OGC WCS to access processed EO data. More information will follow soon...","title":"Client Developers"},{"location":"gettingstarted-clients/#getting-started-for-client-developers","text":"For easy access to openEO back-ends it is essential to provide client libraries for users in their well-known programming languages or working environments. This can be either a client library for a specific programming language that hides the technical details of the openEO API or an application with a user interface, e.g. a GIS software plugin or a web-based tool. All software written for openEO should follow the software development guidelines .","title":"Getting started for client developers"},{"location":"gettingstarted-clients/#client-library-developers","text":"If your preferred programming list is not part of the available client libraries you may consider writing your own client library. Our client libraries are basically translating the openEO API into native concepts of the programming languages. Working with openEO should feel like being a first-class citizen of the programming language. Get started by reading the guidelines to develop client libraries , which have been written to ensure the client libraries provide a consistent feel and behavior across programming languages. You certainly need to understand the concepts behind jobs , processes and process graphs . This helps you understand the API specification and related documents. If you do not want to start from scratch, you could try to generate a client library stub from the OpenAPI 3.0 -based API specification with the Swagger code generator . Make sure the generated code complies to the client library guidelines mentioned above. More information will follow soon, for example about client testing.","title":"Client library developers"},{"location":"gettingstarted-clients/#applications-and-software-plugins","text":"Standalone applications and software plugins written in a certain programming language could use the existing client libraries to facilitate access to openEO back-ends. Web applications potentially could use the JavaScript client to access openEO back-ends. Back-Ends may also provide standardized web interfaces such as OGC WMS or OGC WCS to access processed EO data. More information will follow soon...","title":"Applications and Software plugins"},{"location":"gettingstarted-users/","text":"Getting started for users Currently, there are three official client libraries and a web-based interface for openEO. If you are unfamiliar with programming, you could start using the web-based editor for openEO . It supports visual modelling of your algorithms and a simplified JavaScript based access to the openEO workflows and providers. If you are familiar with programming, you could choose a client library for three programming languages: JavaScript (client-side and server-side) Python R Follow the links above to find usage instructions for each of the client libraries. Contribute Didn't find your programming language? You can also access the openEO API implementations directly or start implementing your own client library . If you are missing any functionality in the API feel free to open an issue or actively start proposing API changes as Pull Requests. Make sure to read the API Development Guidelines before. Feel free to contact us for further assistance.","title":"Users"},{"location":"gettingstarted-users/#getting-started-for-users","text":"Currently, there are three official client libraries and a web-based interface for openEO. If you are unfamiliar with programming, you could start using the web-based editor for openEO . It supports visual modelling of your algorithms and a simplified JavaScript based access to the openEO workflows and providers. If you are familiar with programming, you could choose a client library for three programming languages: JavaScript (client-side and server-side) Python R Follow the links above to find usage instructions for each of the client libraries.","title":"Getting started for users"},{"location":"gettingstarted-users/#contribute","text":"Didn't find your programming language? You can also access the openEO API implementations directly or start implementing your own client library . If you are missing any functionality in the API feel free to open an issue or actively start proposing API changes as Pull Requests. Make sure to read the API Development Guidelines before. Feel free to contact us for further assistance.","title":"Contribute"},{"location":"glossary/","text":"Glossary This glossary introduces, and tries to define, the major technical terms used in the openEO project. The acronym openEO contracts two concepts: open : used here in the context of open source software; open source software is available in source code form, and can be freely modified and redistributed; the openEO project will create open source software, reusable under a liberal open source license (Apache 2.0) EO : Earth observation; openEO targets the processing and analysis of Earth observation data Further terms: API : application programming interface ( wikipedia ); a communication protocol between client and back-end client : software environment (software) that end-users directly interact with, e.g. R (rstudio), Python (jupyter notebook), and JavaScript (web browser); R and Python are two major data science platforms; JavaScript is a major language for web development (cloud) back-end : server; computer infrastructure (one or more physical computers or virtual machines) used for storing EO data and processing it big Earth observation cloud back-end server infrastructure where industry and researchers analyse large amounts of EO data simple many end-users now use Python or R to analyse data and JavaScript to develop web applications; analysing large amounts of EO imagery should be equally simple, and seamlessly integrate with existing workflows unified current EO cloud back-ends all have a different API , making EO data analysis hard to validate,difficult to reproduce, and back-ends difficult to compare in terms of capability and costs, or to combine in a joint analysis across back-ends. A unified API can resolve many of these problems. Datasets CEOS ( CEOS OpenSearch Best Practice Document v1.2 ) defines Granules and Collections as follows: \"A granule is the finest granularity of data that can be independently managed. A granule usually matches the individual file of EO satellite data.\" \"A collection is an aggregation of granules sharing the same product specification. A collection typically corresponds to the series of products derived from data acquired by a sensor on board a satellite and having the same mode of operation.\" The same document lists the synonyms used (by organisations) for: granule : dataset (ISO 19115), dataset (ESA), granule (NASA), product (ESA, CNES), scene (JAXA) collection : dataset series (ISO 19115), collection (CNES, NASA), dataset (JAXA), dataset series (ESA), product (JAXA) Here, we will use granule and collection . A granule will typically refer to a limited area and a single overpass leading to a very short observation period (seconds), or a temporal aggregation of such data as e.g. for 16-day MODIS composites. The open geospatial consortium published a document on OGC OpenSearch Geo and Time Extensions . Processes and Jobs The terms process and process graph have different meanings in the openEO API specification. A process is simply the description of an operation as provided by the back end, similar to a function definition in programming languages. In this context openEO will: consider, or allow to consider, band as a dimension consider imagery (image collections) to consist of one or more collections, as argument to functions; allow filtering on a particular collection, or joining them into a single collection allow filtering on attributes, e.g. on cloud-free pixels, or pixels inside a MULTIPOLYGON describing the floodplains of the Danube. This filters on attributes rather than dimensions. Provide generic aggregate operations that aggregate over one or more dimensions. Clients may provide dimension-specific aggregation functions for particular cases (such as min_time ) A process graph includes specific process calls, i.e. references to one or more processes including specific values for input arguments similar to a function call in programming. However, process graphs can chain multiple processes. In particular, arguments of processes in general can be again (recursive) process graphs, input datasets, or simple scalar or array values. User-defined functions (UDFs) The abbreviation UDF stands for user-defined function . With this concept, users are able to upload custom code and have it executed e.g. for every pixel of a scene, allowing custom calculations on server-side data. See the section on UDFs for more information. Aggregation vs. resampling Aggregation computes new values from sets of values that are uniquely assigned to groups. It involves a grouping predicate (e.g. monthly, 100 m x 100 m grid cells; think of SQL's group_by ), and an aggregation function (e.g., mean ) that computes one or more new values from the original ones. Examples: a time series aggregation may return a regression slope and intercept for every pixel time series, for a single band (group by: full time extent) a time series may be aggregated to monthly values by computing the mean for all values in a month (group by: months) spatial aggregation involves computing e.g. mean pixel values on a 100 x 100 m grid, from 10 m x 10 m pixels, where each original pixel is assigned uniquely to a larger pixel (group by: 100 m x 100 m grid cells) Note that for the first example, the aggregation function not only requires time series values, but also their time stamps. Resampling is a broader term where we have data at one resolution, and need values at another (also called scaling ). In case we have values at a 100 m x 100 m grid and need values at a 10 m x 10 m grid, the original values will be reused many times, and may be be simply assigned to the nearest high resolution grid cells (\"nearest neighbor\"), or may be interpolated somehow (e.g. by bilinear interpolation). Resampling from finer to coarser grid by nearest neighbor may again be a special case of aggregation. When the target grid or time series has a lower resolution (larger grid cells) or lower frequency (longer time intervals) than the source grid, aggregation might be used for resampling. For example, if the resolutions are fairly similar, say the source collection has values for consecutive 10 day intervals and the target needs values for consecutive 16 day intervals, then some form of interpolation may be more appropriate than aggregation as defined here.","title":"Glossary"},{"location":"glossary/#glossary","text":"This glossary introduces, and tries to define, the major technical terms used in the openEO project. The acronym openEO contracts two concepts: open : used here in the context of open source software; open source software is available in source code form, and can be freely modified and redistributed; the openEO project will create open source software, reusable under a liberal open source license (Apache 2.0) EO : Earth observation; openEO targets the processing and analysis of Earth observation data Further terms: API : application programming interface ( wikipedia ); a communication protocol between client and back-end client : software environment (software) that end-users directly interact with, e.g. R (rstudio), Python (jupyter notebook), and JavaScript (web browser); R and Python are two major data science platforms; JavaScript is a major language for web development (cloud) back-end : server; computer infrastructure (one or more physical computers or virtual machines) used for storing EO data and processing it big Earth observation cloud back-end server infrastructure where industry and researchers analyse large amounts of EO data simple many end-users now use Python or R to analyse data and JavaScript to develop web applications; analysing large amounts of EO imagery should be equally simple, and seamlessly integrate with existing workflows unified current EO cloud back-ends all have a different API , making EO data analysis hard to validate,difficult to reproduce, and back-ends difficult to compare in terms of capability and costs, or to combine in a joint analysis across back-ends. A unified API can resolve many of these problems.","title":"Glossary"},{"location":"glossary/#datasets","text":"CEOS ( CEOS OpenSearch Best Practice Document v1.2 ) defines Granules and Collections as follows: \"A granule is the finest granularity of data that can be independently managed. A granule usually matches the individual file of EO satellite data.\" \"A collection is an aggregation of granules sharing the same product specification. A collection typically corresponds to the series of products derived from data acquired by a sensor on board a satellite and having the same mode of operation.\" The same document lists the synonyms used (by organisations) for: granule : dataset (ISO 19115), dataset (ESA), granule (NASA), product (ESA, CNES), scene (JAXA) collection : dataset series (ISO 19115), collection (CNES, NASA), dataset (JAXA), dataset series (ESA), product (JAXA) Here, we will use granule and collection . A granule will typically refer to a limited area and a single overpass leading to a very short observation period (seconds), or a temporal aggregation of such data as e.g. for 16-day MODIS composites. The open geospatial consortium published a document on OGC OpenSearch Geo and Time Extensions .","title":"Datasets"},{"location":"glossary/#processes-and-jobs","text":"The terms process and process graph have different meanings in the openEO API specification. A process is simply the description of an operation as provided by the back end, similar to a function definition in programming languages. In this context openEO will: consider, or allow to consider, band as a dimension consider imagery (image collections) to consist of one or more collections, as argument to functions; allow filtering on a particular collection, or joining them into a single collection allow filtering on attributes, e.g. on cloud-free pixels, or pixels inside a MULTIPOLYGON describing the floodplains of the Danube. This filters on attributes rather than dimensions. Provide generic aggregate operations that aggregate over one or more dimensions. Clients may provide dimension-specific aggregation functions for particular cases (such as min_time ) A process graph includes specific process calls, i.e. references to one or more processes including specific values for input arguments similar to a function call in programming. However, process graphs can chain multiple processes. In particular, arguments of processes in general can be again (recursive) process graphs, input datasets, or simple scalar or array values.","title":"Processes and Jobs"},{"location":"glossary/#user-defined-functions-udfs","text":"The abbreviation UDF stands for user-defined function . With this concept, users are able to upload custom code and have it executed e.g. for every pixel of a scene, allowing custom calculations on server-side data. See the section on UDFs for more information.","title":"User-defined functions (UDFs)"},{"location":"glossary/#aggregation-vs-resampling","text":"Aggregation computes new values from sets of values that are uniquely assigned to groups. It involves a grouping predicate (e.g. monthly, 100 m x 100 m grid cells; think of SQL's group_by ), and an aggregation function (e.g., mean ) that computes one or more new values from the original ones. Examples: a time series aggregation may return a regression slope and intercept for every pixel time series, for a single band (group by: full time extent) a time series may be aggregated to monthly values by computing the mean for all values in a month (group by: months) spatial aggregation involves computing e.g. mean pixel values on a 100 x 100 m grid, from 10 m x 10 m pixels, where each original pixel is assigned uniquely to a larger pixel (group by: 100 m x 100 m grid cells) Note that for the first example, the aggregation function not only requires time series values, but also their time stamps. Resampling is a broader term where we have data at one resolution, and need values at another (also called scaling ). In case we have values at a 100 m x 100 m grid and need values at a 10 m x 10 m grid, the original values will be reused many times, and may be be simply assigned to the nearest high resolution grid cells (\"nearest neighbor\"), or may be interpolated somehow (e.g. by bilinear interpolation). Resampling from finer to coarser grid by nearest neighbor may again be a special case of aggregation. When the target grid or time series has a lower resolution (larger grid cells) or lower frequency (longer time intervals) than the source grid, aggregation might be used for resampling. For example, if the resolutions are fairly similar, say the source collection has values for consecutive 10 day intervals and the target needs values for consecutive 16 day intervals, then some form of interpolation may be more appropriate than aggregation as defined here.","title":"Aggregation vs. resampling"},{"location":"guidelines-api/","text":"API Development Guidelines To provide the smoothest possible experience, it's important to have these APIs follow consistent design guidelines, thus making using them easy and intuitive. Language Language Generally, English language MUST be used for all names, documentation etc. In the specification the key words \u201cMUST\u201d, \u201cMUST NOT\u201d, \u201cREQUIRED\u201d, \u201cSHALL\u201d, \u201cSHALL NOT\u201d, \u201cSHOULD\u201d, \u201cSHOULD NOT\u201d, \u201cRECOMMENDED\u201d, \u201cMAY\u201d, and \u201cOPTIONAL\u201d in this document are to be interpreted as described in RFC 2119 . Casing Unless otherwise stated the API works case sensitive. All names SHOULD be written in snake case, i.e. words are separated with one underscore character (_) and no spaces, with all letters lowercased. Example: hello_world . This applies particularly to endpoints and JSON property names. HTTP header fields follow their respective casing conventions, e.g. Content-Type or OpenEO-Costs , despite being case-insensitive according to RFC 7230 . Technical requirements HTTP The API developed by the openEO project uses HTTP REST Level 2 for communication between client and back-end server. Public APIs MUST be available via HTTPS only and all inbound calls MUST be HTTPS. Verbs Endpoints SHOULD use meaningful HTTP verbs (e.g. GET, POST, PUT, PATCH, DELETE). If there is a need to transfer big chunks of data via GET requests, POST requests MAY be used as a replacement as they support to send data via request body. Unless otherwise stated, PATCH requests are only defined to work on the direct (first-level) children of the full JSON object. Therefore, changing a property on a deeper level of the full JSON object always requires to send the whole JSON object defined by the first-level property. Resource naming Naming of endpoints SHOULD follow the REST principles. Therefore, endpoints SHOULD be centered around resources. Resource identifiers MUST be named with a noun in plural form except for single actions that can not be modelled with the regular HTTP verbs. Single actions MUST be single endpoint with a single HTTP verb (POST is RECOMMENDED) and no other endpoints beneath it. Cross-Origin Resource Sharing (CORS) All back-end providers SHOULD support CORS. More information can be found in the corresponding section . Status codes and error handling The success of requests MUST be indicated using HTTP status codes according to RFC 7231 . More information can be found in the section about status und error handling . Requests and response formats JSON Web-based communication, especially when a mobile or other low-bandwidth client is involved, has moved quickly in the direction of JSON for a variety of reasons, including its tendency to be lighter weight and its ease of consumption with JavaScript-based clients. Therefore, services SHOULD use JSON as the default encoding. Other response formats can be requested using Content Negotiation . Clients and servers MUST NOT rely on the order in which properties appears in JSON responses. When supported by the service, clients MAY request that array elements be returned in a specific order. Collections SHOULD NOT include nested JSON objects if those information can be requested from the individual resources. Temporal data Date, time, intervals and durations MUST be formatted according to ISO 8601 if there is an appropriate encoding available in the standard. Open date ranges are not supported by ISO 8601 and MUST be encoded as proposed by Dublin Core Collection Description: Open Date Range Format .","title":"API Specification"},{"location":"guidelines-api/#api-development-guidelines","text":"To provide the smoothest possible experience, it's important to have these APIs follow consistent design guidelines, thus making using them easy and intuitive.","title":"API Development Guidelines"},{"location":"guidelines-api/#language","text":"","title":"Language"},{"location":"guidelines-api/#language_1","text":"Generally, English language MUST be used for all names, documentation etc. In the specification the key words \u201cMUST\u201d, \u201cMUST NOT\u201d, \u201cREQUIRED\u201d, \u201cSHALL\u201d, \u201cSHALL NOT\u201d, \u201cSHOULD\u201d, \u201cSHOULD NOT\u201d, \u201cRECOMMENDED\u201d, \u201cMAY\u201d, and \u201cOPTIONAL\u201d in this document are to be interpreted as described in RFC 2119 .","title":"Language"},{"location":"guidelines-api/#casing","text":"Unless otherwise stated the API works case sensitive. All names SHOULD be written in snake case, i.e. words are separated with one underscore character (_) and no spaces, with all letters lowercased. Example: hello_world . This applies particularly to endpoints and JSON property names. HTTP header fields follow their respective casing conventions, e.g. Content-Type or OpenEO-Costs , despite being case-insensitive according to RFC 7230 .","title":"Casing"},{"location":"guidelines-api/#technical-requirements","text":"","title":"Technical requirements"},{"location":"guidelines-api/#http","text":"The API developed by the openEO project uses HTTP REST Level 2 for communication between client and back-end server. Public APIs MUST be available via HTTPS only and all inbound calls MUST be HTTPS.","title":"HTTP"},{"location":"guidelines-api/#verbs","text":"Endpoints SHOULD use meaningful HTTP verbs (e.g. GET, POST, PUT, PATCH, DELETE). If there is a need to transfer big chunks of data via GET requests, POST requests MAY be used as a replacement as they support to send data via request body. Unless otherwise stated, PATCH requests are only defined to work on the direct (first-level) children of the full JSON object. Therefore, changing a property on a deeper level of the full JSON object always requires to send the whole JSON object defined by the first-level property.","title":"Verbs"},{"location":"guidelines-api/#resource-naming","text":"Naming of endpoints SHOULD follow the REST principles. Therefore, endpoints SHOULD be centered around resources. Resource identifiers MUST be named with a noun in plural form except for single actions that can not be modelled with the regular HTTP verbs. Single actions MUST be single endpoint with a single HTTP verb (POST is RECOMMENDED) and no other endpoints beneath it.","title":"Resource naming"},{"location":"guidelines-api/#cross-origin-resource-sharing-cors","text":"All back-end providers SHOULD support CORS. More information can be found in the corresponding section .","title":"Cross-Origin Resource Sharing (CORS)"},{"location":"guidelines-api/#status-codes-and-error-handling","text":"The success of requests MUST be indicated using HTTP status codes according to RFC 7231 . More information can be found in the section about status und error handling .","title":"Status codes and error handling"},{"location":"guidelines-api/#requests-and-response-formats","text":"","title":"Requests and response formats"},{"location":"guidelines-api/#json","text":"Web-based communication, especially when a mobile or other low-bandwidth client is involved, has moved quickly in the direction of JSON for a variety of reasons, including its tendency to be lighter weight and its ease of consumption with JavaScript-based clients. Therefore, services SHOULD use JSON as the default encoding. Other response formats can be requested using Content Negotiation . Clients and servers MUST NOT rely on the order in which properties appears in JSON responses. When supported by the service, clients MAY request that array elements be returned in a specific order. Collections SHOULD NOT include nested JSON objects if those information can be requested from the individual resources.","title":"JSON"},{"location":"guidelines-api/#temporal-data","text":"Date, time, intervals and durations MUST be formatted according to ISO 8601 if there is an appropriate encoding available in the standard. Open date ranges are not supported by ISO 8601 and MUST be encoded as proposed by Dublin Core Collection Description: Open Date Range Format .","title":"Temporal data"},{"location":"guidelines-clients/","text":"Client library development guidelines TBD","title":"Client Library Development"},{"location":"guidelines-clients/#client-library-development-guidelines","text":"TBD","title":"Client library development guidelines"},{"location":"guidelines-software/","text":"Software Development Guidelines This document describes guidelines for software developers, written for the openEO project. Since the openEO infrastructure will encompasses several programming languages and software environments, this document does not prescribe particular tools or platforms but rather focuses on general principles and methods behind them. License: all software developed in the openEO project and published on the openEO GitHub organisation shall be licensed under the Apache 2.0 license . If software repositories deviate from this, or contain code or other artifacts that deviates from this, this shall be described in the README.md file. Location: Official openEO software is developed under the openEO GitHub organisation . Proof-of-concept versus sustainable: each repository shall indicate its status: either proof-of-concept , or sustainable . Proof-of-concept code is meant to work but comes without quality assurance. Software repositories with proof-of-concept developments shall clearly say so in the first paragraph of the README.md file. Sustainable code should undergo standard quality checks , and point out its documentation . Sustainable code shall undergo code review ; no direct commits to master; any commit shall come in the form of a PR, commit after review. Sustainable code shall be written in a Test-driven manner , and repositories shall at the top of their README.md give indication of the degree to which code is covered by tests. Continuous integration shall be used to indicate code currently passes its test on CI platforms. A Code of conduct describes the rules and constraints to developers and contributors. Version numbers of sustainable software releases shall follow Semantic Versioning 2.0.0 . Software quality guidelines software shall be written in such a way that another person can understand its intention comment lines shall be used sparsely, but effectively reuse of unstable or esoteric libraries shall be avoided Software documentation guidelines Software documentation shall include: installation instructions usage instructions explain in detail the intention of the software pointers to reference documents explaining overarching concepts Each repository's README.md shall point to the documentation. Reference documentation shall be written using well-defined reference documentation language, such as RFC2119 or arc42 , and refer to the definitions used. Software review sustainable software development shall take place by always having two persons involved in a change to the master branch: individuals push to branches, pull request indicate readiness to be taken up in the master branch, a second developer reviews the pull request before merging it into the master branch. software review discussions shall be intelligible for external developers, and serve as implicit documentation of development decisions taken Test-driven development Software shall be developed in a test-driven fashion, meaning that while the code is written, tests are developed that verify, to a reasonable extent, the correctness of the code. Tools such as codecov.io to automatically indicate the amount of code covered by tests, and code that is not covered by tests shall be used in combination with a continuous integration framework. Continuous integration Repositories containing running software shall use an appropriate continuous integration platform, such as Travis CI or similar, to show whether the current build passes all checks. This helps understand contributors that the software passes tests on an independent platform, and may give insights in the way the software is compiled, deployed and tested. Additional guidelines There a specific guidelines for client library development and API development .","title":"Software Development"},{"location":"guidelines-software/#software-development-guidelines","text":"This document describes guidelines for software developers, written for the openEO project. Since the openEO infrastructure will encompasses several programming languages and software environments, this document does not prescribe particular tools or platforms but rather focuses on general principles and methods behind them. License: all software developed in the openEO project and published on the openEO GitHub organisation shall be licensed under the Apache 2.0 license . If software repositories deviate from this, or contain code or other artifacts that deviates from this, this shall be described in the README.md file. Location: Official openEO software is developed under the openEO GitHub organisation . Proof-of-concept versus sustainable: each repository shall indicate its status: either proof-of-concept , or sustainable . Proof-of-concept code is meant to work but comes without quality assurance. Software repositories with proof-of-concept developments shall clearly say so in the first paragraph of the README.md file. Sustainable code should undergo standard quality checks , and point out its documentation . Sustainable code shall undergo code review ; no direct commits to master; any commit shall come in the form of a PR, commit after review. Sustainable code shall be written in a Test-driven manner , and repositories shall at the top of their README.md give indication of the degree to which code is covered by tests. Continuous integration shall be used to indicate code currently passes its test on CI platforms. A Code of conduct describes the rules and constraints to developers and contributors. Version numbers of sustainable software releases shall follow Semantic Versioning 2.0.0 .","title":"Software Development Guidelines"},{"location":"guidelines-software/#software-quality-guidelines","text":"software shall be written in such a way that another person can understand its intention comment lines shall be used sparsely, but effectively reuse of unstable or esoteric libraries shall be avoided","title":"Software quality guidelines"},{"location":"guidelines-software/#software-documentation-guidelines","text":"Software documentation shall include: installation instructions usage instructions explain in detail the intention of the software pointers to reference documents explaining overarching concepts Each repository's README.md shall point to the documentation. Reference documentation shall be written using well-defined reference documentation language, such as RFC2119 or arc42 , and refer to the definitions used.","title":"Software documentation guidelines"},{"location":"guidelines-software/#software-review","text":"sustainable software development shall take place by always having two persons involved in a change to the master branch: individuals push to branches, pull request indicate readiness to be taken up in the master branch, a second developer reviews the pull request before merging it into the master branch. software review discussions shall be intelligible for external developers, and serve as implicit documentation of development decisions taken","title":"Software review"},{"location":"guidelines-software/#test-driven-development","text":"Software shall be developed in a test-driven fashion, meaning that while the code is written, tests are developed that verify, to a reasonable extent, the correctness of the code. Tools such as codecov.io to automatically indicate the amount of code covered by tests, and code that is not covered by tests shall be used in combination with a continuous integration framework.","title":"Test-driven development"},{"location":"guidelines-software/#continuous-integration","text":"Repositories containing running software shall use an appropriate continuous integration platform, such as Travis CI or similar, to show whether the current build passes all checks. This helps understand contributors that the software passes tests on an independent platform, and may give insights in the way the software is compiled, deployed and tested.","title":"Continuous integration"},{"location":"guidelines-software/#additional-guidelines","text":"There a specific guidelines for client library development and API development .","title":"Additional guidelines"},{"location":"jobs/","text":"Processing data using a process graph Process graphs can be executed in three different ways. Results can be pre-computed by creating a batch job using POST /jobs . They are submitted to the back office's processing system, but will remain inactive until POST /jobs/{job_id}/results has been called. They will run only once and store results after execution. Results can be downloaded. Batch jobs are typically time consuming such that user interaction is not possible. Another way of processing and accessing data are web services . Web services allow web-based access using different protocols such as OGC WMS , OGC WCS or XYZ tiles . These protocols usually allow users to change the viewing extent or level of detail (zoom level). Therefore, computations may run on demand , i.e. the requested data is calculated during the request. Back-ends should make sure to cache processed data to avoid additional/high costs and waiting times for the user. Process graphs can also be executed synchronously ( POST /jobs/previews ). Results are delivered with the request itself and no job is created. Only lightweight computations, for example small previews, should be executed using this approach as timeouts are to be expected for long-polling HTTP requests . Data processing details Heterogeneous datasets are unified by the back-ends based on the processes in the process graphs. For instance, the difference between a PROBA-V image and a Sentinel image, which have e a different projection and resolution, are automatically resampled and projected by the back-ends as soon as it is required to do so. Clients are not responsible to ensure that the data matches by first applying resampling or projections processes. Temporal references are always specified on the basis of the Gregorian calendar . Examples Synchronously executed jobs Retrieval of a GeoTIFF Request POST /preview HTTP / 1.1 Content-Type : application/json; charset=utf-8 { \"process_graph\" :{ \"process_id\" : \"min_time\" , \"imagery\" :{ \"process_id\" : \"NDVI\" , \"imagery\" :{ \"process_id\" : \"filter_daterange\" , \"imagery\" :{ \"process_id\" : \"filter_bbox\" , \"imagery\" :{ \"process_id\" : \"get_data\" , \"data_id\" : \"S2_L2A_T32TPS_20M\" }, \"left\" : 652000 , \"right\" : 672000 , \"top\" : 5161000 , \"bottom\" : 5181000 , \"srs\" : \"EPSG:32632\" }, \"extent\" : \"2017-01-01/2017-01-31\" }, \"red\" : \"B04\" , \"nir\" : \"B8A\" } }, \"output\" :{ \"format\" : \"GTiff\" , \"args\" :{ \"tiled\" : true , \"compress\" : \"jpeg\" , \"photometric\" : \"YCBCR\" , \"jpeg_quality\" : 80 } } } Response HTTP / 1.1 200 OK Content-Type : image/tiff Access-Control-Allow-Origin : <Origin> omitted (the GeoTiff file contents) Retrieval of time series Request POST /preview HTTP / 1.1 Content-Type : application/json; charset=utf-8 { \"process_graph\" :{ \"process_id\" : \"zonal_statistics\" , \"imagery\" :{ \"process_id\" : \"filter_daterange\" , \"imagery\" :{ \"process_id\" : \"filter_bbox\" , \"imagery\" :{ \"process_id\" : \"filter_bands\" , \"imagery\" :{ \"process_id\" : \"get_data\" , \"data_id\" : \"Sentinel2-L1C\" }, \"bands\" : 8 }, \"left\" : 16.1 , \"right\" : 16.6 , \"top\" : 48.6 , \"bottom\" : 47.2 , \"srs\" : \"EPSG:4326\" }, \"extent\" : \"2017-01-01/2017-01-31\" }, \"regions\" : \"/users/me/files/\" , \"func\" : \"avg\" }, \"output\" :{ \"format\" : \"GPKG\" } } Response HTTP / 1.1 200 OK Content-Type : application/octet-stream Access-Control-Allow-Origin : <Origin> omitted (the GeoPackage file contents)","title":"Jobs"},{"location":"jobs/#processing-data-using-a-process-graph","text":"Process graphs can be executed in three different ways. Results can be pre-computed by creating a batch job using POST /jobs . They are submitted to the back office's processing system, but will remain inactive until POST /jobs/{job_id}/results has been called. They will run only once and store results after execution. Results can be downloaded. Batch jobs are typically time consuming such that user interaction is not possible. Another way of processing and accessing data are web services . Web services allow web-based access using different protocols such as OGC WMS , OGC WCS or XYZ tiles . These protocols usually allow users to change the viewing extent or level of detail (zoom level). Therefore, computations may run on demand , i.e. the requested data is calculated during the request. Back-ends should make sure to cache processed data to avoid additional/high costs and waiting times for the user. Process graphs can also be executed synchronously ( POST /jobs/previews ). Results are delivered with the request itself and no job is created. Only lightweight computations, for example small previews, should be executed using this approach as timeouts are to be expected for long-polling HTTP requests .","title":"Processing data using a process graph"},{"location":"jobs/#data-processing-details","text":"Heterogeneous datasets are unified by the back-ends based on the processes in the process graphs. For instance, the difference between a PROBA-V image and a Sentinel image, which have e a different projection and resolution, are automatically resampled and projected by the back-ends as soon as it is required to do so. Clients are not responsible to ensure that the data matches by first applying resampling or projections processes. Temporal references are always specified on the basis of the Gregorian calendar .","title":"Data processing details"},{"location":"jobs/#examples","text":"","title":"Examples"},{"location":"jobs/#synchronously-executed-jobs","text":"","title":"Synchronously executed jobs"},{"location":"jobs/#retrieval-of-a-geotiff","text":"Request POST /preview HTTP / 1.1 Content-Type : application/json; charset=utf-8 { \"process_graph\" :{ \"process_id\" : \"min_time\" , \"imagery\" :{ \"process_id\" : \"NDVI\" , \"imagery\" :{ \"process_id\" : \"filter_daterange\" , \"imagery\" :{ \"process_id\" : \"filter_bbox\" , \"imagery\" :{ \"process_id\" : \"get_data\" , \"data_id\" : \"S2_L2A_T32TPS_20M\" }, \"left\" : 652000 , \"right\" : 672000 , \"top\" : 5161000 , \"bottom\" : 5181000 , \"srs\" : \"EPSG:32632\" }, \"extent\" : \"2017-01-01/2017-01-31\" }, \"red\" : \"B04\" , \"nir\" : \"B8A\" } }, \"output\" :{ \"format\" : \"GTiff\" , \"args\" :{ \"tiled\" : true , \"compress\" : \"jpeg\" , \"photometric\" : \"YCBCR\" , \"jpeg_quality\" : 80 } } } Response HTTP / 1.1 200 OK Content-Type : image/tiff Access-Control-Allow-Origin : <Origin> omitted (the GeoTiff file contents)","title":"Retrieval of a GeoTIFF"},{"location":"jobs/#retrieval-of-time-series","text":"Request POST /preview HTTP / 1.1 Content-Type : application/json; charset=utf-8 { \"process_graph\" :{ \"process_id\" : \"zonal_statistics\" , \"imagery\" :{ \"process_id\" : \"filter_daterange\" , \"imagery\" :{ \"process_id\" : \"filter_bbox\" , \"imagery\" :{ \"process_id\" : \"filter_bands\" , \"imagery\" :{ \"process_id\" : \"get_data\" , \"data_id\" : \"Sentinel2-L1C\" }, \"bands\" : 8 }, \"left\" : 16.1 , \"right\" : 16.6 , \"top\" : 48.6 , \"bottom\" : 47.2 , \"srs\" : \"EPSG:4326\" }, \"extent\" : \"2017-01-01/2017-01-31\" }, \"regions\" : \"/users/me/files/\" , \"func\" : \"avg\" }, \"output\" :{ \"format\" : \"GPKG\" } } Response HTTP / 1.1 200 OK Content-Type : application/octet-stream Access-Control-Allow-Origin : <Origin> omitted (the GeoPackage file contents)","title":"Retrieval of time series"},{"location":"processes/","text":"Processes A process is an operation that performs a specific task on a set of parameters and returns a result. It's definition includes a name, a set of parameters, a return type, a set of possible errors or exceptions and some other metadata. In openEO they are used to build a chain of processes (process graph), which can be applied to EO datasets to derive your own findings from the data. To define new processes, back-end providers should know: The name is the identifier for the process and MUST never contain a forward slash / . Each parameter has a name and the content follows a schema. The content returned by a process also follows a schema. The schema usually defines the data type and a format according to JSON schema. There are openEO specific formats defined below. openEO specific formats TBD Core Processes There are some processes that we define to be core processes that are pre-defined and back-ends should follow these specifications to be interoperable. Not all processes need to be implemented by all back-ends. See the process reference for pre-defined processes. Note Currently, there are only few defined processes. Those are only meant as an example how future documentation of processes may look like.","title":"Processes"},{"location":"processes/#processes","text":"A process is an operation that performs a specific task on a set of parameters and returns a result. It's definition includes a name, a set of parameters, a return type, a set of possible errors or exceptions and some other metadata. In openEO they are used to build a chain of processes (process graph), which can be applied to EO datasets to derive your own findings from the data. To define new processes, back-end providers should know: The name is the identifier for the process and MUST never contain a forward slash / . Each parameter has a name and the content follows a schema. The content returned by a process also follows a schema. The schema usually defines the data type and a format according to JSON schema. There are openEO specific formats defined below.","title":"Processes"},{"location":"processes/#openeo-specific-formats","text":"TBD","title":"openEO specific formats"},{"location":"processes/#core-processes","text":"There are some processes that we define to be core processes that are pre-defined and back-ends should follow these specifications to be interoperable. Not all processes need to be implemented by all back-ends. See the process reference for pre-defined processes. Note Currently, there are only few defined processes. Those are only meant as an example how future documentation of processes may look like.","title":"Core Processes"},{"location":"processgraphs/","text":"Process graphs A process graph includes specific process calls, i.e. references to one or more processes including specific values for input arguments similar to a function call in programming. However, process graphs can chain multiple processes. In particular, arguments of processes in general can be again (recursive) process graphs, input datasets, or simple scalar or array values. Schematic definition A process graph is defined to consist of chained processes: <ProcessGraph> := <Process> An argument value of a process can hold a Process again. This allows chaining of processes. Process A single process in a process graph is defined as follows: <Process> := { \"process_id\": <string>, \"description\": <string>, \"<ArgumentName>\": <Value>, ... } A process MUST always contain a key-value-pair named process_id and MAY contain a process_description . It MAY hold an arbitrary number of additional elements as arguments for the process. process_id can currently contain three types of processes: Backend-defined processes, which are listed at GET /processes , e.g. filter_bands . User-defined process graphs, which are listed at GET /users/{user_id}/process_graphs . They are prefixed with /user/ , e.g. /user/my_process_graph . User-defined functions (UDF) are prefixed with /udf and additionally contain the runtime and the process name separated by / , e.g. /udf/Python/apply_pixel . Arguments A process can have an arbitrary number of arguments. The key <ArgumentName> can be any valid JSON key, but it is RECOMMENDED to use snake case and limit the characters to a-z , 0-9 and _ . <ArgumentName> MUST NOT use the names process_id or process_description as it would result in a naming conflict. A value is defined as follows: <Value> := <string|number|boolean|null|array|object|Process|Variable> Note The specified data types except Process and Variable (see definition above) are the native data types supported by JSON. Some limitations apply: An array MUST always contain one data type only and is allowed to contain the data types allowed for <Value> . Objects are not allowed to have keys with the following names: * process_id , except for objects of type Process * variable_id , except for objects of type Variable Caution The expected names of arguments are defined by the process descriptions, which can be discovered with calls to GET /processes and GET /udf_runtimes/{lang}/{udf_type} . Therefore, the key name for a key-value-pair holding an image collection as value doesn't necessarily need to be named imagery . The name depends on the name of the corresponding process argument the image collection is assigned to. Example 2 demonstrates this by using collection as a key once. Variables Process graphs can also hold a variable, which can be filled in later. For shared process graphs this can be useful to make them more portable, e.g in case a back-end specific product name would be stored with the process graph. Variables are defined as follows: <Process> := { \"variable_id\": <string>, \"description\": <string>, \"type\": <string>, \"default\": <Value> } The value for type is the expected data type for the content of the variable and MUST be one of string (default), number , boolean , array or object . The value for variable_id is the name of the variable and can be any valid JSON key, but it is RECOMMENDED to use snake case and limit the characters to a-z , 0-9 and _ . Whenever no value for the variable is defined, the default value is used. <Value> can be used as defined above, but MUST NOT be a Variable . Values for variables can be specified in the query string or body of endpoints supporting variables. See the API reference for more information. Examples Example 1: A full process graph definition including a variable for the data_id. { \"process_id\" : \"min_time\" , \"imagery\" :{ \"process_id\" : \"/udf/Python/custom_ndvi\" , \"imagery\" :{ \"process_id\" : \"filter_daterange\" , \"imagery\" :{ \"process_id\" : \"filter_bbox\" , \"imagery\" :{ \"process_id\" : \"get_data\" , \"data_id\" :{ \"variable_id\" : \"product\" , \"description\" : \"Identifier or the dataset\" , \"type\" : \"string\" , \"default\" : \"S2_L2A_T32TPS_20M\" } }, \"left\" : 652000 , \"right\" : 672000 , \"top\" : 5161000 , \"bottom\" : 5181000 , \"srs\" : \"EPSG:32632\" }, \"extent\" : \"2017-01-01/2017-01-31\" }, \"red\" : \"B04\" , \"nir\" : \"B8A\" } } Example 2: If a process needs multiple processes as input, it is allowed to use arrays of the respective types. { \"imagery\" :{ \"process_id\" : \"union\" , \"collection\" :[ { \"process_id\" : \"filter_bands\" , \"imagery\" :{ \"process_id\" : \"get_data\" , \"data_id\" : \"Sentinel2-L1C\" }, \"bands\" : \"8\" }, { \"process_id\" : \"filter_bands\" , \"imagery\" :{ \"process_id\" : \"get_data\" , \"data_id\" : \"Sentinel2-L1C\" }, \"bands\" : \"5\" } ] } }","title":"Process Graphs"},{"location":"processgraphs/#process-graphs","text":"A process graph includes specific process calls, i.e. references to one or more processes including specific values for input arguments similar to a function call in programming. However, process graphs can chain multiple processes. In particular, arguments of processes in general can be again (recursive) process graphs, input datasets, or simple scalar or array values.","title":"Process graphs"},{"location":"processgraphs/#schematic-definition","text":"A process graph is defined to consist of chained processes: <ProcessGraph> := <Process> An argument value of a process can hold a Process again. This allows chaining of processes.","title":"Schematic definition"},{"location":"processgraphs/#process","text":"A single process in a process graph is defined as follows: <Process> := { \"process_id\": <string>, \"description\": <string>, \"<ArgumentName>\": <Value>, ... } A process MUST always contain a key-value-pair named process_id and MAY contain a process_description . It MAY hold an arbitrary number of additional elements as arguments for the process. process_id can currently contain three types of processes: Backend-defined processes, which are listed at GET /processes , e.g. filter_bands . User-defined process graphs, which are listed at GET /users/{user_id}/process_graphs . They are prefixed with /user/ , e.g. /user/my_process_graph . User-defined functions (UDF) are prefixed with /udf and additionally contain the runtime and the process name separated by / , e.g. /udf/Python/apply_pixel .","title":"Process"},{"location":"processgraphs/#arguments","text":"A process can have an arbitrary number of arguments. The key <ArgumentName> can be any valid JSON key, but it is RECOMMENDED to use snake case and limit the characters to a-z , 0-9 and _ . <ArgumentName> MUST NOT use the names process_id or process_description as it would result in a naming conflict. A value is defined as follows: <Value> := <string|number|boolean|null|array|object|Process|Variable> Note The specified data types except Process and Variable (see definition above) are the native data types supported by JSON. Some limitations apply: An array MUST always contain one data type only and is allowed to contain the data types allowed for <Value> . Objects are not allowed to have keys with the following names: * process_id , except for objects of type Process * variable_id , except for objects of type Variable Caution The expected names of arguments are defined by the process descriptions, which can be discovered with calls to GET /processes and GET /udf_runtimes/{lang}/{udf_type} . Therefore, the key name for a key-value-pair holding an image collection as value doesn't necessarily need to be named imagery . The name depends on the name of the corresponding process argument the image collection is assigned to. Example 2 demonstrates this by using collection as a key once.","title":"Arguments"},{"location":"processgraphs/#variables","text":"Process graphs can also hold a variable, which can be filled in later. For shared process graphs this can be useful to make them more portable, e.g in case a back-end specific product name would be stored with the process graph. Variables are defined as follows: <Process> := { \"variable_id\": <string>, \"description\": <string>, \"type\": <string>, \"default\": <Value> } The value for type is the expected data type for the content of the variable and MUST be one of string (default), number , boolean , array or object . The value for variable_id is the name of the variable and can be any valid JSON key, but it is RECOMMENDED to use snake case and limit the characters to a-z , 0-9 and _ . Whenever no value for the variable is defined, the default value is used. <Value> can be used as defined above, but MUST NOT be a Variable . Values for variables can be specified in the query string or body of endpoints supporting variables. See the API reference for more information.","title":"Variables"},{"location":"processgraphs/#examples","text":"Example 1: A full process graph definition including a variable for the data_id. { \"process_id\" : \"min_time\" , \"imagery\" :{ \"process_id\" : \"/udf/Python/custom_ndvi\" , \"imagery\" :{ \"process_id\" : \"filter_daterange\" , \"imagery\" :{ \"process_id\" : \"filter_bbox\" , \"imagery\" :{ \"process_id\" : \"get_data\" , \"data_id\" :{ \"variable_id\" : \"product\" , \"description\" : \"Identifier or the dataset\" , \"type\" : \"string\" , \"default\" : \"S2_L2A_T32TPS_20M\" } }, \"left\" : 652000 , \"right\" : 672000 , \"top\" : 5161000 , \"bottom\" : 5181000 , \"srs\" : \"EPSG:32632\" }, \"extent\" : \"2017-01-01/2017-01-31\" }, \"red\" : \"B04\" , \"nir\" : \"B8A\" } } Example 2: If a process needs multiple processes as input, it is allowed to use arrays of the respective types. { \"imagery\" :{ \"process_id\" : \"union\" , \"collection\" :[ { \"process_id\" : \"filter_bands\" , \"imagery\" :{ \"process_id\" : \"get_data\" , \"data_id\" : \"Sentinel2-L1C\" }, \"bands\" : \"8\" }, { \"process_id\" : \"filter_bands\" , \"imagery\" :{ \"process_id\" : \"get_data\" , \"data_id\" : \"Sentinel2-L1C\" }, \"bands\" : \"5\" } ] } }","title":"Examples"},{"location":"processreference/","text":"Placeholder for generated process specifications.","title":"Process Reference"},{"location":"udfs/","text":"User-defined functions The abbreviation UDF stands for user-defined function . With this concept, users are able to upload custom code and have it executed e.g. for every pixel of a scene, allowing custom calculations on server-side data. More information regarding the current proposal for UDFs can be found in a separate repository .","title":"UDFs"},{"location":"udfs/#user-defined-functions","text":"The abbreviation UDF stands for user-defined function . With this concept, users are able to upload custom code and have it executed e.g. for every pixel of a scene, allowing custom calculations on server-side data. More information regarding the current proposal for UDFs can be found in a separate repository .","title":"User-defined functions"},{"location":"usermanagement/","text":"User Management and Accounting In general, the openEO API only defines a minimum subset of user management and accounting functionality. It allows to authenticate and authorize a user, which may include user registration with OpenID Connect , handle storage space limits (disk quota), manage billing, which includes to query the credit a user has available, estimate costs for certain operations (data processing and downloading), get information about produced costs, limit costs of certain operations. Therefore, the API leaves some aspects open that have to be handled by the back-ends separately, including credential recovery, e.g. retrieving a forgotten password user data management, e.g. changing the users payment details or email address payments, i.e. topping up credits for pre-paid services or paying for post-paid services other accounting related tasks, e.g. creating invoices, user registration (only specified when OpenID Connect is implemented).","title":"User Management and Accounting"},{"location":"usermanagement/#user-management-and-accounting","text":"In general, the openEO API only defines a minimum subset of user management and accounting functionality. It allows to authenticate and authorize a user, which may include user registration with OpenID Connect , handle storage space limits (disk quota), manage billing, which includes to query the credit a user has available, estimate costs for certain operations (data processing and downloading), get information about produced costs, limit costs of certain operations. Therefore, the API leaves some aspects open that have to be handled by the back-ends separately, including credential recovery, e.g. retrieving a forgotten password user data management, e.g. changing the users payment details or email address payments, i.e. topping up credits for pre-paid services or paying for post-paid services other accounting related tasks, e.g. creating invoices, user registration (only specified when OpenID Connect is implemented).","title":"User Management and Accounting"}]}